
Code School
Oracle PL/SQL: Transactions, Dynamic SQL & Debugging
by Pankaj Jain

In this course, we will talk in detail about transaction management in Oracle. We will take an in-depth look at dynamic SQL and debugging options in PL/SQL, and explore in detail the SQL Developer Debugger.

Resume CourseBookmarkAdd to ChannelLive mentoring
Table of contents
Description
Transcript
Exercise files
Discussion
Learning Check
Recommended
Introduction

Overview

Hello, and welcome to Pluralsight. My name is Pankaj Jain, and welcome to this module on Oracle PL/SQL: Transactions, Dynamic SQL, and Debugging. In this course we start off discussing transactions in Oracle PL/SQL. We will take a high-level overview of how transaction management occurs in Oracle database. Then we will talk in depth about commits, rollbacks, savepoints, naming transactions, initiating read-only transactions etc, and other concepts, which will provide you a firm grip on transaction management in PL/SQL in order to write code which is accurate and performs as expected. Autonomous transaction is a really neat feature which was introduced in Oracle version 8i. It allows you to initiate an independent transaction from within another transaction. This enables you to do with extreme ease and grace activities like logging, auditing, usage monitoring, and several other use cases, which would have otherwise required a lot of complex coding. Dynamic SQL lets you write SQL, which is determined at runtime versus compile time. You can write extremely flexible code and avoid a lot of redundancy using dynamic SQL. Native dynamic SQL is one way of writing dynamic SQL. It is very simple to use and understand and supports both Oracle-supplied data types, as well as user-defined data types. We will see how to write single row and multi-row selects, DDL, DML, and session-controlled statements using native dynamic SQL. We will also see how to execute anonymous blocks and store subprograms using it. DBMS_SQL is a built-in package in Oracle, which provides us an ordinate way of writing dynamic SQL. This has been around from very early versions of Oracle and has some unique capabilities of executing dynamic SQL where the number of columns and the select statement are unknown or the number of bind variables in a SQL statement are unknown. We will see how to execute anonymous blocks, DDL, DML statements, etc using it. Finally, we will talk about how we can effectively debug our code when it is not giving us the expected results or is erroring out. We spoke about DBMS output and DBMS utility packages in the Oracle PL/SQL Fundamentals Part 1 course. We will briefly touch upon them over here. We will build on our discussion of SQL development debugger from the Part 1 course and explore the capabilities when debugging stored program units. We will see how to observe the data values and execution flow, as well as how to quickly reach the offending lines of code in a sub program even it is buried several layers down in the call stack.

Prerequisites

This course is a continuation of the Oracle PL/SQL Fundamentals Part 1 and Part 2 course on Pluralsight and builds on the concepts we learned in those courses, so if you have not checked out those courses yet I would highly recommend you to do so. In the Part 1 course we cover some of the basic concepts and programming constructs like anonymous blocks, PL/SQL data types, conditional executions, loops, cursors, exceptions, etc. In the Part 2 course we talk extensively about named program units like stored procedures, functions, and packages, which help encapsulate business logic precompiled and stored in the database. These concepts will come into play as we discuss transactions, dynamic SQL, and debugging stored program units in this course. So, one of the prerequisites to be more effective with this course is to take the Oracle PL/SQL Fundamentals Part 1 and Part 2 course or otherwise you should have equivalent programming knowledge to get the most out of this course.

Audience

So, who would be the audience for this course? This course is for Oracle programmers having a fair experience and a good understanding of the basic concepts of PL/SQL programming, for Oracle programmers who want to learn in detail concepts around transactions, write dynamic SQL, and know how to effectively debug PL/SQL code in order to understand the execution flow and provide bug fixes quickly, concepts essential to become proficient in Oracle PL/SQL programming and harness its power. This course would also be useful for web programmers who like to gain the optimization and efficiency of executing the database and SQL-intensive business logic in the database layer versus web layer thus providing multiple roundtrips to the database and network latency. And this course would also be useful to other programmers who have been following along the Oracle PL/SQL Fundamentals Part 1 and Part 2 courses and want to expand their knowledge of Oracle PL/SQL programming to be more effective with it.

Tools

In this course we will be talking about PL/SQL programming constructs, which should work with most Oracle database versions; however, it is always recommended to use the latest version in order to take advantage of the optimizations and features which come in with each new version. For the demos I will be using Oracle Express Edition. It is an entry level small footprint database free to develop, deploy, and distribute on, and it would be sufficient to practice the concepts that we will talk about in this course. For our tools I will be using Oracle SQL Developer exclusively. It is a free IDE or development environment, which you can download from Oracle's website. It has IntelliSense, Code Explorer, and several other useful features to make writing and debugging PL/SQL very easy and efficient. SQLPLUS is a command-line utility, which you can install on the client machine to communicate with the database. You can use it for running scripts against the database. A lot of the examples we will talk about in this course can be run using SQLPLUS. However, if you have access to and are more familiar to other IDEs like Toad and SQL Navigator just to name a few, feel free to use them. I will be using SQL Developer exclusively for running the demos in this course and use its easy to use and powerful debugger when we talk about debugging in the last module in this course.

Transaction Management in PL/SQL

Transaction Management in Oracle

Hi. Welcome to Pluralsight. My name is Pankaj Jain, and welcome to this module on Transaction Management in PL/SQL. We all are very familiar with the concept of transactions in our day-to-day lives. We do transactions at checkout counters in stores where we pay money and buy some goods or at an ATM counter where we swipe our debit card to debit our bank account and receive some cash in return. Transactions are an integral part of our database management system, and one of the ways a database differentiates itself from a file management system. Oracle Database has a very robust and powerful transaction management framework, and it provides us with a lot of transaction control statements to have greater control over our transactions. It is very important to understand the concepts of transaction management in PL/SQL in order to write code which is accurate and provides expected results. In this module we will first try and understand transaction management in Oracle at a higher level. Knowing how things work behind the scenes always helps in understanding the concept better. We will then talk about commits, rollbacks, and savepoints, which are transaction-controlled statements, which enable us to set boundaries for our transactions and have more control over them. We will see how we can name our transactions to better monitor them, initiate read only transactions for point-in-time reports, and briefly talk about locks. Let us take a closer look at this important concept. Oracle Database supports ACID properties for the transaction. The A in ACID stands for the atomicity of a transaction. It means it is either all or nothing. Either everything goes or nothing goes through. It supports atomicity in all possible situations, even during crashes or power failures. C stands for consistency, which ensures that a transaction will bring the database from one consistent state to another. I stands for isolation, which means that concurrent transactions would result in the same final state as if the transactions were executed one after another. And D stands for durability, which means that once the transaction is committed it will be durable or will be prominent even if there is a system crash or power failure. The database will have recovery mechanisms in place to ensure that the commit is applied. So, what is a transaction? A transaction is a logical atomic unit of work comprising of one or more SQL statements. All these statements either are applied together as one or not applied at all. The transaction begins with the issuance of a DML statement like select, insert, update, or delete statements. They also begin with the issuance of a DDL statement like CREATE TABLE. Set transaction statement for which you can give your own name to a transaction or specify a transaction mode like read only, etc, also can start a transaction. The transaction ends when you issue a commit, which makes changes permanent; a rollback, which undoes the changes. We will talk more about them shortly. A DDL statement also implicitly shows a commit thus ending the transaction. You should remember this as sometimes we accidently commit a transaction even though when you don't intend to do so by issuing a DDL statement in your transaction. A transaction also ends when the session ends, either when the user exits normally or when the session is terminated abnormally. We will talk about the implications of these situations shortly. Let us take a high-level overview of what happens in a transaction. Let us say we have our transaction started which has a bunch of DML statements. When the transaction starts, Oracle takes a snapshot of data to capture the before image and keeps the undo information in an undo table space. This can be useful to bring the data back to the old state if the transaction is rolled back or is undone. As the transaction proceeds, changes made are recorded in memory in an online Redo Log Buffer. The data blocks worked upon during the transaction are kept in the Data Buffer Cache, and any changes made to the data is also reflected by changing the data in the Data Buffer Cache. Soon an error occurs, and we decide to roll back the changes. ROLLBACK is to undo the changes made. Then the database makes use to the undo information it had stored earlier to bring the database back to the state before the transaction started. However, say everything goes well; then we decide to commit. In that case the database will write the online redo buffers to a redo log file on disk. This will be useful for recovering the database in case of crashes. The changes in the Data Buffer Cache will make their way to the data files on disk. This can happen any time before or after the commit. Oracle tries to optimize the write to the database files by batching them up in order to reduce network IO's, locks, and latency. Since Oracle has the redo information and redo logs, it can use it for recovery in case the database crashes before it gets a chance to propagate the data before cache changes to the data files. Let us take a quick look at an example to one of those time transactions. Let us say we have a procedure process_order, which is used to place orders. It takes in p_act_id, the account ID placing the order of type accounts.act_id%TYPE. P_item_id is the item for which the order is being placed of type items.item_id%TYPE, and p_item_value is the value of the item. The transaction begins with the first DML statement, which is an UPDATE to the accounts table reducing the account balance WHERE the item value for the passed account ID. Next we INSERT an order into the orders table inserting order_id as the next value from the sequence order_seq.NEXTVAL and p_item_id and p_act_id. If everything goes well, we COMMIT, which ends the transaction making the changes permanent in the database. However, if there is an error, say during the INSERT statement, then an error would be raised. Inside the EXCEPTION block we have a WHEN OTHERS handler where we would print the DBMS_UTILITY.FORMAT_ERROR_STACK and DBMS_UTILITY.FORMAT_ERROR_BACKTRACE using DBMS_OUTPUT statements. Then we issue the ROLLBACK, which will end the transaction and will also undo the changes made by the transaction. Lastly, we issue a raise to raise the exception to the calling client. We want both the legs of the transaction to either go together or not go at all, so the use of the transaction controlled statements like COMMIT helps us draw the boundaries of our transaction, and ROLLBACK ensures that if there is an error the entire transaction is rolled back. We will talk about COMMIT and ROLLBACK in more detail shortly. Before that, let us take a look at how to name a transaction.

Transaction Names & Read Only Transactions

If you want, you can give your transaction a name. You do so using the SET TRANSACTION command. The syntax is SET TRANSACTION NAME followed by the 'txn_name.' So, a transaction name is a user-defined name you can give to identify your transaction. It has to be the first statement of a transaction, and if there are DML statements before it Oracle will require you to either commit or rollback those before you can issue a SET TRANSACTION statement. It is of course optional to name a transaction. Its main use is to monitor long-running transactions as a DBA can identify the transaction by its name using Oracle Enterprise Manager or data dictionary tables. The database also inserts the transaction name to the transaction audit redo record, which is useful for searching the transaction in a log using a utility like LogMiner. V$TRANSACTION is a table which a DBA can query to monitor the status of a long-running transaction. This table has the transaction name, status, consistent read gets, physical IO, undo records held, and a lot of other useful information about a running transaction. It shows information only for active transactions, and as soon as a transaction is over the row disappears from the table for that transaction. So, for instance, here is the same procedure, and I am just showing partial code over here. We can put the SET TRANSACTION NAME'place_order' in the beginning of our procedure to identify the transaction. We can then run the procedure as EXEC process_order passing in account ID 1, item ID 2 with an item value of 500. While this procedure is running, a DBA can issue a query like SELECT name, status FROM V$TRANSACTION to identify the transaction from the transactions table and then further issue queries to monitor the resource usage. Of course we can also see the transaction name when you issue DML statements in SQL Developer or SQLPLUS. For instance, say a DBA user logs into SQL Developer and issues the SET TRANSACTION statement setting the TRANSFORMATION NAME to 'mytxn' and then inserts into the demo.items table. Then after that the DBA can issue SELECT name, status FROM V$TRANSACTION to identify the transaction and further query to monitor the resource usage related to that transaction. Once the transaction has committed a rollback, the transaction disappears from the transaction table. Using SET TRANSACTION command, you can also initiate a read only transaction. The syntax says SET TRANSACTION READ ONLY NAME with the 'txn_name.' So, where would we use that? Typically when you want to read data using a snapshot of data from when the transaction started. In a typical transaction a select statement would read data, which is the committed data at the point in time when the select statement started and not the set of data when the transaction started. After the transaction started, say another session changes and commits some data. Somewhere later in our transaction say we have a select statement which queries that same information. It'll see the latest changes, committed data at that point of time and not the data at the point of time the transaction started, but for some kind of reporting, say our month end report job, which starts at midnight, there might be a requirement that when the transaction starts we just want a report based on the snapshot of data at the point in time when the transaction started. The read only transactions might be useful in those cases. Oracle provides a multi-table, multi-query read consistent view. It has to with the first statement of a transaction, and the read only transaction ends when a commit or a rollback is issued. So, in this code snippet we issue a read only transaction by using the command SET TRANSACTION READ ONLY 'Current Month Order Count.' Then we issue a bunch of SELECT statements to read a total count of orders for a customer for a current month. The COMMIT at the end ends the transaction.

Commits

Let us talk about commits in more detail. As we know already that commits make the changes in the transaction permanent, online redo buffer information is read into the redo logs, and database files are also updated ultimately to reflect the changes. A System Change Number is generated. A System Change Number is a logical internal timestamp used by Oracle to keep track of the events in the order in which they occur. Every committed transaction gets an SCN number. All changes in the transaction get the same SCN number, so SCN 10 is an event which has occurred before SCN 11. This also helps Oracle during the recovery process in marking and keeping track of what changes are on disk. You can observe the SCN number for an active transaction from the same V$ transaction table. A transaction places locks on the rows it affects. A commit releases all the locks and frees all the sources tied to that transaction. We have not talked about savepoints yet, but a commit also erases all savepoints set until that time. One thing to know about commits is that until a commit is issued changes are not visible to other sessions. Uncommitted changes are only visible to the session making the changes. For instance, let us see in a session we issue an INSERT to the items table for item_id 3 with the item_name Treadmill and item_value of 400. So, if you try and issue a SELECT from the items table for item_id 3 in the same session and try to show the value of item_name using DBMS_OUTPUT, it will print out Treadmill as it can see the changes. However, let us say we have another session in SQL Developer. It doesn't matter if it is another session with the same user ID. If it tries to query the item_name for item_id 3, the query will return back with nothing. However, after the transaction is committed the same query in another session will be able to fetch the item_name Treadmill for item_id 3. Let us talk about DDL statements as I think they fall in a special category. When you issue a DDL statement, it will first commit any outstanding changes and then will run as a transaction of its own. So, here with the UPDATE statement the transaction begins. Then we issue a DDL statement EXECUTE IMMEDIATE 'DROP TABLE TEMP_TABLE.' Notice the syntax EXECUTE IMMEDIATE. This is the dynamic SQL syntax, which is the only way you can issue a DDL inside PL/SQL. We will talk about dynamic SQL in more detail in a later module, so do not worry too much about its syntax. Issuing the DDL statement will end the previous transaction by issuing an implicit commit. It'll then also start a new transaction for the DDL statement. Once the DDL statement is executed, the transaction would end. When the INSERT statement is executed later on, it'll start a new transaction, which will end with COMMIT if everything goes fine or with a ROLLBACK in case of an error. I just wanted to specially point out how DDL statements can affect an existing transaction by committing when you may not be intending to, something you should be aware of. Finally, say if you exit your session normally and there are outstanding changes which you have not committed or rolled back yet. Tool flag SQL Developer will prompt you with whether you want to commit or roll back the outstanding changes are about the disconnect. In SQLPlus the default behavior is to rollback. In SQL Developer in the menu option under Tools Preferences Database Object Preferences you can set auto- commit on, which will commit automatically after each DML statement, but I would not recommend doing that to avoid accidental commits.

Rollbacks & Statement Level Atomicity

We have talked about the rollback statement a few times earlier. A rollback statement undoes all changes made by the transaction and brings the data out of the state before the transaction. It ends the current transaction. All the locks and resources on the objects involved in the transaction are released, and it also reveals any savepoints established in the transaction. We will talk about savepoints shortly. Let us take a look at the same procedure as before. The transaction begins with the UPDATE statement, and if everything is successful ends with a COMMIT. But in case there is an error the execution will flow to the EXCEPTION block where a ROLLBACK will be issued to revert the changes made by the transaction. So, the ROLLBACK ensures that any errors reverts all changes in the transaction as a whole and thus supports the atomicity aspect of an ACID transaction. The error can be system generated, say in the case of an error during the insert where the execution will flow automatically to the EXCEPTION block, or it can be a user-defined exception whereby using the RAISE statement we make it flow to the EXCEPTION block where we issue the ROLLBACK. Let me now talk about Statement Level Atomicity. A SQL statement is an atomic unit of work. When you issue an update, it does the relevant updates and comes back and instead will insert a row and come back. Each of these statements performs a unit of work. Then you tie the work done by these statements together in the transaction using a COMMIT OR A ROLLBACK. So, if a SQL statement fails, it draws back automatically the work done by that statement alone, but your work done by another statement before it would still stay. So, if you have an UPDATE statement followed by an INSERT statement in the transaction and the INSERT statement fails, it will only rollback the changes the INSERT statement was trying to make, but your work done by the UPDATE statement will still stay. All the side effects of the statement are also rolled back. So, for instance, if that statement in turn calls a trigger to be executed, then the changes made by the trigger are also rolled back, so all changes made by that statement and the statements it invokes automatically are treated as a unit. This is called statement level atomicity. So, if you take a look at the same procedure, see our UPDATE goes to it successfully, but there is an error during the INSERT in which case the execution flows to the EXCEPTION block. Here if you were to issue a COMMIT the changes made by the UPDATE statement would be committed. This is an important concept to know so that you can avoid unexpected behavior during commits where you might think that since one of the statements failed all the work done by the previous statements in the transaction are also rolled back automatically. Let us talk about some other rollback considerations. If the session ends abnormally, say due to a machine crash or power outage or whatever, Oracle will automatically roll back any uncommitted changes. A rollback transaction does not undo any package or session variable changes. Let us say we have a package acct_mgmt with a global variable g_debit. So, for instance, in this partial code if after the successful update we set acct_mgmt.g_debit to a value of Y and then say later there is an error in the INSERT statement, which causes the flow to go to the EXCEPTION block where we issue a ROLLBACK statement, but this would not roll back the changes to the package variable as evident by the DBMS_OUTPUT statement after the ROLLBACK, so we need to keep track of changes to package variables and reset them manually in the ROLLBACK section.

Demo: Commit & Rollback

In this demo we will take a look at a transaction. We will see the effects of rollbacks and commits and understand statement level atomicity. First let's create the set of tables. We have first the CUSTOMERS table, which has a cust_id, which is its PRIMARY KEY. Cust_name and cust_location both watch our two columns. Then we have the ACCOUNTS table having the act_id as the PRIMARY KEY, act_cust_id linking the customer to the account, and act_bal and the CONSTRAINT act_cust_fk, which is a FOREIGN KEY constraint linking act_cust_id to the cust_id in the customers table. Then we have the ITEMS table where we have the items to be sold having the item_id, item_name, and item_value. Next is the ORDERS table, which will contain the orders placed and would have the order_id, order_item_id, the item for which the item is placed, and order_act_id. We have two FOREIGN KEY constraints, the order_item_fk linking the order_item_id to the item_id of the items table and the order_act_fk linking the order_act_id to the act_id of the accounts tale. Finally we have a TEMP_TABLE with one column. We are then inserting three customers with ID 1, 2, and 3 in the customers table, an account for each of the customers with ID 1 for customer ID 1, ID 2 for customer ID 2, and 3 for customer ID 3. We insert a couple of items with ID 1 and 2 in the items table. Finally we have the COMMIT. Let's run this script. Everything ran successfully. Let us create a package act_mgmt, which has just one variable g_debit of type VARCHAR2 with an initial value of N. Let's compile this. Let us create a sequence, orders_seq, with the command CREATE SEQUENCE orders_seq START WITH 1 INCREMENT BY 1. We will use this sequence to generate the order ID while inserting in the orders table. Let's create this. So, here is the PROCEDURE process_order, which we are using to place an order. It takes p_act_id, the act_id placing the order; p_item_id, which is the item_id for which the order is being placed; and the item_value p_item_value. We have defined two variables l_new_bal and l_balance_again both of type accounts.act_bal%TYPE. The BEGIN keyword begins the execution section. We can name this transaction using the command SET TRANSACTION NAME 'place_order.' This will be useful if this was a long-running transaction and we wanted to monitor it. In our case, however, it will finish fast, and we will not be able to take a look at the V$TRANSACTION table as it is removed from there as soon as a transaction finishes. First we debit the account for the item value by issuing an UPDATE to the accounts table reducing the balance by the p_item_value for the past account ID fetching the new balance by using the RETURNING INTO clause in the l_new_bal variable. We then print using DBMS_OUTPUT Account id debited for p_item_value new balance is l_new_bal. We are then setting the package variable act_mgmt.g_debit to a value of Y. We print a message stating that. Then we insert into the orders table the order generating the order_id using the orders_seq for the item_id in act_id. We then print 'Successful insertion in the orders table.' If everything goes well, we COMMIT. In case of an error, we have an EXCEPTION block. In the EXCEPTION section we first select the act_bal for act_id 1 at this point of time before calling the ROLLBACK. Based on the statement level atomicity the error in the insert statement should not affect the update, and it should still have the reduced value over here. We print using DBMS_OUTPUT.PUT_LINE statement 'In Exception Section. Balance for account id 1 before rollback' and print the l_balance_again. Then you print the error message using DBMS_UTILITY.FORMAT_ERROR_STACK and FORMAT_ERROR_BACKTRACE. We issue the ROLLBACK and then again print the package variable act_mgmt.g_debit at this time. It should still retain the value assigned to it as a ROLLBACK does not rollback the package level variable assignment. Let's compile this procedure. Let us first select from the ACCOUNTS table as we can notice all the accounts have a balance of $1000.00. Let's select from the ITEMS table. There are two items, ITEM_ID 1 and 2. Let's select from the ORDERS table. It currently has no orders. Now let us run the procedure EXEC process_order passing account id 1 placing the order for item id 3, which does not exist for $200.00. We are purposely passing a wrong item id to raise an error during the insertion in the ORDERS table. Let's run this. As expected, the update was successful, and account id 1 was debited for $200.00 bringing its new balance to $800.00. The package variable act_mgmt.g_debit was assigned a value of Y. While trying to insert there was an error due to integrity constraint violation, and the execution flows to the Exception Section where before rolling back we print the balance for account id 1, which is still 800. This demonstrates a statement level atomicity that the error caused in the insert statement did not cause the _____ update statement that would roll back. The rollback was issued. The value of the package variable act_mgmt.g_debit after the rollback is still Y, so this demonstrates that the package level variable value does not revert back due to rollback statement. Let's check the balance in the ACCOUNTS table. The rollback was successful as all the accounts still have a balance of $1000.00. Let's check in the ORDERS table. There is still no order inserted. What happens if we issue a DDL statement in between our transaction? So, here after the update I have put in EXECUTE IMMEDIATE 'DROP TABLE temp_table' command. EXECUTE IMMEDIATE is a dynamic SQL command. Do not worry too much about its syntax at this point in time. Just know that we are issuing a DDL statement over here. Let's compile our procedure again. Let us execute our procedure process_order again where we are trying to raise an error by passing an invalid item ID of 3. As before, the update statement was successful reducing the balance of account id 1 to $800.00. The package variable act_mgmt.g_debit was assigned a value of Y. Trying to insert into the ORDERS table again raised an exception, and the exception section account id 1 had a balance of $800.00 before the rollback statement. The rollback statement was issued. The value of the package variable act_mgmt.g_debit after the rollback is still Y. Let us select from the ACCOUNTS table. This time the balance has been committed to $800.00 demonstrating that the DDL statement issued an implicit commit. This is something you need to be careful of. Now, back to our procedure process_order. Let's comment the EXECUTE IMMEDIATE statement, and let's compile again. Let us execute our procedure process_order again, but this time let us change the item ID to 1, which has a value of $600.00. Let's run the procedure again. The update was successful. Account id 1 was debited for $600.00 bringing its new balance down to $200.00 from the previous $800.00. Package variable act_mgmt.g_debit was assigned a value of Y, and this time since the item ID was valid there was a successful insertion in the orders table. Let's query ACCOUNTS table. The new balance of $200.00 was successfully committed. Let's query the ORDERS table. The order was placed successfully for account id 1, so the commit made the changes permanent.

Savepoints

Let us now talk about savepoint, which is another transaction control statement. The syntax for defining a savepoint is SAVEPOINT followed by 'savepoint_name,' so a savepoint is a user- defined marker within a transaction. A savepoint helps you break a really long transaction into logical chunks, so creating a savepoint makes sense where you can identify logical groups within a transaction, which would be okay to commit as a logical unit even though the main transaction fails. We will explain this more shortly. You can have multiple savepoints within a transaction, and you're not just limited to one or two savepoints. Savepoints enable you to do partial rollbacks. So, for instance, if there is a failure in a statement after a savepoint, instead of issuing a rollback, which goes all the way to the beginning of the transaction, you can ask it to be rolled back just to the savepoint. So, only changes made after the savepoint will be rolled back, and the changes made before the savepoint would still stay. Oracle will release all locks it acquires because of changes it made after the savepoint. So, let us understand savepoint with an example. Let us say that instead of one order we have two orders to place, and we are trying to place both orders as a part of a single transaction. In each order we debit the account for the item value and then insert the order in the orders table. Each order by itself is a logical, complete unit of work, and so we can break the transaction into two parts and place a savepoint in between, which we are calling as savepoint_after_first_order. Now, let us say that order 1 goes through fine, but during the placing of order 2, say while inserting in the orders table there is an error causing an exception to be raised. Inside the exception block we will print the error message and what went wrong. Then we ROLLBACK to savepoint_after_first_order. To begin the exception section, you might want to log the problem in a log table, and you have to place it after the ROLLBACK statement and before the COMMIT to make sure that it does not roll back too. So, the savepoint allows us to only undo the changes made in the second order while still allowing us to commit the first order. This is just a simple example to demonstrate how savepoints can be declared and used. You would use them in situations where you have a transaction which can be logically split into groups, and you do not want to start all over again when a statement way down in the transaction fails. You can have multiple savepoints in a transaction. For instance, here after the first insert we have defined SAVEPOINT first. Then after UPDATE and any of the statements after that we have defined SAVEPOINT second. It is not required to have more than one statement in order to define a savepoint. If you want, you can define a savepoint after each statement, though practically you will not do it in most cases. After the INSERT statement, we define SAVEPOINT third. Now, let us say somewhere down based on some condition we decide that we want to rollback to SAVEPOINT second. So, at this point whatever work we had done after SAVEPOINT second is rolled back, and the state of the data goes back as at SAVEPOINT second. With this rollback we have erased SAVEPOINT third. It now no longer exists. SAVEPOINT first still exists though. Also, any locks placed on rows after SAVEPOINT second are released. What if we just issued the ROLLBACK statement? In that case the transaction will be rolled back to the very first statement erasing all the savepoints in between. So here, if I just say ROLLBACK, it'll go all the way up and undo everything including the first INSERT. If you define a savepoint with a name, which is the same name for a savepoint defined earlier, Oracle will override the old savepoint with a new one so the old savepoint is no longer there, and the marker is now moved to the new savepoint. Here is an example of how you might use savepoints practically. Say we have a table which holds the order queue information and we have a PL/SQL process, which processes the orders one-by-one in a batch. Let us say we have a cursor cur_get_order_queue, which fetches the order info. We ran a cursor loop in which we fetch the data in the order_var. In the beginning of the loop we set a SAVEPOINT savepoint_before_order. We then open a block inside, which begins with the BEGIN statement. Then we perform the debit via the UPDATE to the accounting table and then INSERT the order in the orders table. If everything goes well, we do a COMMIT. However, if there is an error in any statement and it goes to the EXCEPTION block, then we ROLLBACK TO savepoint_before_order thus rolling back that order only. Then we log the error recording the account ID, item ID, and the reason for the error. We COMMIT afterwards to commit the insert to the log table inside the log_error procedure. So, every time in the loop the savepoint is overwritten setting it just before the current order being processed. This way we process all orders, which go successfully, as well as also record orders, which have errors in one loop without having to start over and over again.

Demo: Savepoints

In this demo we will see how to set up savepoints and use the ROLLBACK statement to explicitly go to it in case of an error. We will also see how declaring a savepoint overwrites a previous savepoint with the same name. We will demonstrate how using savepoints we can ensure that we process all orders in one go recording the ones that fail and processing the rest. First, let's update all the accounts to a balance of $1000.00. Let us delete anything we might have inserted into the orders table and then commit. Let's run these statements. Now, here is an anonymous block where we are processing two orders. We have declared l_act_id1_bal and l_act_id2_bal to hold the new balances for the two accounts after placing the order. We start with the first order in UPDATE accounts setting the act_bal to act_bal - 600 bringing it down to 400 for act_id 1. Using the RETURNING clause, we return back the new balance into l_act_id1_bal. Using DBMS_OUTPUT, we print a new balance. Next we place the order by inserting the orders table for order_id 1, item_id 2, and act_id 1. We then establish a SAVEPOINT savepoint_after_first_order at this point in time. Next we start the second order. We again debit account ID 2 for $600.00 and get a new balance of $400.00 into l_act_id2_bal. But now while inserting the second order we are purposely inserting a wrong item number of 10, which does not exist. This would cause a referential integrity constraint to be violated raising the error to the exception block where we print some messages and then ROLLBACK to savepoint_after_first_order. Then we COMMIT. We expect the rolling back to savepoint_after_first_order should revert only the update to account ID 2, and then the subsequent commit should commit the changes for the first order. Let's run this block. As expected, we debited act_id 1 for $600.00 bringing the new balance to $400.00. We inserted in the orders table for act id 1. We then established savepoint_after_first_order. The second order started, and the update to act_id 2 was successful bringing the balance down to $400.00. However, while inserting there was an error. Inserting for the second order you do referential integrity violation. We then roll back to savepoint_after_first_order and then commit. Let us now take a look at the accounts table and the orders table. Let's first select from the accounts table for act_id 1 and 2. Act_id 1 was debited bringing the balance down to $400.00. Update to act_id 2 was rolled back successfully restoring the balance to $1000.00. Let us now examine the orders table. The order was successfully place for act_id 1 as expected. For the next demo let us create an orders_queue table where we queue up orders for batch processing. It has a QUEUE_ID, queue_act_id, the account ID placing the order for queue_item_id. First let us update the account balances for account's 1, 2, and 3 to $1000.00 each. We then insert in the orders_queue table three orders, one for act_id 1, the next for act_id 20, which does not exist and should error out during processing, and lastly for act_id 3. Each one is placing an order for item_id 2, which is worth $600.00. Next let's flush out the orders table and commit. Let's run these statements. Let us insert another item, item_id 3 in the items table and commit. Now, next is a procedure we use for batch processing called batch_processing. We start off by declaring a cursor, cur_get_orders, where we get a queue_act_id, queue_item_id, and item_value making a join between the orders_queue table and the items table on the item_id. Inside the execution section we open the cursor for LOOP getting the results in orders as well. We first establish a SAVEPOINT at the beginning of a loop called savepoint_before_order. So, before each order this will be set and will override any savepoint set earlier in the loop. Next we have block inside the loop, which we have created so that the exception is caught inside the exception section, and then it again goes back to the loop. Inside we process the order updating the account, reducing its balance, and then inserting in the orders table. If all goes well, we COMMIT. Then we have an EXCEPTION section where we ROLLBACK TO savepoint_before_order erasing the changes we made for the last order in the loop. Using DBMS_OUTPUT we output the account id, item id, and the error message. You will most probably have a call to a log table to log the error over here. The way we have inserted the order in the orders_queue table the second transaction would fail, but since we have a savepoint we can just roll back the second order and still commit the first order and the third. END batch_processing ends the procedure. Let's compile this procedure. The procedure was successfully compiled. Let us now execute this procedure by calling exec batch_processing. From the Output we notice that it processed order for account id 1, order for account id 20 failed as it does not exist, and order for account id 3 processed successfully. Let us now check the account balances. The balance for account id 1 and 3 were indeed reduced to $400.00. Let us look at the orders table now. The order was successfully placed for account id 1 and 3. So, using savepoints we could process all the orders in one go, recording the ones that failed, and processing the rest.

Explicit Locks

One of the major benefits and strengths of using Oracle Database is its efficient and optimized lock management. It allows different sessions to read and write data without interfering with each other and at the same time ensuring that the locking time is reduced to a minimum. Generally you would want to leave it to Oracle to handle the locks for you, but in certain situations you might want to have more control of the locks. Oracle provides you mechanisms to do that. For instance, a couple of ways to do that is by the use of the cursor FOR UPDATE statement and the Lock Table statement. Let us take a look at them. When you open a cursor and fetch a set of rows based on a query, Oracle does not lock them right away. It only locks them once we have changed them. This helps in reducing the locking time to a minimum and increases general availability, but then there is a chance that some other session can obtain the same row and change the data and commit it before our session gets a chance to issue an update and lock the row. The cursor will have a snapshot of the data from the point of time it was opened, and it will not be able to see the latest data change committed outside your session. And if your change is based on a certain column having a certain value, and if it was changed by another session, your cursor, which still sees the old data value, will make the update based on that thus causing incorrect updates. It will make the changes without knowing how the data last looked. Sometimes you might want exclusive lock on rows right from the time you select them in your cursor to ensure that no one else can change it outside your session. These records will be available to others only after your session issues a commit or a rollback. This ensures that as you iterate over the rows in your result set you can be certain these are the most current values. Cursor FOR UPDATE can help with these kinds of situations. We spoke about these in detail in the Cursors module of the Part 1 course, so I will just briefly touch upon them over here. So, say we want to import a fine of $10.00 for all accounts having a balance less than $500.00 at month end, so we have a CURSOR cur_upd_acts, which takes in p_bal of type accounts.act_bal%TYPE, which selects act_id, act_bal FROM accounts WHERE act_bal<p_bal FOR UPDATE OF act_bal. Using the FOR UPDATE OF act_bal makes it a FOR UPDATE cursor, and Oracle will obtain locks on the rows fetched from the cursor as soon as it is opened. Inside the execution section we open the cursor in a loop and update the account setting the balance to be act_bal - 10 using the special keyword using CURRENT OF cursor to identify the current row in the loop. The COMMIT will end the transaction and release the locks. So, since this update is based on the act_bal being less than p_bal, the FOR UPDATE cursor ensures that at the point in time where we issue the actual update to the accounts table the balance is still less than 500 and that the rows obtained by the cursor are locked until this session issues a commit. Oracle, as I mentioned before, tries to minimize locking to provide high availability and concurrency at the same time ensuring data integrity and asset transactions, so in most situations I would leave it to Oracle to manage locks. There are shared locks and exclusive locks. For example, many users can have row share locks on a table, but only one user can have an exclusive lock on the table. Once a table is an exclusive lock, it prevents other users from inserting, updating, or deleting in the table. When a transaction tries to insert, update, or delete a row, it obtains a row exclusive lock on those rows, and if another session wants to act upon the same rows then it has to wait until the first one finishes. Locks never prevent querying of the data, and that query will not obtain table lock. I will not go in much detail over these. In most situations you would not need to use these in PL/SQL programming. LOCK TABLE accounts IN ROW SHARE MODE NOWAIT will enable you to acquire a row share lock on the table and telling Oracle not to wait and come back right away if it cannot obtain the lock. ROW SHARE lock allows concurrent access to the table, but will prevent any other session from getting an exclusive lock on the entire table. With LOCK TABLE accounts IN ROW EXCLUSIVE MODE the transaction requires a ROW EXCLUSIVE MODE lock on the table. When you commit a rollback, these locks are released.

Summary

Understanding how transactions work and can be managed is very important while writing PL/SQL code. Oracle Database has a very powerful transaction management framework. Oracle provides locks and reads consistent snapshots automatically in an optimized fashion in order to provide high availability and concurrency along with assent guarantee for the transactions. It provides us with mechanisms like commit, which makes the changes permanent, rollback to undo the changes, and savepoint, which allows us to put markers in between transaction boundaries to which we can grow programmatically. Lock table statements are available if you want to have more control, but generally I would leave it to Oracle to manage locks automatically.

Autonomous Transactions

What Are Autonomous Transactions?

Hi. Welcome to Pluralsight. My name is Pankaj Jain, and welcome to this module on Autonomous Transactions. Autonomous transactions is a very neat feature introduced with Oracle version 8i, which allows us to initiate independent transactions from within a parent transaction. That has enormous use for logging, usage monitoring, and a lot of other tasks. It allows us to do tasks, which otherwise would take a lot of complex coding and require a complex setup of database pipes and jobs. It has a very simple syntax, but packs a lot of punch in providing us with powerful functionality with amazing ease. Let us learn more about this important feature. What is an Autonomous Transaction? As I mentioned earlier, an autonomous transaction allows us to initiate an independent transaction from within another outer transaction. It is not affected by commits, rollbacks, or savepoints initiated in the outer transaction and does not share any context information of the outer transaction. It is a PRAGMA Directive. A PRAGMA Directive is a compiler directive, which is processed at compile time and not at runtime. The pragma passes the relevant information to the compiler. Why should you use or need to know about autonomous transactions? Autonomous transactions allow you to easily accomplish certain tasks, which otherwise would require a lot of complex setup and coding. Some of the common uses of an autonomous transaction are: Logging. Sometimes you want that the log information be saved even if the main transaction is rolled back. You can try and do it by calling the login procedure after the rollback and issuing a commit right after that, but it makes the coding more complex, and sometimes it can get a little tricky and may not be possible. Autonomous transactions take away all the complexity and provide an easy, straightforward way to accomplish that. Another place they can be used is in usage counters for say software programs if we really want to restrict the program of usage only to a certain number of times or in general track the program usage. You can use them to track the retry counters say when a user is trying to log in. In general, it helps in creating modular, reusable code. It helps you encapsulate business logic and call it in an independent fashion.

How to Define Autonomous Transactions

It is extremely simple to define an autonomous transaction. Its syntax is PRAGMA AUTONOMOUS_TRANSACTION followed by a semicolon. It can be defined in the declaration section. It can appear anywhere within the declaration section, but as a good programming practice I like to declare it as the first thing in my declaration section. Where can the autonomous transaction be defined? They can be defined in the top level anonymous block. For instance, here is an anonymous block where we are inserting some information in the log table. It has the PRAGMA AUTONOMOUS_TRANSACTION as the first thing in the declaration section. This is followed by l_act_id of type accounts.act_id%TYPE followed by l_msg, which is a VARCHAR2 given the value of Test. In the execution section we insert in the log_table the log_id using a sequence log_seq.NEXTVAL. NEXTVAL is a keyword, which fetches the next value from a sequence. We insert the l_act_id, l_msg, and the SYSTIMESTAMP. Finally, we COMMIT and END the transaction. Autonomous transaction pragma, however, cannot be defined in a nested block, so here if you try and define it in a nested block Oracle will raise errors. PRAGMA AUTONOMOUS_TRANSACTION can be defined in standalone procedures as shown in this example where we have defined it after the IS keyword and before the execution section begins. It can be defined in standalone functions too. Here is a function, log_msg_fn, which returns a number. The PRAGMA AUTONOMOUS_TRANSACTION is defined after the RETURN clause and before the execution section begins. The autonomous transaction pragma can also be defined in packaged procedures and functions. It cannot be defined in the specification section. It has to be defined in the package body section. So, for instance, we have defined the pragma for the procedure log_msg after the IS keyword. We have also defined it for the function log_msg_fn after the RETURN clause and before the BEGIN keyword. It is not necessary that all the procedures or functions have this pragma. You can have it only with the ones you want. We haven't talked about triggers yet, but triggers are program units you tie to a table, view, schema, or the database defined automatically when an event happens like say a trigger defined to fire whenever an insert happens in a table. Database triggers cannot issue commits or rollbacks, but if you define them as autonomous transactions then they can. This is a feature which might be useful for auditing where you want to have the audit information committed even when the insert is rolled back. We haven't talked about SQL object types. Oracle Database supports object-oriented programming. You can create objects and define methods and attributes on them. You use PL/SQL to call the methods. These methods can also be defined as autonomous.

How Autonomous Transactions Work & Considerations

Let us understand what happens when an autonomous transaction is initiated from a main transaction. Here is our procedure process_order again, which we are using to place orders for a passed p_act_id for a given p_item_id of value p_item_value. We first debit the account and reduce its balance. Next we want to log our action using a procedure log_msg, which defined as an AUTONOMOUS_TRANSACTION. The log_msg procedure is a simple procedure which takes in p_act_id and the log_msg and inserts it in the log_table. It is defined as AUTONOMOUS_TRANSACTION using the PRAGMA. Now, let us understand how the transaction flows. The UPDATE statement will start the OuterTransaction. Next log_msg is called, which will transfer the execution to the log_msg procedure. When the log_msg procedure starts, it will suspend the OuterTransaction. The INSERT INTO log_table statement will start the autonomous transaction. When we COMMIT later, this will end the autonomous transaction. When the log_msg procedure completes, the OuterTransaction resumes. The execution flow goes to the next statement after the log_msg procedure. Finally, the COMMIT in the process_order procedure ends the OuterTransaction. Every autonomous transaction which issues a DML should be ended by a COMMIT or a ROLLBACK. Of course if it is not doing any DML like insert, update, and delete, then commits and rollbacks are not needed, but there is not much point in creating an autonomous transaction anyways if you are not doing any DML inside them. If you do not put a commit or rollback inside, Oracle will still allow you to compile them, but when executing them it will raise an error ORA-06519: active autonomous transaction detected and rolled back. If an exception is raised inside an autonomous program unit which goes unhandled, then Oracle will automatically roll back the transaction. So say inside of a log_msg procedure we are also doing some housekeeping every time it is run. We delete all messages older than seven days first and then see later in the INSERT statement there is an error. Since there is no exception block to handle the error, it'll be raised to the calling client unhandled, and Oracle will automatically roll back the delete made with the first statement. The exception will flow to the exception block of the order program if it exists as it does with normal exception propagation when one program calls another. What are the valid transaction management commands we can use inside the autonomous transaction? Well, you can use all the familiar commands like commit, rollback, savepoint, and the set transaction command. And you're not limited to having just one commit inside it. You can have multiple commits if you need. So here, if you wanted it for some reason, we can issue a COMMIT after the DELETE FROM the log_table first and then again issue a COMMIT after the INSERT statement. You can in the same way have multiple rollbacks or savepoints inside your autonomous transaction. The only thing to take care of is that there is no change which is either not committed or rolled back when you come out of the autonomous transaction.

Need for Autonomous Transactions

Let us understand the need for autonomous transactions with an example. Let us take a look at our procedure process_order, which takes in p_act_id placing order for p_item_id of value p_item_value. Inside we first debit the account by issuing an update and releasing the act_bal of p_act_id by p_item value. Then we log a message passing in two parameters, p_act_id and the message Debited. Later we insert the order in the orders table. Then we log a message again for p_act_id with a message Order Placed. If everything goes well, we issue a COMMIT. If, however, there is an error, then inside the EXCEPTION block in the WHEN OTHERS handler we log a message for p_act_id putting in the error message returned by the SQLERRM function. Now, here is our log_msg procedure, which takes in two parameters, p_act_id and p_msg. It simply inserts in the log_table. As the log_msg procedure stands now if there is an error say while doing the INSERT in the orders table, the execution will flow to the EXCEPTION block, and the ROLLBACK over there will roll back not only the update to the accounts table, but it will also roll back the log_msg inserts in the log_table. But what if our requirement is to log the messages even if the transaction is rolled back? Autonomous transactions can come to our rescue in this situation. Let us make a slight change to the log_msg procedure. Let us just add the PRAGMA AUTONOMOUS_TRANSACTION statement at the beginning of the declaration section. Let us add a COMMIT at the end. This simple command packs in a lot of punch and would enable us to run the log_msg procedure in the transaction of its own unaffected by the rollbacks, commits, or savepoints in the outer transaction. So, now the rollback can revert changes to the accounts table, but inserts in the log_table will persist. An autonomous transaction does not share the transaction context with the outer transaction, so it does not share the commits, rollbacks, savepoints, locks, object changes, and resources of the outer transaction. To illustrate the point, let us look at our procedure process_order again. Let us say after the UPDATE to the accounts table we issue a SAVEPOINT by the name of first. Then we call the log_msg procedure inside which we again establish a SAVEPOINT with the same name first in the beginning. This savepoint is different from the savepoint in the outer transaction, and it will not overwrite it. After the log_msg procedure finishes and the execution comes back to the process_order procedure again, executing ROLLBACK TO first will take us to SAVEPOINT first in the process_order procedure. It will now roll back the INSERT we did in the log_table that then got committed independently in the autonomous transaction. Once the autonomous transaction is completed, the changes committed by it are visible to the main transaction, as well as the other sessions. So, in this procedure we're inserting in the log_table using the log_msg procedure by passing by passing an account ID of 1 and a message'Act 1 debited.' Inside the log_msg procedure, which is an autonomous transaction, we insert in the log_table. When the execution resumes to the main transaction, if we have to query we COUNT FROM THE log_table WHERE log_act_id is 1, we will get a value of 1 in our main transaction. Say we have another session outside, and if it queries the COUNT FROM the log_table for log_act_id of 1, it will also get a value of 1. This is the default behavior; however, if you do not want this to happen you can set the isolation level to serializable for the main transaction using the command SET TRANSACTION ISOLATION LEVEL SERIALIZABLE. Then the main transaction cannot see the changes made in the autonomous transaction when it resumes. Transaction mode SERIALIZABLE would allow only serial execution of transactions versus concurrent execution. You should not be using serial more generally as it severely compromises performance, but I just wanted to mention the behavior in case this isolation level is set.

Demo: Need for Autonomous Transactions

In this demo we will see how to create an autonomous procedure and also demonstrate a common usage scenario for it, which is application logging. First, let's create a logging table called log_table, which has a log_id, a log_act_id, a log_msg column, and a log_insert_time. We create a sequences log_seq to insert in the log_id. We create it as create sequence log_seq START WITH 1 INCREMENT BY 1. Let's run these statements. Next we create a procedure to log messages, log_msg, which takes in p_act_id of accounts.act_id%TYPE and p_msg of type VARCHAR2. We insert in the log_table the log_id using the log_seq.NEXTVAL, log_act_id as p_act_id, and log_msg as p_msg. As it is defined now, it is not an autonomous transaction. Let's compile this procedure. Let us now create the procedure process_order, which takes in the act_id p_act_id placing the order for p_item_id of value p_item_value. We first debit the account by updating the accounts setting the account balance to be act_bal - p_item_value for act_id equal to p_act_id. We log the message p_act_id, 'Debited.' Then we place the order by inserting the orders table. We log the message p_act_id, 'Order Placed' and then COMMIT. If there is an error signaling the insertion in the orders table, the execution will flow to the EXCEPTION block where we log the message again with p_act_id and SQLERRM and then ROLLBACK finally raising the error to the calling client. Let's compile this procedure. Let us first SELECT * FROM the log_table. There is nothing in there right now. Let us now execute the process_order procedure passing in account of 1, an invalid item ID 3, with an item value of $200.00. Let's execute this procedure. Since the item ID is invalid and we have a referential integrity constraint on the orders table linking it to the item ID of the items table, the insertion fails making it go to the exception section where everything is rolled back. At this point of time if you query the log_table there is nothing in the table as the rollback statement rolled it back. However, we wanted the log information to stay even though the main transaction was rolled back. This can be easily accomplished using the autonomous transaction. Let us go back to our log_msg procedure and add the PRAGMA AUTONOMOUS_TRANSACTION at the top. We have to end it with a COMMIT. Let's compile it again. Let us run the procedure again with the same parameters. It again failed due to referential integrity violation during the insert to the orders table. Everything was rolled back. Let's query the log_table again. This time the information in the log_table was saved. This shows the simplicity and power of the autonomous transaction pragma.

Demo: Transaction Context With Autonomous Transactions

In this demo we will see how autonomous transactions maintain their own transaction context independent of the parent transaction. First let's delete everything from the orders table and the log_table, update the accounts setting the act_bal to $1000.00, committing the information, and then let's select from the accounts table. There are two accounts with ACT_ID 1 and 2. Here is the log_msg procedure again, which is an autonomous procedure. Let's first establish a savepoint called SAVEPOINT first over here. Then we insert in the log_table. Let's compile this. Next we have an anonymous block. Inside the execution section we first establish a SAVEPOINT called first. Notice that this is the same name we had established of the savepoint in the log_msg procedure. Then we INSERT in the ACCOUNTS table act_id 10. Next we call the log_msg autonomous procedure passing act_id 10 and a message 'Created.' After this we execute a command ROLLBACK TO first. Since the autonomous transaction does not share the transaction context with the main transaction, the savepoint inside it is totally different from the SAVEPOINT first in the anonymous block, so the command ROLLBACK TO first would take it back to the SAVEPOINT before the INSERT in the ACCOUNTS table and would not roll back the autonomous transaction, which is independent. Let's run it to confirm. Running again from the accounts table we see that there are still the two account IDs 1 and 2, and insertion for account ID 10 was rolled back. Selecting from the log_table we notice that the insertion in it was successful and committed. It was not affected by the rollback to savepoint in the anonymous block. So, this demonstrates that autonomous transactions do not share the transaction context like locks, resources, savepoints, etc. with the main transaction.

Creating Atomic & Independent Work Units

An autonomous transaction is an atomic unit of work. It allows you to encapsulate a piece of business logic, which can be then run atomically. For instance, if you wanted to have the placing of a single order considered as an atomic unit of work, you can call the process_order procedure as a PRAGMA AUTONOMOUS_ TRANSACTION. Now, each order is encapsulated as an atomic unit of work, so now what it can do is that if you have an orders queue then you can get the orders one by one in a loop using a cursor cur_get_order_queue, and then in the LOOP call the procedure process_order. You do not have to worry about committing orders which pass and rolling back the ones which fail as running the process_order procedure as an autonomous transaction takes care of that for us. We can call one autonomous transaction from within another autonomous transaction. Each one will work independently of the other, and they will not share the transaction context of each other. We can have our process_order procedure, which is defined with PRAGMA AUTONOMOUS_TRANSACTION call the log_msg procedure, which is also autonomous. Each one will be committed or rolled back independently.

Demo:Creating Atomic & Independent Work Units

In this demo we will demonstrate another common usage for autonomous transactions with the help in encapsulating business logic and forming an atomic unit of work. We will also see how autonomous transactions can be nested and their behavior. We will be using the orders_queue table we had created in the transactions module and show how autonomous transactions can help simplify the logic and make it accurate. Let's first SELECT * FROM the orders_queue table. We see that there are three orders. There's an order from account ID 1 for item ID 1, account ID 10 for item ID 2, and account ID 2 for item ID 1. Account ID 10 does not exist. Let us now delete everything from the orders table and the log_table and commit. Here is our process_order procedure. We have made it autonomous by including the PRAGMA AUTONOMOUS_TRANSACTION at the top, so processing a single order becomes an independent atomic unit of work. It again takes in the act_id, item_id, and item_value parameters, debits the account, and then inserts in the orders table and then commits. Using the autonomous log_msg procedure it logs the act_id and 'Order Successful' message. In case of an error it goes to the EXCEPTION block where everything is rolled back, and using the autonomous log_msg procedure we log act_id and the message 'Failed' concatenated with the SQLERRM function. This will demonstrate nesting of autonomous transactions. The process_order procedure is an autonomous procedure, and it calls the log_msg procedure, which is also autonomous. Let's compile this procedure. Now, here's an anonymous block where we have declared a CURSOR cur_get_order_queue, which selects the queue_act_id, queue_item_id, and item_value from the orders_queue table joining it with the items table on the item_id. Inside the execution section we open a cursor for LOOP and get the information in the cur_info val. Inside the LOOP we call the autonomous procedure process_order passing the queue_act_id, queue_item_id, and item_value. Running it this way we do not have to worry putting savepoints to roll back unsuccessful orders. We expect the first and third orders to process and the second one to fail. Let's run this anonymous block. Now let us look in the ORDERS table. As expected we see that orders for account ID 1 and 2 were inserted successfully, but not for account ID 10. Let us now select from the log_table. We see that it logs the successful completion message for account ID 1 and 2 and also logs the failure for account ID 10 with the error message regarding integrity constraint violation, so this demonstrates how an autonomous program can be called from within another autonomous program and how autonomous transaction pragma helps us create independent atomic unit of work.

Summary

In this module we talked about what autonomous transactions are. They are transactions which can be launched independently from within a main transaction. They can be defined in outer level anonymous blocks, standalone procedures and functions, packaged procedures and functions, database triggers, and object methods. It has an amazingly simple syntax and allows us to achieve functionality, which otherwise will require very complex coding and setup. It has use in logging and auditing applications, tracking program usage, retry counters, and in general encapsulating business logic to create independent atomic unit of work. We talked about some considerations like having a commit or rollback to end an autonomous transaction and that any unhandled exceptions will cause the changes to be rolled back. Autonomous transactions do not share transaction context like commits, rollbacks, and savepoints with the main transaction.

Native Dynamic SQL

What Is a Dynamic SQL?

Hi. Welcome to Pluralsight. My name is Pankaj Jain, and welcome to this module on Native Dynamic SQL. Dynamic SQL, as the name indicates, allows you to write SQL, which is dynamic. The SQL statement is determined at runtime based on say user input parameters or some processing logic. It helps us write extremely flexible and reusable code. In this module we will take a look at native dynamic SQL, one of the ways of executing dynamic SQL. We will talk about advantages and some common usage scenarios for dynamic SQL. We will see how to perform selects, inserts, updates, and deletes, as well as execute DDL statements and anonymous blocks inside PL/SQL using dynamic SQL. We will also talk about SQL injection risks associated with using dynamic SQL and some best practices related to that. In the next module we will take a look at DBMS_SQL package, another way of executing dynamic SQL, which has some unique advantages compared to native dynamic SQL. Knowing how to write dynamic SQL is very important to be fully proficient with PL/SQL and to take care of various usage scenarios and business needs, so let's take a closer look at this important topic. So, what is dynamic SQL? As we said earlier for dynamic SQL, the SQL statement is not known until runtime. Let us understand this by comparing a code snippet which has static SQL versus a code snippet which has dynamic SQL. So, here is a simple function get_count, which takes in a parameter p_act_id of type accounts.act_id%TYPE and returns a number. It has l_count, a number variable. Inside the execution section is a SQL statement SELECT COUNT(*) INTO l_count FROM orders WHERE order_act_id is equal to p_act_id. Now, this is static SQL, which we have been using so far. The select clause, the table and the from clause, and the where conditions are all known to the compiler when it goes to compile the function. Compare this with this function redone for dynamic SQL. It takes in a parameter p_where of type VARCHAR2, which is the user-defined where condition not known until runtime. Again, it has l_count variable of type NUMBER, but here l_select variable is defining the select clause of 'SELECT COUNT(*)', l_from is defining the from clause of 'FROM orders', and then we have a variable l_query of type VARCHAR2(200). Inside the execution section we are building the query string l_query, concatenating the variables l_select, l_from, and the where condition using the parameter p_where. Using EXECUTE IMMEDIATE, which is the native dynamic SQL syntax, we run this query and get the result in l_count. Do not worry too much about the native dynamic SQL syntax as we will talk in more detail about it soon. So, as you can see here, the SQL is assembled at runtime using variables, so until the time this block is run the compiler has no idea as to what SQL is going to be run. Since the select clause, where clause, etc. are all variables, you can change their values anytime in the code based on sudden conditions giving you the flexibility to run different queries for different conditions. This is dynamic SQL.

Static vs. Dynamic SQL

So having taken a brief look at dynamic SQL, let us compare it with static SQL. Some of the obvious differences are that for static SQL the compiler knows the SQL at compile time versus for dynamic SQL where the compiler does not know of the SQL until runtime. Since the compiler knows the SQL at compile time, it can verify the object references and do the syntax checks during compilation, which it cannot do for dynamic SQL, so the chances of runtime errors are more in case of dynamic SQL if invalid objects or syntax is specified using variables. Similarly for static SQL the compiler can verify the object privileges during compilation, which it cannot verify in the case of dynamic SQL. With static SQL, the PL/SQL compiler does early binding during compile time. Early binding is the process by which it allocates space for the identifiers, checks for permissions, valid object references, etc. Since Oracle Database allows the ability to compile and store the program units in the database even though early binding requires a longer compile time, this is an efficient model for faster runtime execution. With dynamic SQL it does a late binding where the compiler passes the SQL statement at runtime. Since we have to specify everything at compile time, it makes static SQL less flexible, and this is where the major advantage of using dynamic SQL comes in. It is extremely flexible and creates highly reusable code. Since the checks and verifications are done at compile time, static SQL is faster than using dynamic SQL where these validations are done at runtime. So, in general, if you do not need a degree of flexibility and dynamism in your code, which dynamic SQL provides, you should stick with using static SQL. If you see yourself doing a lot of code duplication, say in the previous example of the get_count function if you were creating several versions of the same function based on different where conditions or using a lot of hard-worded if's to use different versions of queries, then it might make sense to use dynamic SQL.

Common Usage Scenarios

Let us look at some of the common usage scenarios where dynamic SQL might be useful. It is useful where queries are dynamically changing. Say you have a web application where you allow the users to specify the query criteria for the information they want to see against your table then it might make sense to create a dynamic function which takes in a select clause and a where clause as parameters allowing us to reuse it for any variant of user-specified query criteria. Dynamic sorts is another place where on the web page you might want to allow the users to specify the sort criteria for the information requested. Then we can from the get-go get it in the right order from the database function. Dynamic SQL would enable us to do this easily. You might have modular functions or procedures you want to execute for certain conditions. You can do hard-coded ifs and else ifs to call them, but then if you ever change them very often it would be a lot of work. Instead, you can store the condition criteria to the subprogram name mapping in the table, which can read to get the subprogram name for a given condition and execute it dynamically. Sometimes you might use naming conventions to build a function or procedure name based on conditions or input data and then execute them. For example, if you want to process order in a specific way for each customer and you have created versions of procedures say called process order customer A, process order customer B, etc, then based on the passed customer name you appended to the process order string to build a procedure name and then invoke it dynamically. Oracle also allows you to specify hints while running queries, which can alter the way the query is run by the optimizer in order to make it more efficient based on the information you know. Using dynamic SQL you can pass the hint as an input parameter, construct the SQL statement using that hint, and execute it. DDL statements like create or drop commands can be run in PL/SQL only using dynamic SQL. Traditional static PL/SQL only allows DML statements and transaction control statements as these do not alter objects or privileges thus enabling early binding. DDL statements can potentially alter these; thus, these are not allowed except by using dynamic SQL where the binding does not have to be done until runtime. Dynamic SQL is useful for tools who rely heavily on looking up information from Oracle data dictionary to build up SQL statements or execute procedures, etc. dynamically. Dynamic SQL is also useful in scenarios like where the table name or the schema object names in the database are frequently changing and you want to use a pattern based on certain conditions or user input to construct them dynamically and invoke them. These are just some of the common usage scenarios I can think of, but there are many more which you might discover while working on business problems where dynamic SQL can impart you a lot of ease and flexibility.

Native Dynamic SQL Syntax Advantages & Syntax

There are two main ways to invoke dynamic SQL, the native dynamic SQL, a small snippet of fetch we saw earlier, and then the other method is by using the DBMS_SQL package. Native dynamic SQL was introduced later starting with Oracle version 8i. Each one of these ways has its own advantages and drawbacks. We will talk more about the DBMS_SQL package in the next module. Why should you use native dynamic SQL? One major advantage of native dynamic SQL is that it is very easy and simple to use. It allows us to place the dynamic SQL statements directly in the PL/SQL code, and its syntax is relatively simple to understand. It provides better performance than using the DBMS_SQL package. In addition to providing a rich set of built-in data types, Oracle Database also allows users to define their own data types if they need to. Native dynamic SQL supports all the user-defined data types and collections which DBMS_SQL package did not support until version 11g. Native dynamic SQL supports fetching into records. Records allow us to logically group related items. We will talk about the advantages of using DBMS_SQL package in the next module when we talk in detail about it. Let us look at the syntax of the EXECUTE IMMEDIATE statement, which is used for native dynamic SQL execution. It starts with the EXECUTE IMMEDIATE keyword followed by the dynamic_sql_string. This is the string you build based on parameters, variables, etc. Next is the optional INTO clause. This is used for single row select statements to fetch the return value INTO select variables or a PL/SQL record. Following this is the USING clause by which you provide bind arguments for any bind variables you might have placed in the dynamic_sql_string. Bind variables are placeholders inside your SQL string for which you provide value by bind arguments with a using clause. They are optional to use. If you have no bind variables, you do not have to use these. They have a default mode of IN, but can also be passed in the OUT mode to receive values or IN OUT mode to pass and receive values. Then there is the RETURNING OR RETURN INTO clause, which is used to capture the values returned by the DML statements having the RETURNING clause into bind variables. These are again optional to specify. We will take a look at the various components of the syntax as we walk through various examples. For multi-row selects along with using the bulk collect feature with the EXECUTE IMMEDIATE statement you can also use ref cursors. We talked about them in the part 1 course, but here we will take a look to see how we can pass bind variables, etc. while calling them.

DDL Operations

Let's first take a look at to how to execute DDL statements using native dynamic SQL. Let's start with creating objects like tables. We are defining a procedure create_table, which takes in the table_name and the table_columns as input parameters, so using this procedure we can dynamically create tables. Inside the execution section we create the table by simply showing EXECUTE IMMEDIATE followed by the 'CREATE TABLE' string. The 'CREATE TABLE' string is constructed by concatenating 'CREATE TABLE' with p_table_name, which is concatenated with the p_table_columns. Notice how simple the syntax is. This one line command is all that is needed to create the table. So, an example of using this procedure is by issuing EXEC create_table passing in the table name of 'ORDERS_QUEUE_CA' and the columns as a string queue_id NUMBER, queue_act_id NUMBER, and queue_item_id NUMBER within parentheses. At runtime all these pieces are concatenated together to create a valid table create statement and execute it with the EXECUTE IMMEDIATE command. Before we go further, let's talk about how privileges are resolved in native dynamic SQL. Only direct grants work inside stored procedures and privileges granted via roles are disabled during compilation. So, for instance, say user demo created a create_table_procedure. With the EXECUTE IMMEDIATE statement inside the procedure, a 'CREATE TABLE' command is issued. Let us say that the user demo has not been granted direct CREATE TABLE privilege. We create a role called create_table_role and assign the CREATE TABLE privilege to this role. We assign this role to the demo user. When user demo tries and executes the procedure using the code snippet, it'll get an error for insufficient privileges since the user demo does not have the CREATE TABLE privilege assigned to it directly. So, to make it work there are two options. Prior to Oracle 12c you would have to GRANT CREATE TABLE privilege to the user demo directly. Then the execution would be successful. Starting with Oracle version 12c you can assign roles to procedures, so we can assign the create table role to the create table procedure. This is called code-based access control, and with that the privileges checked for dynamic SQL will be the privileges granted directly to the owner of the subprogram, as well as the privileges granted via the role to the subprogram unit. This would help limit a number of direct grants we need to give to the subprogram owner. So, if you wanted the users in our system to be able to create tables only using the create_table_procedure and not directly, you will not grant the CREATE TABLE privilege to them. They just need to have the execute privileges on the create_table_procedure. Since the procedure has the CREATE TABLE privilege assigned to it via the role, it can do the necessary job of creating the table. Let us now see an example of how we can create a dynamic procedure, which can be used to drop tables. This procedure called drop_table takes in the table_name parameter, which is the table name to be dropped. Inside the execution section it issues a native dynamic command EXECUTE IMMEDIATE 'DROP TABLE' concatenated with the p_table_name to drop the table. An example to use it would be EXEC drop_table passing in the table name ORDERS_QUEUE_CA.

Demo: Executing DDL Statements

Let us see dynamic SQL for DDL statements in action. Here is a procedure create_table, which takes in the name of the table to be created p_table_name and the columns p_table_columns. We have a VARCHAR2 variable l_sql to hold the SQL. Inside the execution section we can start the SQL as CREATE TABLE concatenated with p_table_name concatenated with p__table_columns. Using DBMS_OUTPUT we print the SQL statement, and then we issue the statement EXECUTE IMMEDIATE l_sql to execute the DDL statement. Let's compile this procedure. Now let us execute the procedure as EXEC create_table passing in the table name ORDERS_QUEUE_CA and its columns in the right format for a create_table statement within brackets separated by commas with a data type specification. Let's run this. We get an error for insufficient privileges. This is because for dynamic SQL the privileges have to be directly granted to the demo user. Here is another session where we are logged in as a DBA user. Let's run the statement grant create table to demo. Now let's try and execute the create_table procedure again. This time it is successful. In the output window we see the l_sql variable, which is a valid create_table statement with all the codes. Let us describe the table by issuing the command desc orders_queue_ca. It comes back with information, and so the table indeed got created. Let us now create a procedure to drop a table, the name of which is passed in as a parameter. Inside we simply issue EXECUTE IMMEDIATE 'DROP TABLE' concatenated with the p_table_name. Let's compile this. Now let us drop the table we created earlier by issuing EXEC drop_table('ORDERS_QUEUE_CA'). Now when we try and describe the table we get an error that it does not exist.

Executing Single Row & Multiple Row Selects

Here is an example of doing a single row select using the native dynamic SQL. As I mentioned before, the using clause is optional if there is no bind variable. So, here in this code snippet we are creating a generic function, which gets the count of a table passed in as a parameter. It returns the count as a number. We declare l_count to hold the count and l_query to hold the query. Inside the execute section we define l_query as 'SELECT COUNT(*) FROM' and concatenate it with the p_table name passed in as the parameter. Then we execute the SQL string using native dynamic SQL as EXECUTE IMMEDIATE l_query INTO l_count. INTO l_count fetches the selected count in the l_count variable. Pretty simple, right? Now we can execute this function by passing the orders TABLE parameter in this code snippet, which will return the count of items in the ORDERS table. We can then call the same function passing in the ITEMS table as a parameter to get the count of the ITEMS table. Let us look at an example of using both the into clause and the using clause with select statements. Here is a function get_order_count, which gets the count of orders from the orders table for the dynamic condition passed in of p_column parameter having a value of p_value. Inside we again declare l_count to hold the count and l_query to hold the query. Inside the execution section we form the query by concatenating 'SELECT COUNT(*) FROM orders WHERE' p_column concatenated is equal to :col_value. Putting a colon in front of an identifier makes it bind variable, and it is inside the coded string. Then we issue the dynamic SQL as EXECUTE IMMEDIATE l_query INTO l_count USING p_value. So, INTO l_count fetches the selected count in the l_count variable, and using p_value clause we supply the value for the bind variable :col_value. The bind variable can have any name, and we can have more than one. In that case we can pass their values separated by commas in the using clause. Now in the next code snippet we invoke the get_order_count function passing in the column order_act_id having a value of 1. So, this will translate into a query of SELECT COUNT(*) FROM orders WHERE order_act_id is equal to 1. In the next code snippet we pass the column order_item_id having a value of 2, which will translate to a query of SELECT COUNT(*) FROM orders WHERE order_item_id is equal to 2. So, this allows us to create a very flexible function, which takes in user-specified criteria to return the results. We can further expand the parameters to take in more column value pairs, and this can be useful say for generating reports with user-specified filters and can be the backend behind a self-service reporting web page. We looked at the example of passing the table name earlier for dynamic counts. I just wanted to mention that when we build SQL strings we have to concatenate the schema object names and column names versus passing them as bind variables. For instance, here instead of concatenating the table name if we had to pass it as a bind variable with the USING clause Oracle will raise an error of ORA 00903 of invalid table name. Similarly, instead of concatenating the column name either in the condition or in the select clause to the SQL string, if you try to pass it in as a bind variable Oracle will raise errors. Bind variables should be typically used for supplying values to be compared for column conditions or for inserting or updating column values in DML statements for returning into clause or as parameters in subprograms. So, here is one typical use of bind variables in a select statement. The passed column name p_column in the WHERE clause is concatenated, but the value is passed as a bind variable :col_value. Later we supplied the bind argument with the USING p_value clause. Let us also see how we can improve the performance of our dynamic SQL statement. If you look in the second code snippet, here we are concatenating both the p_column and p_value. This is also valid. We do not have to use a using clause in the EXECUTE IMMEDIATE statement here. So, both the ways as in the first code snippet and the second code snippet are valid, but using it with bind variable in the first code snippet is more efficient. In static SQL when you write insert, update, and delete statements Oracle automatically treats the values as bind variables to improve SQL performance. However, in case of dynamic SQL, you have to do it manually. If you do it using bind variables as in the first code snippet, Oracle can reuse the same cursor for different values of p_value; however, if you do it the second way without using binds Oracle will have to open a new cursor for each different value of p_value, which is not very efficient. As far as multi-row selects are concerned, you can do it in two ways. We can use the execute immediate statement with the bulk collect feature and fetch them in collection types. Since we haven't covered bulk collect features and collections yet, we will not cover that option right now, but will cover it in a later course. The other way to do multi-row selects is by using ref cursors. Let us take a look at this option. In this example we will see how we can use ref cursors to create a dynamic procedure called apply_fees, which is used to apply a monthly account fee to the accounts. It takes two parameters, p_column and p_value through which we can specify the where condition to filter out the accounts where we want to apply the fees. If you pass in null to these, it'll apply the fees to all the accounts. You first start by defining a ref cursor by declaring TYPE cur_ref IS REF CURSOR. Next we declare a variable of this type called cur_account, l_query is VARCHAR2(400) to hold the query string, and l_act_id is of type accounts.act_id%TYPE. We begin the execution section by building the query string as 'SELECT act_id FROM accounts'. Next we check IF p_column was passed in. If it not null, we further build a query string concatenating 'WHERE' p_column equal to :pvalue. We have passed in pvalue as a bind variable, which is an efficient way to do it. Then we open the ref cursor as OPEN cur_account FOR l_query USING p_value. P_value provides the value for the bind variable. However, if p_column was not specified, then our l_query is just 'SELECT act_id FROM accounts,' and we open cur_account FOR l_query. Now from this point on we can do our normal cursor processing running it in a LOOP fetching the account ID into l_act_id. We EXIT WHEN cur_account% is NOT FOUND, so we can use all the regular cursor attributes over here. Then we UPDATE the accounts debiting the act_bal by $10.00 WHERE act_id is l_act_id and then COMMIT. So, this way using ref cursors we can process multi-row selects for dynamic select statements. Alright, let us now take a look at an example of fetching into records with dynamic SQL. We covered records in the Oracle PL/SQL Fundamentals Part 1 course, but just to recap records allow us to group related variables together. Using the %ROWTYPE syntax we can create records, which contain an element for each table column or we can create our own record with user-specified elements. We will see how to create each one and use them with dynamic SQL statement both for single row select and multi-row selects. So, here we are creating a procedure initiate_order, which takes in a user-specified optional where clause. We first define type cur_ref as REF CURSOR, and then we define cur_order as type cur_ref. Next we define order_rec as a user-defined record containing two elements, act_id of orders_queue.queue_act_id%TYPE and item_id of orders_queue.queue_item_id%TYPE. We then declare l_order_rec of type order_rec. L_item_rec is a record of items%ROWTYPE. L_query will hold the query. So, inside the execution section we start l_query as SELECT queue_act_id, queue_item_id FROM orders_queue and then concatenate it with the p_where clause. Using the ref cursor syntax we open cur_order FOR l_query and in a LOOP FETCH cur_order INTO l_order_rec. So, both the queue_account_id and queue_item_id will be assigned to the corresponding columns of l_order_rec. And next we define the exit condition for the LOOP, which is EXIT WHEN cur_order%NOTFOUND. Now we do a single row select into a record by issuing EXECUTE IMMEDIATE SELECT(*)FROM items WHERE item_id is equal to :item_id INTO l_item_rec USING l_order_rec.item_id, which is the item ID fetched in the order record earlier. And then we call the process_order procedure passing in the various values from the two records like l_order_rec.act_id, l_order_rec.item_id, and l_item_rec.item_value and then END the LOOP.

Demo: Single Row & Multi-Row Select

In this demo we will see how to do single row and multi-row selects using dynamic SQL, as well as see how to fetch into records. So, here is a procedure initiate_order, which is used to initiate orders from the orders_queue table, and it takes in an optional p_where clause to limit the orders it picks up from the queue table for processing. We start off by declaring a ref cursor as type cur_ref IS REF CURSOR. We declare cur_order as of type cur_ref. Next we declare a record order_rec having two columns, act_id of type orders_queue.queue_act_id%TYPE, and item_id as orders_queue.queue_item_id%TYPE. L_order_rec is of type order_rec. L_item_rec is another record of items%ROWTYPE. L_query will hold the query. Inside the execution section we start off by building the query as 'SELECT queue_act_id, queue_item_id FROM orders_queue ' and concatenated with the optional p_where clause. Then we print the l_query using dbms_output statement. Next we open the ref cursor cur_order FOR l_query. In the LOOP we fetch it in the l_order_rec, so queue_act_id and queue_item_id will go in the corresponding columns in the l_order_rec. We EXIT WHEN cur_order%NOTFOUND. Now we do a single row select using EXECUTE IMMEDIATE SELECT * FROM items WHERE item_id is equal to :item_id INTO l_item_rec USING l_order_rec.item_id as the bind argument. Then we call the process_order procedure passing in the appropriate values l_order_rec.act_id, l_order_rec.item_id, and l_item_rec.item_value. We had created this procedure in the transactions module. This procedure debits the account ID for the item value and then inserts the order in the orders table. We END the LOOP. Let's compile the procedure. Let's first select from the orders_queue table. There are two orders, one for account ID 1 and the other for account ID 2. Next let's delete everything from the orders table, update the accounts with an act_bal of 1000, and commit. Now let's execute the procedure initiate_order passing in the filter where condition of WHERE queue_act_id is equal to 1. We see the query as it is executed in the output window as SELECT queue_act_id, queue_item_id FROM orders_queue WHERE queue_act_id is equal to 1. Let's select from the ORDERS table now. The order for account ID is indeed over there. Let's now again delete from the orders table and commit. Let's now run it again without a where condition passing in a NULL. This time it will have no where condition, and it will pick up everything from the orders_queue table. Let's select from the ORDERS table again, and this time we see orders placed for both account ID 1 and 2.

Executing DML Statements

Let us see how we can use native dynamic SQL to execute DML statements. Let us start with the insert statement. Let us say we create a generic procedure insert_record to insert a row in a two column table. It takes in the table name p_table_name, the first column name p_col1_name and its value p_col1_value, the second column name p_col2_name and its value p_col2_value. Then we use the EXECUTE IMMEDIATE statement INSERT INTO, concatenate the p_table_name, open the brackets, concatenate p_col1_name, comma, p_col2_name, close the brackets, the VALUES clause followed by :col1_value,:col2_value brackets close, and then the USING clause to supply the bind arguments for the two bind variables as p_col1_value,p_col2_value. We then COMMIT. As we had mentioned earlier, we are concatenating the column names, however, using bind variables to supply the column values. When you are passing bind variables in EXECUTE IMMEDIATE statements, you have to pass values or bind arguments equal to the number of bind variables even though there might be repetition. For every bind variable there has to be a corresponding bind argument either in the using clause or returning into clause and defined variables in the into clause. This is true for all kinds of statements except when we're executing anonymous blocks. We will talk about them later. So, for instance, here in the INSERT statement even though we are inserting the same bind variable :col1_value for both p_col1_name and p_col2_name we have to pass the value twice in the USING clause. Since there are two bind variables, we pass the value twice. Native dynamic SQL supports all the SQL datatypes like _____ date, number types, etc. It did not support the PL/SQL only datatypes like Boolean, etc. until Oracle 12c where it now supports a Boolean, associative arrays with PLS_INTEGER indexes, and composite types like records and collections declared in the packet specification. You cannot use the literal NULL as a bind argument. So here if you want to pass a null for the :col2_value, we cannot pass it as a NULL literal. Rather, we will have to declare a variable, which is initialized to a null value by default and pass it as a bind argument. Let us see how we can execute a dynamic update statement. Let us say we have a procedure update_record, which takes in the table name as p_table_name, the column to be updated as p_col1_name, the update value for that column as p_col1_value. P_where_col is the column in the WHERE condition, and p_where_value is its value. So, inside the execution section we construct the update statement as EXECUTE IMMEDIATE UPDATE concatenated with p_table_name concatenated with 'SET' p_col1_name equal to :p_col1_value WHERE p_where_col equal to :p_where_value. Since :p_col1_value and :p_where_col are bind variables, we supply their values by the using clause USING p_col1_value and p_where_value. We are concatenating the table name and column names and passing the values using bind variables. This one statement will construct the SQL and execute it also. The RETURNING INTO clause is used for DML statements with a return value using the RETURNING clause. So as in this example we have an update statement, which after updating returns the act_cust_id and act_bal. So, here we declared l_item_value as items.item_value%TYPE and given it a value of 100. L_act_id is accounts.act_id%TYPE with a value of 1. L_cust_id and l_act_bal will hold the return values from the update statement. Using EXECUTE IMMEDIATE we issue UPDATE accounts SET act_bal is equal to act_bal - :p_item_val WHERE act_id is equal to :p_act_id RETURNING act_cust_id,act_bal INTO :l_id, :l_bal. With the USING clause we supply the values for the bind variables, l_item_value for :p_item_value, l_act_id for :p_act_id. The values we return are received using RETURNING INTO clause. :l_id is received into l_cust_id, and :l_bal into l_act_bal, so the name of the bind arguments need not be the same as bind variables. We need to have a corresponding bind argument for each bind variable with the USING clause and the return bind variables with a RETURNING INTO clause. Instead of using the RETURNING INTO clause, you could also receive the variables returned by the RETURNING clause using OUT mode bind arguments as shown in this code snippet. Instead of returning into l_cust_id l_act_bal, you could do OUT l_cust_id, OUT l_act_bal, but it is recommended to use the RETURNING INTO clause. Let's see how to issue a dynamic delete statement. Here is a procedure delete_table, which takes in the table name to delete as a parameter p_table_name. Inside the execution section we issue EXECUTE IMMEDIATE 'DELETE FROM' concatenated with p_table_name. This one line command will construct and execute the delete statement.

Executing Anonymous Blocks, Procedures & Functions

Using native dynamic SQL we can also execute anonymous blocks. Let us see how we can use the IN OUT and the OUT mode for bind variables. For instance, here is a code snippet where we are trying to perform some calculation in an anonymous block. We have declared l_sql as VARCHAR2(500). L_inout is a NUMBER with a value of 1. L_inout will be an IN OUT mode bind argument. L_out is a NUMBER with a value of 2. L_num is a NUMBER with a value of 1. Inside the execution section we start constructing l_sql. We start with BEGIN :l_inout equal to :l_inout + :l_num multiplied by 2. L_out, an OUT type of variable, is :l_num divided by 2. This is followed by the END keyword, a semicolon, and the close quotation mark. Then there is another semicolon to end the l_sql statement. Now we say EXECUTE IMMEDIATE l_sql USING IN OUT l_inout. So, l_inout variable is used in the IN OUT mode, and its value will be passed in and the new value passed out. L_num is passed in the default IN mode. L_out is in the OUT mode, and its value will not be passed in, but the calculated value will be passed out and would overwrite its value. Also notice that for an anonymous you end it with a semicolon inside the string. We did not have to put the semicolon inside the string for other statement types. Secondly, for duplicate placeholders, for example the l_num bind variable, which is there two times, we just pass it to value once. This is unlike other statements where we pass the same number of bind arguments as the bind variables even though there might be repetition. For anonymous blocks we pass only distinct values in the right order. So, :l_inout occurs first, so we pass it first in the mode we want. :l_inout appears again, but we do not have to pass its value again. And next is the next distinct bind variable :l_num for which we pass the bind argument l_num. And next is the next distinct argument allowed for which we pass the bind argument l_out. Since we have already passed a bind argument for l_num earlier, we do not have to pass it again. A VARCHAR2 PL/SQL variable can have a maximum length of 32K or 32,767 bytes. Prior to version 11g the maximum length of string native dynamic SQL could pass was 64K by concatenating two VARCHAR2's together as shown in the first code snippet. However, starting with version 11g native dynamic SQL can pass a CLOB, which can hold up to 128 terabytes, so this length restriction is no longer there. In the second code snippet we declare l_sql as a CLOB type and use it for the EXECUTE IMMEDIATE statement. You can execute procedures in two ways using the call statement or by using anonymous blocks. Let us say we have a procedure calculate_tier, which takes in three parameters p_act_id in the default IN mode of accounts.act_id%TYPE, p_act_bal in the IN OUT mode of accounts.act_bal%TYPE, and p_tier in the OUT mode of type NUMBER. In the first code snippet we use the CALL syntax to call the calculate_tier procedure passing in the three parameters as :act_id, :act_bal, and :tier and pass them in their respective modes with the USING clause, l_act_id in the IN mode for the first parameter, l_act_bal in the IN OUT mode for the p_act_bal, and l_tier in the OUT mode for the p_tier parameter. It is important to pass the parameters in the right order and mode and compatible data types. The second way of executing it is by using the anonymous block with the EXECUTE IMMEDIATE statement, which starts with the BEGIN keyword. We then call calculate_tier again passing in the three bind variables and then ending it with the END and a semicolon. With the USING keyword we pass the three parameters in the right mode. We can execute functions using the anonymous block. Here is a function get_tier, which takes in p_act_id in the IN mode, p_act_bal in the IN OUT mode, and p_tier in the OUT mode. So, in the anonymous block we declare l_act_id, l_act_bal, l_tier, and l_out variables, and then using EXECUTE IMMEDIATE start the anonymous block with the BEGIN keyword. We hold the value returned by calling the get_tier function :l_out variable. We pass the three parameters as bind variables. With the USING clause we go from beginning to end looking at distinct variable names. As with anonymous blocks, l_out bind variable is mapped to l_out variable in the OUT mode as it is a returned value, bind variable act_id is mapped to l_act_id in the default IN mode, act_bal bind variable is mapped to l_act_bal in the IN OUT mode, and bind variable tier is bound to l_tier in the OUT mode.

Demo: Executing Procedures

In this demo we will see how to execute a DDL statement, a DML statement, and a procedure using dynamic SQL. We will also see the RETURNING INTO clause in action. First let's create a sequences queue_seq to insert the primary key queue_id in the orders_queue table. Let us say we want to build a function which takes in the location from where the order initiated like Washington, California, etc. along with the account placing that order for the item_id for which the order was placed. We first declare a few variables l_count, l_id, and l_item_val, all numbers. We declare an EXCEPTION no_table_exists, and then using PRAGMA EXCEPTION_INIT map it to the error code -942. We then declare a CURSOR get_item_val to get the item_value FROM the items table for the passed item_id. Say in our system we want to create an orders_queue table for each location like orders_queue_ca for California, orders_queue_wa for Washington, etc. So, if it the first time receiving an order for a location, we create a table for that location also in our function. So, that is happening in the first part of the execution section where we try and take the count of rows from the table orders_queue_ appended with the passed p_loc INTO l_count. This is enclosed in a block of its own so that in case the table does not exist there will be an exception with error code -942 raised, which using PRAGMA EXCEPTION_INIT we have mapped to no_table_exists exception. So, if it is the first time an order is placed for a location, we create the table in the EXCEPTION section using EXECUTE IMMEDIATE 'CREATE TABLE orders_queue_' concatenated with p_loc and the columns queue_id, queue_act_id, and queue_item_id. Having taken care of that, outside the block we issue a dynamic insert with EXECUTE IMMEDIATE 'INSERT INTO orders_queue_' concatenated with p_loc to form the table name, its columns, followed by the values as bind variables :p_id, :p_act, and :p_item with the RETURNING clause returning queue_id INTO :l_id. Now with the USING clause we supply the bind arguments queue_seq.NEXTVAL for queue_id, p_act_id for :p_act, and p_item_id for :p_item RETURNING INTO l_id for :l_id. We print the returned l_id. Next using the get_item_val cursor we FETCH the item_val into l_item_value for the p_item_id. Next we use native dynamic SQL to call the process_order procedure passing in the parameters as bind variables and supplying their values using the bind arguments p_act_id for :p_act_id, p_item_id for :p_item_id, and l_item_value for :p_item_value. We return l_id and any function. Let's compile this. Now let's go see if the orders_queue table for California exists. Select * from orders_queue_ca. No, it does not. Also, let's delete from the orders table and commit. Let's now run the generate_order function for the location California for account ID 1 and item ID 1. It was executed successfully. Let's select from the orders_queue_ca table now. We see that the table was created and a row was inserted into it. Now let's select from the orders table. Indeed the order was placed for account ID 1.

Hints, Session Control Statements & Invokers Right

We can also use dynamic SQL for query optimization by passing in appropriate hints to aid the optimizer choose the right execution plan based on the information we know. For instance, here is a procedure apply_fees, which takes is a where column, p_column, and its value, p_value, and a hint, p_hint. So, we can append the hint to the SELECT statement, which is then executed using a ref cursor. In the next code snippet we have an example of calling this procedure with p_act_id column with a value of 1 and a hint to execute the query in PARALLEL on accounts table with a degree of 3. The hint starts with a slash star and a plus and ends with a star and a slash, so in this way we can overwrite the default PARALLEL degree for the accounts table. Only using dynamic SQL can you specify session control statements in PL/SQL directly. So, for instance, say we want to issue the command ALTER SESSION SET NLS_DATE_FORMAT equal to 'DD-MON-RRRR' inside a PL/SQL procedure called set_date_format. We pass in the desired date format called p_format, and then using EXECUTE IMMEDIATE issue ALTER SESSION SET NLS_DATE_FORMAT equal to p_format concatenated. The way we call this procedure is as EXEC set_date_format and the format DD-MON-RRRR. You might be noticing three quotes at the beginning and three at the end. Since we are trying to pass the string value as code, DD-MON-RRRR code, the first quote is for the start of the string literal. The next two quotes will translate to a single quote as the first quote will be the escape character. Similarly, the first two ending quotes will translate to a single quote, and the last quote is to end the string literal. Using a combination of invokers right and dynamic SQL, you can create very generic centralized sub-programs, which can be called by any user and will act upon the schema of the calling user. For instance, the create_table procedure, which creates a table, is defined as an invokers right with AUTHID CURRENT_USER clause, and so it can be a generic procedure, which can be called by any user to create a table in its own schema. Similarly, the delete_table invokers right procedure uses dynamic SQL to delete data for the passed table in the invokers user's schema. You can use dynamic SQL to access programs or tables in remote databases using database links. For instance, say we have two database instances, db2 and db1. Db2 instance creates a database link, db1link, to connect to the demo schema in the db1 instance. Then inside the delete_table procedure inside db2 instance it can delete data for the past table name in the db1 instance by using the dblink. The dblink is appended to the table_name with an at sign followed by the dblink name.

SQL Injection

One risk with dynamic SQL is SQL injection, which is the way a hacker can pass string input to an application to run malicious statements or get unauthorized access. One way is through statement modification. For instance, here is a procedure to delete an order for a given column and its value from the orders table. Inside we build an l_query string as DELETE FROM orders WHERE p_column is equal to p_value, both p_column and p_value being concatenated. A normal execution would we say by calling delete_order procedure with a column order_act_id with a value of 1. This would translate to a statement DELETE FROM orders WHERE order_act_id is equal to 1 inside the procedure. But a malicious hacker can execute the procedure by passing order_act_id and a value of 1 OR 1 equal to 1. This will translate to a SQL statement DELETE FROM orders WHERE order_act_id is equal to 1 OR 1 is equal to 1. Since 1 is always equal to 1, the OR condition will evaluate to true, which will lead to all orders being deleted and not just for account ID 1. This is an example of how a hacker might modify the statement and cause unintended deletes. The way to prevent these kinds of attacks is to pass the value as a bind variable, which would also help improve performance. So, if the procedure is _____ in to pass p_value as a bind variable and supplies value with the USING clause, the normal execution will be fine as before, but the malicious execution will error out with ORA-01722 invalid number. The other way SQL injection can happen is through statement injection. This happens most often with dynamic anonymous block executions. So, for instance, if we have a procedure called calc, which takes in p_condition, we then build and anonymous block in which we have an IF clause concatenated with p_condition concatenated with equal to A. Again, two codes inside a string is a single code as one code is the escape character. If it is true, we execute procedure proc1. Now, a normal execution of this will be EXEC calc passing in A with quotes around it. This will build and execute a normal anonymous block where the condition will be compared as if A is equal to A. However, a malicious attacker may call this as EXEC calc 1 equal to 1, THEN DELETE FROM orders END IF, and then start another IF to continue the IF in the anonymous block. So, this will translate to IF 1 equal to 1 THEN DELETE FROM ORDERS END IF IF A equal to A THEN proc1, so the DELETE FROM ORDERS statement is injected by the string manipulation. The way to prevent these would be to have validations on the passed string and to check for occurrences of semicolon, END IF clause, etc, essentially anything which might not be normal. This would trap the error and raise it preventing malicious execution, so incorporating validations on the passed string is always a good idea. Some other things we might do to prevent SQL attacks is to be more explicit with date formats to prevent hackers from altering dates. These are some of the ways to reduce the risk of SQL injection in your code.

Demo: SQL Injection

In this demo we will demonstrate SQL injection by way of statement modification. Let's create a procedure delete_orders, which allows us to specify the WHERE clause using p_column and its value p_value passed in its parameters. We declare l_query. Inside the execution section we start constructing the query as DELETE FROM orders WHERE p_column concatenated is equal to p_value also concatenated. Using DBMS_OUTPUT.PUT_LINE we print the query. We then issue EXECUTE IMMEDIATE l_query to execute it, and then using DMBS_OUTPUT print the Rows Deleted using the SQL%ROWCOUNT function. Let's compile this procedure. Let's first select from the ORDERS table. There are currently two orders in it, one for account ID 1, and the other for account ID 2. Let's run the delete_order procedure passing in the WHERE condition column order_act_id having a value of 1. It forms the query as DELETE FROM orders WHERE order_act_id is equal to 1 and deleted one row. Let's ROLLBACK the delete. Now let's say a malicious user runs the procedure as delete_order order_act_id with a value of 1 OR 1 equal to 1. It translates to DELETE FROM orders WHERE order_act_id is equal to 1 OR 1 is equal to 1, which will be always true. This causes both rows to be deleted. Let's ROLLBACK. Now let's see how we can prevent this injection. Let's modify the procedure to pass p_value as a bind variable instead. Let's compile it. Let's execute the procedure with the malicious arguments again. This time it errors out. Let's run our normal execution now. It works fine with one row deleted. So, using bind variables not only helps with performance, but also helps prevent SQL modifications.

Summary

In this module we talked about dynamic SQL, which is SQL determined at runtime instead of compile time. The SQL is assembled at runtime based on the user input parameters or processing logic. Dynamic SQL helps build extremely flexible and reusable program units. They have a variety of use cases like building dynamic queries with user-specified filters, dynamic sorts, running DDL statements, building frameworks and engines just to name a few. Native dynamic SQL has a very simple syntax, and using the execute immediate command you can easily run within your PL/SQL code dynamic select, insert, delete, update statements, as well as DDL session control statements in anonymous blocks. Using dynamic SQL makes your code more vulnerable to SQL injection attacks. Using bind variables, validations, and explicit data type conversions are some ways you can minimize those risks.

Dynamic SQL Using DBMS_SQL

Introduction

Hello, and welcome to Pluralsight. My name is Pankaj Jain, and welcome to this module on Dynamic SQL Using DBMS_SQL package, which is an alternate way to execute dynamic SQL. It was the only way available to execute dynamic SQL until Oracle version 8. Oracle has upgraded this package with subsequent releases to handle collections, make it more secure, allow it to interoperate with native dynamic SQL with REF Cursors, as well as to use it to return implicit results to the calling client from within a procedure. In this module, we will take a look at the various APIs available with this built-in Oracle package. You will understand how to execute dynamic queries, DML statements, DBL statements, and anonymous blocks with it, as well as to see how it can interoperate with native dynamic SQL. You need to know about the DBMS_SQL package to maintain a lot of code in existing applications, as well as to use it for some tasks, which can be only done using this package, so let's get started. So why should you use the DBMS_SQL package? The DBMS_SQL can perform most of the dynamic SQL operations you can do using native dynamic SQL, plus there are certain tasks you can do only using this package. For instance, if you have an unknown number of select columns in a dynamic query, you can execute it only using this package. Similarly, if you have an unknown number of placeholders, or bind variables in your query, or DML statements, which are not known at the runtime, then you have to use this package to execute such dynamic statements. Using native dynamic SQL, you have to know the number of input and output bind variables in the code while compiling them. With Oracle version 12c, the DBMS_SQL package has been enhanced to return results of select statements to the calling client from within a stored param unit like a procedure, much like how T-SQL allows it.

DBMS_SQL Workflow

The DBMS_SQL package is owned by the user SYS, which is a superuser, so naturally this package is defined with invoker's rights, so that it runs under the privileged set of the invoker, and not the SYS user. The type of statements you can execute using the DBMS_SQL package are DML, DDL, and Alter Session statements, queries, procedures and functions, and anonymous blocks. I should also add that the transaction control statements like Commit and Rollback can also be executed using this package. Let us look at some of the common steps when executing dynamic SQL using the DBMS_SQL package. It provides APIs to call these different steps. You would start with opening a cursor. This step just locates a context area in memory and returns a cursor_id. This cursor_id would be useful for all subsequent steps. The next step is to parse the dynamic SQL statement. This step checks the syntax and semantics of the statement, and in the case of queries, it also prepares an execution plan. The same cursor_id can be used for parsing multiple SQL statements. In case of DDL statements, the parsing step will also execute the DDL statement. Next is binding the input and output variables to the various placeholders in the SQL statement. The placeholders are identifiers in a SQL statement with a colon in front, and through binding we associate it with the variable, and tell the package its type and length. This is done only for the input and output variables, and not for the columns in the select statement. For the columns in the select statement, we use a define column API to tell DBMS_SQL the name and type of the variables, which will be used to hold the values returned by the select columns of our query. So this step is needed only for query execution. Execute will execute the statement and will return the number of rows processed. Only in this step are the bind variables examined for type and size compatibility. Fetch rows step is again applicable only for queries where the next row in the result set is fetched and the variables defined with the define column step. This step will be called in the loop until there is nothing more to fetch. There's also an API executing fetch for queries, which will execute the query and fetch the first row in one column. Variable value step is used while calling procedures, anonymous blocks or DML statements where values are returned in the bind variables. This step gets the output values. Column value step is used to get the values of the select columns for a query. You select columns to bind it with the defined column API. This step will be called after each fetch row call. Finally, we need to close the cursor to release the memory area. Not all these steps are required for executing the different kind of SQL statements. We will look at that, and then also examine the APIs for these steps as we walk through the examples.

Executing DDL & Session Control Statements

Not all these steps are needed for executing DDL and session control statements. All you need are these steps, you open the cursor, parse the SQL statement. DDL and session control statements cannot have binds. Parsing the cursor also executes the DDL and session control statement, so the next step of execute is optional, but this behavior might change in future releases, requiring an execute, so it is better to do an execute also. Close cursor would release the memory. Let's see how to execute a DDL statement, and also understand some of the APIs of the DBMS_SQL package in the process. Here is a procedure drop_table to drop a table, the name parsed in the p_table_name parameter. We declare l_sql as a VARCHAR2 100 to hold the query. L_cursor_id is an INTEGER, and inside the execution section, it holds the cursor_id returned by the DBMS_SQL.OPEN_CURSOR call. So the first variant of the OPEN_CURSOR function takes no import parameter and returns an integer. Starting with Oracle version 11g, it has been enhanced to take an additional integer parameter called security_level to enhance the security and reduce the SQL injection attack risks. It has 3 values, 0, which has no extra security, 1, which requires that the user ID or role parsing the statement be the same as the one binding or executing it, or else Oracle will raise errors. Two is the most secure setting, which requires as addition that the user ID or role binding and executing, defining, describing, and fetching must be the same as the one which they're the most recent parse. We will talk more about this security setting later when we talk about the security aspects of the DBMS_SQL package. The next step is to parse the SQL statement. The parse procedure takes in the cursor_id option earlier with the open cursor call, next is the SQL statement, and finally the language_flag. The values for these are constants defined in the DBMS_SQL package. V6 is for compatibility with Oracle version 6, primarily to handle the chars, as in Oracle version 6, chars used to be variable-length character strings. V7 is for compatibility with version 7. NATIVE, which is most commonly used, provides the behavior according to the database this procedure is running in. FOREIGN_SYNTAX is to specify that the SQL statement being parsed be translated according to the SQL profile in the database session. You would be using the language setting of NATIVE in most situations. Next is executing the DDL statement. The execute API takes on the cursor_id and returns an integer, which is the number of rows processed. The return value is valid only for insert, update, and delete statements, and for other types, it returns an undefined value. The execute step is optional for DDL statements, as the parse step executes the DDL, but that behavior might change in future releases, so it's always a good idea to also do the execute step for DDL statements. The close cursor step will close the cursor, and will also release the _____ memory. The CLOSE_CURSOR procedure takes on the cursor_id to be closed. It is important to close the cursors to control the number of open cursors in the system, and reduce any unnecessary memory or _____. Executing session control statements is similar to executing DDL statements. For instance, if you have to run the statement, ALTER SESSION SET NLS_DATE_FORMAT = DD-MON-RRRR, to change the date format in the session, then we can create a procedure called alter_format, which takes in the format we want to specify, p_format. We declare l_sql to hold the SQL, l_cursor_id and l_return are INTEGERS. Inside the execution section, we'll build a SQL as ALTER SESSION SET NLS_DATE_FORMAT, and concatenate it with the past p_format. We open the cursor and get the cursor_id, call the PARSE step parsing in the cursor_id, the sql, and DBMS_SQL.NATIVE language flag. We do an execute off of that, and then close the cursor. So they can run this procedure as EXEC alter_format DD-MON-RRRR, two quotes inside of the string _____ will translate to a single code, as the first code will be used as an escape character.

Demo: Executing DDL Statements

In this demo, we will see how to execute our DDL statement using the DBMS_SQL package, and see some of the subprograms of this package in action. Here is a PROCEDURE drop_table, which takes in p_table_name of VARCHAR2, which is the name of the table we want to drop. We declare l_sql to hold the query, l_cursor_id to hold the cursor_id returned from DBMS_SQL package, and l_return to hold the value returned by the DBMS_SQL.EXECUTE function call. We start the execution session and build l_sql as DROP TABLE, and concatenate it with p_table_name. Using DBMS_SQL.OPEN_CURSOR, we open the cursor, and obtain a cursor_id, which we accept in l_cursor_id variable. Next, we do a PARSE call, parsing in the cursor_id, the sql to parse, and the language flag of NATIVE. This parse call should also execute the DDL, but we still call the execute call, parsing in the cursor_id, just so that if the behavior changes in the future, and if it is the EXECUTE call, which would actually run the DDL, we do not have the change of our code. We then execute the CLOSE_CURSOR procedure, parsing in the cursor_id. Let's first create a table called temptable, which just has one column, a, which is a number. Let's now describe the temptable to make sure it exists. Let's now call over exec drop_table procedure to drop this table. Let's describe the temptable again. The table call dropped successfully, and so it does not exist anymore.

Executing DML Statements

Executing DML statements, subprograms, and anonymous blocks use the same steps. Only the following steps are needed. We need to open the cursor, parse it, bind any input or output variables if present, execute the statement. If we have any out kind of variables, as with the returning values clause, or as out parameters, we would use the variable value API to retrieve them. Finally the close cursor will close the cursor. Let's talk about the bind variable API. This API, as I said earlier, is used to bind input and output variables to the placeholders in the SQL statement. It takes in the cursor_id as the first parameter, the name of the bind variable, which is the same name as the placeholder in the SQL statement, with or without the colon. The colon before is optional, but I like to put it just for re-consistency. Then you can specify the value of the bind variable by tying it to our variable. So for a VARCHAR2 bind variable, we can specify optionally a character set, and if it is an out type of variable, optionally the maximum size of the return value we expect. We will talk more about the size soon. This procedure is overloaded to accept number, date, club, raw, etc., variable values. Let's see how to execute an insert statement using the DBMS_SQL package. Here is a procedure insert_record, which takes in the table name, p_table_name, and its 2 column names and their values, p_col1_name, and it's value p_col1_value, p_col2_name and its value p_col2_value. We have the same local variables l_sql, l_cursor_id, and l_return. You build the insert string as INSERT INTO p_table_name, bracket, p_col1_name, p_col2_name, bracket closed, and then the values clause with the 2 bind variables, :col1_value and :col2_value. We open the cursor and get l_cursor_id, parse the sql, and then bind the two import variables. Each bind statement takes in the cursor_id, the name of the bind variable, :col1_value, and associates it with p_col1_value to provide the value and its _____. We do it similarly for :col2_value, associating it with p_col2value. We execute it further, bringing the l_return value, which is the number of rows processed, and then close the cursor, finally we COMMIT. Let's look at the variable value API, which is used to fetch the value of OUT type of bind variables. It takes in the cursor_id, the name of the bind variable, the same name we use in the bind variable API, and its value, which can be a VARCHAR2, having an optional CHARACTER SET specification. This procedure is overloaded to accept number, date, club, raw, etc., variable values. So let us take a look at an OUT kind of bind variable, and getting its value using the variable value API. In a DML statement using the returning INTO clause, we can retrieve extra information from the row, or rows _____. So here is an anonymous block where we are trying to update an item value, and then obtain the item_name using the returning INTO clause. So it has l_item_value, l_item_id, and l_item_name local variables, their data types based on the corresponding columns in the items table. We have gained a clear l_sql, l_cursor_id, and l_return local variables. Inside the execution section, we build a SQL as UPDATE items, SET item_value = :p_item_val, where item_id is equal to :p_item_id, returning item_name into :l_name. So here p_item_val and p_item_id are input variables, and l_name is an OUT kind of variable. We open the cursor, pass it from the SQL statement, and then bind the two import variables, p_item_val to l_item_value, p_item_id to l_item_id. We bind the output bind variable l_name to l_item_name, specifying the maximum length of the return value as 60. We execute the statement. Then using the variable value, we retrieve the output l_name bind variable in l_item_name. We print the rows processed, and the l_item_name returned, and then close the cursor. So each output bind variable will be fetched using the variable value call after the cursor execution. So as you notice that in this example, we specified the maximum out value length we expected for the end name bind variable. What if we did not specify the maximum length of the out variable as in the code snippet here? Then Oracle tries to get it from the variable's initialized value. In this case, l_item_name is initialized to null, so it internally restricts the maximum length of the return value as 0. So if we tried to run this block, then we will get an ORA-6502 error, for a numeric or value error. The other way we could have passed the right maximum length is by initializing the output variable to the maximum possible length as shown over here. So now the bind variable statement will get the maximum length from the initialized value of the variable, and there should be no errors in execution.

Executing Anonymous Blocks & Subprograms

Let's see how we can execute subprograms using the DBMS_SQL package. For procedures, we're going to use either the call statement, or execute them using anonymous blocks. For function calls, we do have to use anonymous blocks. Let's look at the call statement for executing a procedure, and we will look at how to use anonymous blocks for function calls with the DBMS_SQL package next. So here's a procedure called calculate_tier, which takes in p_act_id in the IN mode of accounts.act_id%Type, p_act_bal in the IN OUT mode of accounts.act_bal%TYPE, and p_tier in the OUT mode, which is the tier in which the account balance falls. Here is an anonymous block where we have declared l_act_id, and given it a value of 1. L_act_bal is accounts.act_bal%Type, and l_tier is to hold the return tier value. We declare l_sql to hold the SQL, l_cursor_id and l_return are INTEGERS. In the execution section, we build a SQL as CALL calculate_tier, and pass in the bind variables act_id, act_bal, and tier. We open the cursor and get l_cursor_id back. We then parse the cursor_id for the l_sql statement. The bind variable act_id is associated with l_act_id variable, act_bal is associated with l_act_balance, and tier with l_tier variable. We execute the cursor. Using the variable value API, we retrieve the IN OUT variable, act_bal in l_act_bal variable, out variable tier in l_tier variable, print those values, and then close the cursor. So using bind variables, we associate them, and then get their out values using the variable value procedure. Let us see how to execute anonymous blocks using the DBMS_SQL package, and also with that, see how to execute a function inside it. We have rewritten get_tier as a function this time. The parameters are the same in the IN, IN OUT, and OUT mode, and this function returns a number. So we start our anonymous block, which has l_act_id of accounts.act_id%TYPE, and assign it a value of 1. Then there is l_act_bal, l_tier as before, and l_out to accept the return value from the function call. We have l_sql, l_cursor_id, and l_return as INTEGERS. Inside the execution section, we build our l_sql string as BEGIN :l_out:=get_tier function call. L_out is a placeholder to receive the value from the get_tier function call. We parse in the three placeholders for the three function parameters. We end the anonymous block, and put a semicolon before we close the quotes. For anonymous blocks, you need to include a semicolon inside the quote. We open the cursor, and parse l_sql using it. Then we bind the three placeholders to the bind arguments, l_act_id for :act_id, l_act_bal for :act_bal, l_tier for :tier, and finally we bind the value returned from the function l_out to :l_out. We execute the cursor. For anonymous blocks, the value of l_return is undefined, but we still need to receive it in our variable. Using VARIABLE_VALUE, we receive the out variables, act_bal into l_act_bal, tier in l_tier, and the value returned from the function, :l_out in l_out variable. We print them, and then close the cursor. So this is how you can execute an anonymous block and a function using the DBMS_SQL package.

Demo: Executing Anonymous Blocks & Subprograms

Let us look at a demo to understand how to execute a function using anonymous blocks with the DBMS_SQL package. We have created a get_tier function previously, which gets the tier in which the act_bal falls for the parsed act_id. We declare l_act_id of accounts.act_id%TYPE, and give it a value of 1. L_act_bal is accounts.act_bal%TYPE. L_tier, l_out are numbers. L_sql is a VARCHAR2, l_cursor_id and l_return are both INTEGERS. We build l_sql as an anonymous block with the BEGIN keyword, :l_out will accept the return status from the function, 1 for success and 0 for failure. Get_tier function takes in :act_id in the IN mode, returns the act_bal from this account in the IN OUT mode in :act_bal variable, and finally gets the out variable tier. We open the cursor, we parse l_sql, we BIND the VARIABLES, :act_id with l_act_id, the IN OUT parameter :act_bal with l_act_bal. The out variable is bound to l_tier, and the value returned from the function :l_out is tied to the l_out variable. We execute the anonymous block. Then we get the OUT and IN OUT values using the variable value clauses, :act_bal and l_act_bal, :tier and l_tier variable, and :l_out and l_out variable. We print these values, and then close the cursor. Let's run this block. The out parameter, act_bal gets the value of 1000, it falls in tier 3, and the value returned from the function l_out is 1, which indicates success.

Executing Select Statements

Let us now see how we can execute select statements using the DBMS_SQL package. As before for other statements, all the steps are not necessary. These are the typical steps for our select statement. We open the cursor, parse it, bind input and output placeholders to the bind arguments. For the columns in the select statement, we bind them to variables using the defined column API. Using execute, we execute the select statement. The fetch rows call will fetch a row from the result set. Using column_value procedure, we get the select column values in the variables we had associated with the define_column API earlier. For our multi-row select, you would typically call the fetch rows and column_value calls in a loop until fetch rows returns a 0. There is also an execute and fetch method, which you can use to do the execute and fetch in one go. So this will save you a network roundtrip. The close cursor call will close the cursor. Let us look at the defined column API. It is a procedure which takes on the cursor_id, the next parameter is the position of the column as a number starting with 1. So the first select column will have a position 1, second select column will have a position 2, and so on, and so forth. Next is a local variable you want to attach to that column to receive the returned value. Finally is the column_size, which you would use in case of data types like VARCHAR2, etc., which do not have a fixed length. And this is a way for us to tell the DBMS_SQL package the maximum length of the variable it should expect at runtime. The define_column has various overloads for accepting in columns of various data types like numbers, date, bfile, binary double, etc. The column_value procedure is used to retrieve the selected column values after the fetch call into the variables we had tied earlier with the define_column API. It takes in the cursor_id as an INTEGER, position of the column, which is an INTEGER, and the out value, which can be a VARCHAR2, date, number, etc. There are overloaded procedures for the various data types. There is another variant of the column_value procedure, which has the OUT column_error parameter, which has the error code in case there was an error fetching for that column due to the value fetched, saving actual length exceeds the specified length in the define column API. The next parameter of actual length gives the actual length of the selected column retrieved. This helps during the debugging process. The fetch_rows function is a simple function, which takes in the cursor_id and returns an integer, which is the rows actually fetched. If it returns a 0, this means there are no more rows to fetch. The execute_and_fetch function, as I mentioned earlier, executes the cursor, and also fetches the rows, so it is equivalent to calling execute first and then fetch_rows. The second parameter called exact, if set to true will raise an exception if there are more than one rows fetched. It will still fetch the rows though. It has a default value of false, where it will not raise any errors in case there are more than one rows fetched. So using this function can reduce an extra network roundtrip. Let us look at an example of a multi-row select to better understand these subprograms we talked about. Let us select the item_id and item_name from the items_table, where the item_value is greater than a user-specified value. Just to illustrate the mechanics of a multi-row select, I have created an anonymous block where I have declared l_item_id, l_item_name to hold the selected values, l_value to specify the value threshold for the WHERE clause. We have over three variables, l_sql, l_cursor_id, and l_return. Inside the execution section, we build a SQL as SELECT item_id, item_name from items, WHERE the item_value is greater than the p_value placeholder. We open the cursor, PARSE the SQL, then we BIND the VARIABLE l_value to the p_value placeholder. We define the columns for the select, the first position for item_id tied to the l_item_ID variable, the second position for the item_name to the l_item_name variable. We specify the maximum expected length of item_name fetched to be 100, which is how the item_name _____ is defined in the items_table. We execute this SQL statement. Inside the loop, we do a FETCH_ROWS call. If it returns a 0, meaning there is nothing more to fetch, we exit. From the fetched_row, we get the selected values using the column_value procedure, parsing in the cursor_id. Position 1 to get the item_id in l_item_id variable, position 2 for item_name in the l_item_name variable. We print the values and end the loop. So in the loop we keep calling fetch_rows, and the column_value calls until fetch_row returns 0, and then we exit. Finally, we close the cursor. If we accidentally fetch it in a different data type, for instance, for position 1, for item_id, which is a number, if you fetch it in l_item_name, which is a VARCHAR2 variable, we will get an error ORA-6562, type of out argument must match type of column or bind variable. DBMS_SQL.LAST_ERROR_POSITION function returns an integer, which is the byte offset in the SQL statement where the error occurred, the first characteristic at position 0. So for instance, if there was an error in the SQL string syntax, it is very helpful to know the position of the error. So if you put a comma at the end of the item_name, without the last ERROR_POSITION function, the error we will get will be just missing expression, but with the last ERROR_POSITION function, it will also tell us the position of the error, which is very useful for debugging purposes.

Demo: Executing Select Statements

In this demo, we will demonstrate the use of the DBMS_SQL package, to do a multi-row select. We will also see some of the possible errors we can encounter if we do not specify the right column type, for example, and see how we can locate the parse errors easily. So here is an anonymous block where we will do a select against the items_table for items above a certain value. So we have declared l_item_id and l_item_name to hold the item information, l_value to specify the value threshold, l_sql will hold the SQL statement, l_cursor_id and l_return are INTEGERS. L_errpos is to hold the error position. We build a SQL as SELECT item_id, item_name FROM items WHERE item_value is greater than :p_value. We open the cursor, and obtain the cursor_id in l_cursor_id. We parse the l_sql, we bind the variable :p_value to l_value variable. We then define the select columns, and associate the column at position 1, with l_item_id, and at position 2 with l_item_name with a maximum length of 60. We then execute the cursor, then in a loop we call the DBMS_SQL.FETCH_ROWS call, and if it returns a 0 we exit. Now we obtain the selected columns using the column value procedure, getting the column at position 1 in l_item_id, position 2 in l_item_name. We print these values using DBMS_OUTPUT.PUT_LINE statement. We then end the loop, and close the cursor. We have an exception section with the WHEN OTHERS exception handler, where using the DBMS_SQL.LAST_ERROR_POSITION function, we get the last error position. In case of exceptions, we want to make sure that the cursor is closed, and so we have CLOSE_CURSOR call in the exception block also. Let's run this. We see in the Output the Item Name and value of items above a $50 value. Let us now see the last error position in action. Let us put an extra comma after the item_name. Let's run this block again. We get the error ORA-00936, missing expression at pos 28. Position 28 is obtained by the last error position call, without which it would have been very difficult to locate where this index went wrong in the SQL string. Let us now demonstrate the error, which will be raised if you fetch into the wrong data type. So let us fetch item_id at position 1, a number into l_item_name variable, which is a VARCHAR2. (Working) We get an error ORA-06562, type of out argument must match type of column or bind variable at pos 0. Let us correct it, and bring it back to l_item_id as before. Now let us reduce the column length for position 2 for item_name to say 5 characters. This is much lower than the value length in the column. We have declared two more variables l_columns_error and l_actual_length, both of type NUMBER. In the COLUMN_VALUE call, we have brought in l_columns_error and l_actual_length all parameters to get the error and actual length. Using the DBMS_OUTPUT.PUT_LINE, we are also printing these new out parameters. Let's run this block. We get an error code of 1406 at both, and an actual length of 9 for one, and 10 for the other. It, however, truncates the actual value while fetching it, making it 5 characters.

Describing Columns

Let us understand how the DBMS_SQL package can help us with situations where there are an unknown number of columns in the select statement or an unknown number of bind variables in the where condition of the select statement. We will first take a look at some of the APIs, which enable us to do that. The first is the DESCRIBE_COLUMNS API. It takes in the cursor_id as an INTEGER. The second parameter is an OUT type called col_cnt, which is the count of columns in the select statement returned by the procedure. And the last parameter is again an OUT type called desc_t, which in an associative array declared in the DBMS_SQL package. This associative array is a collection type, and each record in the collection describes a column in the select statement like its name, type, etc. The DESCRIBE_COLUMNS procedure is deprecated, and it is now replaced with DESCRIBE_COLUMNS2, where the only difference is the OUT associative array type, which is of type DESC_TAB2. There is also a third procedure, DESCRIBE_COLUMNS3, which has an OUT associative array of type DESC_TAB3. Let us understand these associative arrays a little better. The desc_tab is an associative array or collection object declared inside the DBMS_SQL package. It is defined as TYPE desc_tab, is TABLE OF desc_rec INDEX BY BINARY_INTEGER. This is the syntax for declaring them, in which we are creating a type, which is a table on a collection, and it is a collection, each element of which is of TYPE desc_rec. Desc_rec is a record, the definition of which will see soon. This record contains various elements, which describe the column, and we use INDEX BY BINARY_INTEGER clause to indicate that the index counter for the table or collection will be a binary integer. The only difference between the different desc_tab types is that desc_tab2 is a collection of desc_rec2 type records, desc_tab3 is a collection, each element of which is of type desc_rec3 record. Let us understand the structure of DESC_TAB visually. It is a table or a collection containing several elements. The first element is accessed as desc_tab1, and it will give us access to the desc_rec record stored at index 1. Similarly, desc_tab2 will give us access to the desc_rec record at index 2, and so on, and so forth. One, two, three, etc. are binary integer index counters. Let us take a look at the structure of the DESC_REC record. The desc_tab table has a collection of these record types. As I had mentioned earlier, for each select column, these hold information like the column type, column maximum length, column name, the length of the name, the name of the schema where the column resides, the schema_name_len, in case of numbers, the position and scale of columns, in case of characters, the charsetid, col_charsetform, and whether this column is nullable. So a value of TRUE would mean that null values are okay over here. So the DESC_REC record type gives us a lot of information about the column in the select statement. Knowing the column type, for instance, can help us define the column, and associate it with the right variable type. DESC_REC2, which constitutes the collection element for desc_tab2 table type, has the column name defined as varchar2 32767 bytes, the maximum possible length for a varchar2 versus limiting the column length to 32 characters, as with DESC_REC record. So you should use DESC_REC2 instead of DESC_REC, so that it does not error out if the column name happens to be more than 32 characters. DESC_REC3 has 2 additional elements for user defined types, it has information on the col_type_name and the col_type_name_len. So these fields are populated only when the col_type is 109, which stands for user defined types. Here are the different column types and their binary integer values, which you can use while working with DESC_REC record type.

Unknown Number of Select Statements

When the number of columns in the select statement are unknown, then we have to use the DBMS_SQL package. This is one of the unique advantages of using this package. Let us look at an example to understand the different data structures and procedures we talked about in order to find out the number of columns in a select statement and their information. Typically these kind of subprograms can be used behind a search, say on a website, which allows the users to specify the information they want to see. So here is a procedure desc_columns, which takes in a user specified query with unknown number of columns. We have declared l_cursor_id and l_no_of_columns, both INTEGERS. L_desc_tab2 is DBMS_SQL.DESC_TAB2 type, and l_desc_rec2 is DBMS_SQL.DESC_REC2 type. Inside the execution section, we open the cursor, parse the query, then we call DBMS_SQL.DESCRIBE_COLUMNS2 procedure, parsing in the cursor_id, l_no_of_columns to hold the return column count from the procedure, and l_desc_tab2 to hold the return table, which has the column information. Then you open a LOOP, looping from 1 to l_no_of_columns. You would fetch the desc_rec, add the index value i by using the notation l_desc_tab2, parsing in the locator index. Now we can use the record to find out the column information like col_name as l_desc_rec2.col_name, col_type as l_desc_rec2.col_type, and so on, and so forth. We end the loop and close the cursor. So for instance, say we call this procedure as EXEC desc_columns, parsing in p_query as SELECT order_act_id, item_name FROM orders, items WHERE order_item_id is equal to item_id. The then desc_columns procedure correctly discovers the number of columns in the select statement and their types as are written from the output. So let us extend this procedure to also accept some user defined bind variables. It takes in p_query as before, but also a bind variable with the name p_key and its value p_value. I have just shown a single name value pair to illustrate the concept of working with bind variables in a query, but you can parse a collection of p_name p_value pairs of all types a query takes, like VARCHAR2, date, number, etc. to work with unknown number of bind variables. So inside again we have l_cursor_id, l_return, and l_no_of_columns as INTEGERS. We have the l_desc_tab2 and l_desc_rec2 types, l_number is a NUMBER variable, l_date a date, and l_varchar2 is a VARCHAR2 variable. Inside the execution section, we open the cursor, parse a query, then call the DESCRIBE_COLUMNS2 procedure as in the example before, parsing in the cursor_id, and accepting the return column count in l_no_of_columns, and their information in l_desc_tab2 variable. Then in the LOOP, we describe the columns based on the information we received from the DESCRIBE_COLUMNS2 procedure. We run the LOOP from 1 to l_no_of_columns. We get the desc_rec record, and that index counter in l_desc_rec2 variable. So we are expecting just number, date, and VARCHAR2 types in our select statement. We check the col_type, if it is 2, which is a number type, we call DEFINE_COLUMN, and bind it to position i, with l_number variable. If col_type is 12, which is a date, we bind it with l_date, or otherwise we just bind it with l_varchar2 with a maximum expected length of 100. This way we define all the columns in the loop. Next we bind the variable p_key to p_value. If there were multiple key value pairs parsed as collections, we could have just run this in a loop binding them over here. Then we exclude the cursor and continue on. We will see a working example in the demo shortly.

Demo: Unknown

In this demo, we will demonstrate one of the advantages of using the DBMS_SQL package, which is to execute a select with an unknown number of select columns. Let us start with seeing how we can discover the number of columns and the information using the DBMS_SQL package. So here in the FUNCTION desc_columns, which takes in p_query, which is a VARCHAR2, and p_cursor_id in the IN OUT mode. The third parameter, p_desc_tab2, is of type DBMS_SQL.DESC_TAB2 is also parsed in the IN OUT mode. We declare l_no_of_columns to hold the return number of columns, l_desc_rec2 is the record of type DBMS_SQL.DESC_REC2. Inside the execution section, we open the cursor, and hold the return_cursor_id in p_cursor_id. We parse the input p_query. We then call DBMS_SQL.DESCRIBE_COLUMNS2 procedure, parsing in the p_cursor_id, and getting back the count of columns in l_no_of_columns, and the information in the IN OUT variable p_desc_tab2. Knowing the loop, it goes from 1 to l_no_of_columns, we get the record describing each column in the select statement. We use p_desc_tab2, and parse in the index counter i to access the record at that position, and fetch it in p_desc_rec2 record. Now we can get more information about that column by using p_desc_rec2.col_name to get the column_name, and p_desc_rec2.col_type to get the column type. You can of course get a lot more information using the other elements of the record. We then end the loop. Let's compile this function. Now let us execute this function in an anonymous block. We declare l_cursor_id as an INTEGER, l_desc_tab2 as DBMS_SQL.DESC_TAB2 type, and then call desc_columns function, passing the query SELECT order_act_id, item_name FROM orders, items WHERE order_item_id is equal to item_id. L_cursor_id and l_desc_tab2 are the other 2 parameters being passed in. We hold the return information in l_no_of_columns. Let's run this anonymous block. We see in the output that DBMS_SQL was able to identify the column names and their types. Let's now extend this demo to demonstrate how to bind variables to the select statement, and fetch the results. So here is a procedure print_results, which takes in the query p_query, bind variable p_key, and its value p_value. Inside we declare l_cursor_id, l_return, and l_no_of_columns as INTEGERS. L_desc_tab2 is of type DBMS_SQL.DESC_TAB2, and l_desc_rec2 is of type DBMS_SQL.DESC_REC2. L_number is a NUMBER, l_date a DATE, and l_varchar2 a VARCHAR2 variable. We first call over function desc_columns, parsing in p_query, l_cursor_id, and l_desc_tab2. Now in the loop we define the columns. We go from 1 to l_no_of_columns. From the l_desc_tab2 table at the index counter i, we fetch the record in l_desc_rec2. Then we check the type of l_desc_rec2.col_type. If it is 2, which stands for a number, then we call DBMS_SQL.DEFINE_COLUMN, and tie the column at position i to l_number. If it is 12, or a date, then we tie it to l_date, otherwise we tie it to l_varchar2, as we are only expecting these 3 data types. You can certainly have more conditions for other data types. We then bind the import variable by calling DBMS_SQL.BIND_VARIABLE call, binding p_key with p_value. We execute the cursor, then while DBMS_SQL.FETCH_ROWS call does not return a 0, which would indicate no more data, we fetch the row. Again in the loop from 1 to l_no_of_columns, we get the value of each selected column in the return row with the DBMS_SQL. Again, if the column_type is of 1, we get it in l_varchar2. If column_type is 2, we get it in l_number, and for 12, we get it in l_date variable. We also print the values using the DBMS_OUTPUT statement. We then close the cursor. It is always a good practice to create procedures of functions, which encapsulate a lot of DBMS_SQL calls, and which can be reused by different clients. This way, you don't have to write the lengthy code again and again, as well, you increase reusability, accuracy, and consistency in your code. Let's compile this procedure. Let us run this as EXEC print_results, parsing in the query SELECT item_id, item_name FROM ITEMS WHERE item_value is greater than :pvalue, then parse the key_:p_value and its value 100. From the Output, we see the column name, the column type, as well as the fetched values.

Security

The DBMS_SQL package has a lot of intelligent built-in security checks. One of them is the invalid cursor check. If an invalid cursor_id is used with this package, it will disable the DBMS_SQL package for the entire session, and then we'd have to log out and log back in to work with the package again. This can greatly help avoid SQL injection risks. For instance, let us say that user demo is running a session with dynamic SQL. With the open_cursor function, it obtains the cursor_id. Then later, somewhere down in the code, an exception is raised, making the execution flow to the exception block. The close_cursor call never gets executed, leaving the cursor_id open. Let's say a hacker steals it, and tries to run a malicious delete using it in another session. The execution will error out with ORA-29471, DBMS_SQL access denied. This would also happen if the user in its own session would have accidentally entered an invalid cursor_id to the DBMS_SQL subprograms. Every time the open_cursor statement is called, it generates a random cursor number, making it difficult for hackers to guess. Also there are some checks which can be performed with the DBMS_SQL package. These are controlled by the security_level parameter we saw earlier with the open_cursor function, 0 meaning no security checks, 1 meaning that the userid role parsing be the same as the one binding or executing it, and 2 is the most secure setting. The checks, which are performed are that the current calling user be the same as the one who did the most recent parse, that the enabled role on current call be the same as the enabled role for the most recent parsed call, and also that the container on the current call be the same as the container on the most recent parsed call. If any of these are not true, Oracle will raise the ORA-29470 error, for effective userid or roles are not the same as when the cursor was parsed.

Demo: Security

Let us now see some security aspects of the DBMS_SQL package. Let us say that the desc_columns function was compiled by user demo. This function is a definer's right function, and will execute another privileged set of user demo. User demo then grants the execute privileges on this procedure to the test. Now here's a session running as user test. User test creates the procedure print_results in its own schema, and inside it, it refers to demo.desc_columns function. So now since desc_columns function, which is a definer's right function is executed under the privileged set of user demo, which opens and parses the DBMS_SQL cursor_id, then later in the test, use that same cursor_id for further subprogram calls. Oracle, in this case, would error out the execution. This is because the effective userid and role using the cursor_id, which is user test, is not the same as when the cursor was parsed by user demo. Let's compile this procedure. Now in the test session, let's run the print_results procedure again with the same parameters to confirm that. We get the error ORA-29470, Effective userid and roles are no the same as when the cursor was parsed. This is an important built-in security check in the DBMS_SQL package, where it ensures that the effective user or role using the cursor_id is the same who did the most recent parse, and thus it helps avoid or minimize SQL injection attacks.

DBMS_SQL & Native Dynamic SQL Interoperability

Let us compare DBMS_SQL versus native dynamic SQL, the two ways for executing dynamic SQL. Native dynamic SQL should be a preference for executing dynamic SQL, as it is faster, and easier, and compact. For instance, in the top code snippet, we have an example of writing a dynamic procedure to drop a parsed table_name using DBMS_SQL package, and in the bottom code snippet we do it using native dynamic SQL. Notice with native dynamic SQL how compact and readable the code is. And here's a code snippet for a multi-row select using the DBMS_SQL package. And here is a _____ piece of code doing it using native dynamic SQL. This further emphasizes the compactness and simplicity of using native dynamic SQL. So when should you be using the DBMS_SQL package? You should be using them for the tasks, which only it can do, namely handling dynamic SQL when the number of select columns are unknown, or when the number of bind placeholders are unknown. For all of the situations, I would use native dynamic SQL. However, with Oracle 11g, there have been several enhancements to the DBMS_SQL package, which allow interoperability between the DBMS_SQL package and native dynamic SQL. First is the addition of the DBMS_SQL.TO_REFCURSOR function, which takes in the cursor_id obtained by using the DBMS_SQL.OPENCURSOR function, and it converts it to a REFCURSOR, which can be used with native dynamic SQL. SYS_REFCURSOR is a weakly typed REFCURSOR already provided in the database, so you might want to use it in a situation where the items selected from a select statement are known, but where it might be easy to use native dynamic SQL to fetch them, for instance in records versus using several DBMS_SQL.COL_VALUE calls. So as in here, we are selecting * FROM items_table WHERE item_value = to :p_item_value. Since we have to fetch all columns on the items_table, it is easier to use l_items_rec, which is items%ROWTYPE, to fetch them versus individual variables to hold them. So we first get the cursor_id using DBMS_SQL.OPEN_CURSOR function. You parse it, bind the import variable to it. If there were an unknown number of bind variables, DBMS_SQL would have come handy over there. We execute using DBMS_SQL.EXECUTE function. After the executing step, we can now convert it to a REFCURSOR with the DBMS_SQL.TO_REFCURSOR call, parsing in l_cursor_id. From this time on, l_cursor_id no longer is valid, and we cannot refer to it. We will have to work with l_ref_cursor from this point on. So in a loop, you fetch from l_ref_cursor into l_items_rec, and then close l_ref_cursor towards the end. Let us talk about the other situation where we want to utilize the strengths of the DBMS_SQL package when using native dynamic SQL. This might be in the situation like where the number of columns are unknown in the SQL statement. We can use the TO_CURSOR_NUMBER function in that case, which takes in the REFCURSOR in the IN OUT mode, and returns an integer, which is the cursor_id, which can be used with a DBMS_SQL package. The REFCURSOR has to be opened first before it can be parsed in this function. This function can accept both strongly and weakly type REFCURSORS. If you want to learn more about REFCURSORS, please refer to the Oracle PL/SQL Fundamentals Part 1 course on cursors. Once you have converted the REFCURSOR to a cursor_id, you cannot convert it back to a REFCURSOR again using the TO_REFCURSOR function. So in this code snippet, we have a procedure getinfo, which takes in p_query with an unknown number of select columns. We have declared l_cursor_id to hold the cursor_id to work with the DBMS_SQL package. L_REFCURSOR holds a reference to SYS_REFCURSOR. We again have l_col_count for the return column count, l_desc_tab2 table_type, and l_desc_rec to record type. We open the l_ref_cursor FOR p_query. Now using dbms_sql.to_cursor_number function, you parse in l_ref_cursor, and convert it to l_cursor_id. After that, we can work with DBMS_SQL package using this cursor_id, and DESCRIBE_COLUMNS using that, and from this point on, we also cannot refer to l_REFCURSOR. We finally close l_cursor_id using the DBMS_SQL.CLOSE_CURSOR procedure. So we can run this procedure as EXEC, getinfo, parsing in the query with any number of select columns.

Summary

In this module we took a look at the DBMS_SQL package used for executing dynamic SQL. We looked at the various steps involved in using this package for our different statement types, and along the way, looked at its various procedures and functions. The main reason you'll be using this package is when there is an unknown number of select columns or bind variables. Otherwise, for most other situations, you would use native dynamic SQL, which is faster, simpler to write, and compact. DBMS_SQL has support for SQL data types, as well as user-defined data types. It has been enhanced to be more secure in order to prevent SQL injection risks. It will make the package unusable for the session if an invalid cursor_id is parsed, and has a lot of other checks available with the security setting parameter to prevent SQL injection attacks, say for instance, where it checks that the user parsing is the same as the one binding or executing it.

Debugging PL/SQL Code

Debugging Methods

Hi. Welcome to Pluralsight. My name is Pankaj Jain, and welcome to this module on Debugging PL/SQL code. We all would agree on the importance of being able to quickly diagnose a problem once it occurs. Being able to write good performing code is equally important to being able to quickly troubleshoot problems. Oracle provides several tools and methods to debug PL/SQL code effectively in order to quickly provide bug fixes. We started our discussion on debugging PL/SQL code in Oracle PL/SQL Fundamentals Part 1 course where we talked about several useful methods and techniques for debugging. We will build upon that knowledge and extend it to debugging stored program units. We will learn how to quickly find the place the problem is occurring even if it in a nested program unit buried several layers down in the call stack. We will utilize SQL Developer Debugger, a very powerful and easy to use debugger for drilling down to the root cause of a problem and doing a lot of analysis, so let us take a closer look at this extremely important topic. Some of the debugging methods we will be talking about in this course will be utilizing the PUT_LINE method of the DBMS_OUTPUT package. DBMS_UTILITY is another important built-in package provided by Oracle, which has some useful methods like format_error_stack and format_error_backtrace, which can provide us the information as to where in the call stack the error occurred. We will take an extensive look at SQL Developer Debugger, talk about what is needed to debug stored program units, setting breakpoints, and how to navigate the code to find problems quickly. We have talked about DBMS_OUTPUT and DBMS_UTILITY packages in detail in the Oracle PL/SQL Fundamentals Part 1 course, so I will just briefly touch upon them over here. We will continue to build upon the SQL Developer Debugger concepts we talked about in the Part 1 course and talk here specifically about debugging stored program units.

DBMS_OUTPUT

The DBMS_OUTPUT package has several methods. We discussed a lot of them in the Part 1 course, but we will briefly talk about the PUT_LINE method over here. This method takes in a message parameter of type VARCHAR2. The maximum length of the message you could pass in was 255 bytes for older versions of Oracle, which has been increased to 32,767 bytes for version 10.2 and above. The PUT_LINE procedure automatically puts an end of line marker after the passed message so that each message appears neatly in a separate line. So as shown in this code snippet you could put different DBMS_OUTPUT.PUT_LINE messages throughout your code to observe values of variables or to know the current point of execution in the code. You would typically call it in the EXCEPTION block to output error messages to the client console. One thing to note is that the messages will appear in the output console only after the block or program finishes executing. The output messages will not appear while the code is executing. That is where SQL Developer Debugger comes in handy as it allows you to see the flow and interact with the code while it is executing. SQLPLUS and SQL Developer are the tools where you would be using DBMS_OUTPUT the most. You have to enable the output and then call the get line procedure to get each line. In SQLPLUS setting serveroutput on will enable the output, as well as call the get line automatically for you. You can specify the buffer memory for the output fetches for the session using set serveroutput on size 200,000, for example, to set it to an explicit limit or simply set it to unlimited. SQLDeveloper also allows you to set buffer size. We will see how do we output and set its size in SQLDeveloper in the demo shortly. Once set, it also performs enabling and fetching messages automatically for you. You would mostly be using DMBS_OUTPUT.PUT_LINE calls for your unit testing. One drawback is that you have to keep modifying the code to add more lines in order to see more information, but on the other hand it is an extremely easy way to debug. If the DBMS_OUTPUT is not enabled, then the messages will not show up in the console, so you can use enable and disable procedures to control seeing the output or not, and if you would further like to have more control you could write your program unit such that it takes an input parameter, say p_debug, setting its DEFAULT value to N, and then wrapping the DBMS_OUTPUT calls with an IF check for this parameter. This way you can control which messages you always want to see, say the one's in the EXCEPTION block, but not be presented with the other debug messages until you are setting the p_debug parameter to yes.

DBMS_UTILITY

DBMS_UTILITY is a built-in package provided by Oracle, which has among others a couple of useful functions, DBMS_UTILITY.FORMAT_ERROR_STACK and the DBMS_UTILITY.FORMAT_BACKTRACE functions, which can provide useful debugging information. Both of these functions should be placed in the exception handler section of the code. The FORMAT_ERROR_STACK function provides a formatted error stack. It provides information similar to the SQLERRM function. The only difference is that it can provide information up to 2000 bytes while the SQLERRM function can provide only up to 512 bytes of information, and then it truncates the rest, so this is a preferable function to use. FORMAT_ERROR_BACKTRACE function provides the line number information where the error occurred along the stack. For instance, in this code snippet, and I'm just showing partial code over here, say we have a procedure set_lcn, which in turn calls a function get_dept. Inside the get_dept function we have l_id, a number, to which we try and assign a VARCHAR2 string abc. This would raise the numeric or value error, which will flow to the outer set_lcn procedure. There we have an EXCEPTION handler section with a WHEN OTHERS handler where it would be caught. Here calling the DBMS_UTILITY.FORMAT_ERROR_STACK function will give us the SQL code ORA-6502 with the error message PL/SQL numeric or value error: character to number conversion error. This is similar to the SQLERRM function output, just that it allows for longer error messages. Calling the DBMS_UTILITY.FORMAT_ERROR_BACKTRACE function and showing the return information with DBMS_OUTPUT will give us the error stack and the line numbers of the error propagation. You should read it bottom to top. It says that the error occurred at line 4 of DEMO.SET_LCN procedure, DEMO here being the username owning these subprograms. Then in the stack it occurred at DEMO.GET_DEPT function at line 6. So now you know the line numbers where the error occurred, as well as the propagation flow, pretty useful information for debugging, especially when you expand this to several layers of call as with most practical production applications, so you should include these function calls in your code in the EXCEPTION handler section. Let us now take a look at a demo to see these functions in action.

Demo: DBMS_OUTPUT

In this demo we will see the DBMS_OUTPUT.PUT_LINE function, as well as the DBMS_UTILITY.FORMAT_ERROR_STACK and DBMS_UTILITY.FORMAT_ERROR_BACKTRACE functions in action. Let us say we create a simple, standalone function get_item_value, which gets the item value for passed p_item_id of items.item_id%TYPE. It declares l_item_value to hold the fetched item value. Inside the execution section we first put a DBMS_OUTPUT.PUT_LINE message entering get_item_value function with item_id concatenated with p_item_id. Then we have a simple SELECT, which selects the item_value into l_item_value FROM the items table WHERE item_id is equal to p_item_id. Typically you should obtain this using a cursor and then handle the situation if the item_id is not found, but here to create errors for illustration purposes I have returned it as a select fetch. Then using DBMS_OUTPUT we again put a message exiting get_item_value function. These messages can help us keep track of the execution flow, and you can put more messages in between to indicate other steps or values useful for your debugging. We finally return l_item_value. Let's compile this function. Next let us create a package called order_mgmt, which just contains one function called get_discount, which takes in p_item_id of items.item_id%TYPE. This function will return back the discount we want to offer for an item based on its value. Let's compile this. Next is the PACKAGE BODY with the implementation of this package. Again we output a message entering order_mgmt.get_discount function with an input item ID p_item_id using DBMS_OUTPUT.PUT_LINE. Here we first call the standalone function get_item_value passing in p_item_id to get its value and then calculate l_discount as .05% of its value. We then put a message using DBMS_OUTPUT.PUT_LINE exiting order_mgmt.get_discount function with a discount concatenated with l_discount. We RETURN l_discount. Then there is an EXCEPTION block with a WHEN OTHERS exception handler where we're using DBMS_OUTPUT.PUT_LINE procedure I'm showing the output returned by calling dbms_utility.format_error_stack and format_error_backtrace functions. I wanted to show you the EXCEPTION call stack with different program units in play, here a package function calling a standalone function. Regardless of the program unit type, Oracle will show you the name and the line number in the error stack. Let's compile this PACKAGE BODY now. Next is an anonymous block where we declare a local variable l_discount of type NUMBER, and inside the execution section it holds the return discount value by calling the order_mgmt.get_discount function. First let's call it with a valid item_id of 1. Before we execute this block, let's enable DMBS_OUTPUT. If we do not have the DBMS_OUTPUT window open, let's close ours for now, you can open it by going to View and then clicking on Dbms Output. Then you click on the plus sign and select your connection, in our case the demo connection, and click OK. This would enable DBMS_OUTPUT in SQL Developer with a default buffer size of 20,000. You can change it to another value. Let's say we change it to unlimited. Let's first execute this block with a valid item ID of 1. From the output window you can see the execution flow, which we have indicated using DMBS_OUTPUT messages. It says entering order_mgmt.get_discount function with an input item id of 1. Then it entered the get_item_value function, which is called from within order_mgmt.get_discount function. Next it exits get_item_value function and finally exits order_mgmt.get_dicount function with a discount of 5. This shows how using DBMS_OUTPUT.PUT_LINE calls we can debug the flow and put our messages to help with debugging. Now let's execute this block for an item ID of 3, which does not exist. This time it enters the order_mgmt.get_discount function with an input_ item id of 3. Then it calls the get_item_value function and enters it with the item_id of 3. The select statement will raise no data found exception, which flows to the Exception Block of the order_mgmt.get_discount function. It is caught by the WHEN OTHERS handler where we see the dbms_utility.format_error_stack output of ORA-01403 no data found. We also see the dbms_utility.format_error_backtrace function's output. It shows the call stack as DEMO.ORDER_MGMT package at line 8 where it calls the DEMO.GET_ITEM.VALUE function, and then the error occurs at DEMO.GET_ITEM_VALUE at line 5, so this makes it very easy for us to debug and see what's going on and causing the error. Now let us explore the debugger tool of SQL Developer, which is a very easy and powerful way to debug PL/SQL code.

Compiling for Debug

Before we dive into our demo, I wanted to mention that in order to debug stored program units you would have to compile them in the debug mode; otherwise, you would not be able to set breakpoints or drill down into the code. Compiling it in the debug mode kind of opens the door to peek inside the code during debugging; otherwise, without it the door is closed, and you cannot look inside. We covered the stored program units and the commands to compile them in the Oracle PL/SQL Fundamentals Part 2 course, but let me just give you the commands over here again. To compile a package hr_mgmt in the debug mode you can issue ALTER PACKAGE hr_mgmt COMPILE DEBUG. To compile a procedure test in the debug mode, you could issue ALTER PROCEDURE test COMPILE DEBUG. And to compile the get_item_value function in the debug mode you would issue ALTER FUNCTION get_item_value COMPILE DEBUG. Secondly, in order to launch a debug session from SQL Developer, a DBA user needs to grant some privileges. So say we want to allow the user demo to run debug sessions in which case the DBA can GRANT DEBUG CONNECT SESSION TO demo and GRANT DEBUG ANY PROCEDURE TO demo. This would enable the demo user to launch a debugging session.

Demo: SQL Developer Debugger Setup

In this demo we will do a quick recap of the SQL Developer debugger setup, look at various debugging views, as well as understand the navigation options. We will illustrate how to get to an error location quickly along with being able to watch the flow and the variable values along the way to understand the execution logic and doing what if analysis. Before we start, let's understand the pieces of code we will be using in our debugging demo. Here is a standalone function get_item_value, which we looked at in the earlier demo. This function returns the item value for a passed item_id. Let's compile this in the debug mode. If the function is already compiled, you can compile it in the debug mode by issuing the command ALTER FUNCTION get_item_value COMPILE DEBUG to compile it in the debug mode. We can also do it if we open up the function view, get to this Functions, right-click, and choose Compile for Debug. Notice that the icon color changes to green indicating that this function is compiled in the debug mode. Next is the package specification order_mgmt, which has a function get_discount taking in p_item_id of items.item_id%TYPE. It also has a procedure called print, which takes in p_message of type VARCHAR2. Let's compile this. Next is the PACKAGE BODY implementation where we implement the function and the procedure. This is the same PACKAGE BODY we had seen in the earlier demo with slight modifications. We have declared l_item_value, l_discount and initialized it with a value of 0, and l_discount_ratio, which is declared as a NUMBER. It first calls the get_item_value function passing in the p_item_id, then applies the logic that IF l_item_value is less than 600 then l_discount is $10.00, and if the item_value is greater than 600 then l_discount is $20.00. Then it tries to find a discount_ratio by dividing l_item_value by l_discount. Then we have the DMBS_OUTPUT.PUT_LINE message exiting order_mgmt.get_discount function with a discount concatenated with l_discount. It returns l_discount. This is followed by the EXCEPTION block. In the WHEN OTHERS exception handler we will print the error message using dbms_utility.format_error_stack and dbms_utility.format_error_backtrace functions. The print procedure is a dummy procedure, which just prints the past message using DBMS_OUTPUT.PUT_LINE. Let's compile this package. Let us now recompile this in the debug mode by issuing ALTER PACKAGE order_mgmt COMPILE DEBUG. We can also do it from the object explorer by expanding the Packages node and then clicking on the ORDER_MGMT package, right-clicking, choosing Compile for Debug, and then we choose the package body and then also Compile it for Debug. We need to compile both the package specification and the package body in the debug mode in order to drill down inside it during the debugging session. Again, you would notice that the icon has turned green indicating that these are compiled in the debug mode. Next let's go to Tools, Preferences, and then click on the Debugger. Let's check the Show Debug Actions on Main Debug Menu to see the debug buttons on the main screen. In the Start Debugging Options let's leave it as the default Run Until a Breakpoint Occurs option. If you recall from the Part 1 course for debugging anonymous blocks we had selected the Step Into option. Let's click OK and come out of it. Now let us grant our demo user the necessary privileges for it to start the debugging session. So we are logged in here as a DBA user, and we issue GRANT DEBUG CONNECT SESSION TO demo and GRANT DEBUG ANY PROCEDURE TO demo to enable the demo user to start a debugging session. Let's run these commands.

Demo: Debugger Views, Navigation & Debugging Exceptions

Having done the setup now, let us look at how to locate the line causing an exception quickly. We will run the order_mgmt.get_discount function and pass it an invalid item ID of 3. Let's open the PACKAGE BODY first. You can put a breakpoint by clicking on the left tab. This is the point where the execution will come to a halt. The red dot indicates that a breakpoint is currently set at this line, and you can again click it in order to remove it. Let's put our breakpoint again. There are several ways to start a debug session. You can right-click inside the PACKAGE BODY anywhere and then choose Debug or you could click on this red Debug button over here or you can go the Packages, right-click, and choose Debug. You can similarly start a debug session for a standalone procedure or a standalone function by right-clicking on the procedure or the function and choosing Debug. For the package you can right-click on the specification or the body. It does not matter. It will open up our window for us as we see here creating an anonymous block for us to run the selected subprogram. Since we have two subprograms, it shows both of them as available options for us to run. Let's choose GET_DISCOUNT. SQL Developer creates an anonymous block for us to run this function. Let us put an invalid ITEM_ID of 3 over here, and then let us click OK. Since in the debugger options we had chosen the option of run until a breakpoint occurs it executes the anonymous block and then halts the execution at the first breakpoint. At this point, let us quickly recap the navigation options and debugging views. The first square red button is Terminate debugging. Next the red arrow is to Find the current Execution point in the flow. It'll take you to where the current execution is at this point in time, which is currently at the first breakpoint. Next is the Step Over option, which will execute the current line and go to the next. If the current line is a function call, it will not go into it, but just execute it and go onto the next executable line after that. If, however, you have put in a breakpoint inside the function, then it will go there and stop. Next is the Step Into or F7 key, which will execute the current line like the Step Over option, but will drill down into the subprogram if the current line is a subprogram. Step Out leaves the current method and goes to the next statement. Step to the End of Method will go to the last statement of the current method. Next is the Resume button, which will take the execution to the next breakpoint, and if it doesn't encounter any breakpoint it'll finish the execution. Pause will halt the execution, but allows us to resume the execution again. Garbage Collect helps clear the cache of invalid objects. Now let us take a look at some of the debugging views. First is the Log window where it gives you the messages for connection, etc. You will also see the messages from your DBMS_OUTPUT.PUT_LINE calls over here. Next is the Breakpoint's window where it shows you your breakpoints currently in place. It shows you both the user-defined, as well as the system-defined breakpoints. The first one is the Oracle exception breakpoint, which is a system-defined breakpoint, which Oracle creates by default. Let's right-click on it and choose Edit. Let us checkmark Break for Caught Exceptions. This will allow the execution to break even for caught exceptions. In the Conditions tabs we can define the conditions when the breakpoint should be evaluated. So, if we define a condition which evaluates to false, the execution will not stop at this breakpoint. We will see an example of setting up a conditional breakpoint shortly. Next is the Actions tab. The default action is to halt the execution if it reaches the breakpoint. Let's click OK. For some reason SQL Developer comes up with this warning the class java.lang.Exception could not be found. Are you sure the classname is correct? Let's click Yes to this. Next is our breakpoint, which we have established at line 8. Smart Data tab shows the variables Oracle thinks might be most relevant to you at the current point of execution. Data tab shows all the variables which we have declared so far and their current values. In the Watches tab we can set up a watch for any particular variable we had entrusted in. Let's go to the Data tab, and let's put a watch for the L_ITEM_VALUE. Let's right-click, select Watch, and now it shows up in the Watches window. At this point of time since our exception breakpoint is set and it is set to halt or break for both caught and uncaught exceptions, one quick way to find the line causing the exception is to hit resume at this point of time. If we hit resume, it will stop at the line causing the exception, or if there is no exception, it will just complete the execution, so let's just go ahead and click Resume at this point of time. The execution stops at the SELECT statement inside the get_item_value FUNCTION, which is where an exception has occurred. So if we go to the Data tab now, we'll see that the P_ITEM_ID at this point has a value of 3 and an exception is raised. Let's expand the _throw node. We can see the _sqlcode and sqlerrm for the exception, so this is a very quick way to find the offending line of code causing the exception and see the data values at that point of time. Let's click Resume and finish the execution. In the Log window we can see the DBMS_OUTPUT messages from the exception block. As I had mentioned to your earlier that unless the execution completes the DBMS_OUTPUT messages won't show up in the output window, one of the drawbacks of using DBMS_OUTPUT.PUT_LINE statements. In the next demo we will see how we can navigate the flow of execution and observe the data values along the way.

Demo: Navigating Execution Flow With Debugger

Having taken a look as to how to quickly find the location causing an exception, let us now see how we can navigate the code using breakpoints and go from one line to another to see the flow of logic and variable values. Let's go to Tools, Preferences, Debugger, and let us check the option of Step Into. This will make the execution stop at the first line of the anonymous block versus executing it until the first breakpoint. Let's click OK. Let's start the debugger session again. Let us choose GET_DISCOUNT. This time let us enter a valid item value of 2 and click OK. If you notice, now it has opened up a window with the ANONYMOUS_BLOCK used to run the GET__DISCOUNT function with the execution halted at the first line. Let us go to the ORDER_MGMT package body. Let's remove the breakpoint we had put earlier. You can put breakpoints in the source code before starting the debugger session or do it after starting the debugger session. Of course your debugger has to be in the Step Into mode where it'll stop at the first line of the anonymous block allowing you to put more breakpoints. And while you are halted at one breakpoint, if you feel a need, you can put additional breakpoints in the code at that point of time. This time let us go and put breakpoints, one at IF l_item_value is less than 600, and the other at l_discount_ratio statement. Now in the Breakpoints tab we can see the persistent exception breakpoint, as well as our breakpoints at lines 10 and 15. Let us click on the exception breakpoint, right-click, Edit, and make sure that the option Break for Caught Exception is checked. This would allow the debugger to halt for both caught and uncaught exceptions. Click OK, accept the warning, and click Yes. Let us click on the red arrow to find the current execution point, which as we know is sitting at the beginning of the ANONYMOUS_BLOCK. Let us hit Step Over or F8. It will bring us to the BEGIN statement. It will not stop at the variable declaration unless we are doing some initial assignment over there. Let us hit Step Over again, and it will bring us to P_ITEM_ID is equal to 2. Hit Step Over again, and it will execute the expression and bring us to the line with the function call. If you go to Smart Data tab now, it will tell you the values of the two variables at this point of time. Since now we want to drill down, let us hit F7 or Step Into. If you would have hit Step Over or F8, it would have gone to the next breakpoint if it encountered one or else you would have completed the function call and gone to the next statement. Now it takes us to the first line of the function call. Let us hit Step Into again, and it stops at the l_discount declaration where it is assigning it an initial value of 0. Hit Step Into, and it takes us to the BEGIN statement. Hit Step Into twice, and it takes us to the get_item_value function call statement. Hit Step Into to drill down into the get_item_value function, and it takes us inside the function. Notice that if we had not compiled this function in the debug mode the debugger would not have been able to drill down into it. Now we can again hit F7 or F8 to go line-by-line or just hit Resume to go to the next breakpoint. Let's hit Resume. It goes to the next breakpoint on the IF l_item_value is less than 600 clause. Let us observe the value of l_item_value now in the Smart Data or the Data tab. It is 600, so this IF clause would evaluate to false. Let's hit Step Into again. It goes to evaluate the ELSIf statement. Let's hit Step Into again. It evaluates the ELSIf clause also to be false. Let us observe the value of l_discount in the Data tab now. It is still 0 as initialized since it did not get assigned a value in the IF or ELSIf clause. Let us hit Step Into again. Let us look at the Log window to observe how SQL Developer is logging information. It does log encountering the source breakpoints at lines 10 and 15. Let us hit another Step Into. We notice that now it has hit the exception breakpoint at line 15 meaning an exception has occurred at line 15. Let us go to the Data tab. We observe an exception _throw node. Let us expand it. We see the _sqlcode of 1476 and _sqlerrm of divisor is equal to zero since L_DISCOUNT is in the denominator. Let us hit Step Into, and the execution goes to the EXCEPTION block. Let's hit Step Into, another Step Into, and now it is at the DBMS_OUTPUT statement. Let's hit another Step Into. Let us go to the Log window. Even though it has completed the DBMS_OUTPUT statement execution, we do not see the output in the Log window because if you remember from our earlier discussion the output of the DBMS_OUTPUT statements cannot be seen until the execution of the block or the subprogram completes. Let's hit Resume. The execution completed, and now in the Log window we can see our DBMS_OUTPUT statements. So let us fix the logic by putting a less than equal to operator to cover the value 600 also. Let's compile that change again in the debug mode. In the next demo I want to show you what happens to the navigation when the program unit is not compiled in the debug mode, how to set conditional breakpoints, and how to conduct a lot of analysis.

Demo: What if Analysis & Conditional Breakpoints

We will see the effect of not compiling a program unit in the debug mode to the debugger navigation, how to conduct a lot of analysis, and conditional breakpoints in this demo. Let us compile the GET_ITEM_VALUE function in the normal non-debug mode. You can see the icon color changes from green to gray again. Let us start a debug session in the ORDER_MGMT package body for the GET_DISCOUNT function. The ITEM_ID is 2, which is a valid item ID. Let's remove all breakpoints and just keep one breakpoint at the beginning of the get_item_value call in the order_mgmt.get_discount function. Let us find our current execution point by hitting the red arrow. Let's hit Resume. The execution flows until it encounters the breakpoint at the beginning of the get_item_value call. Let's hit Step Into or F7 to drill down in the function. What happened? This time it just executed the function and comes to the next executable statement. Since we have not compiled a function in the debug mode, it cannot penetrate into it. At this point let's find the value of l_item_value in the Data tab. It is 600, and hitting F7 or Step Into at this point will evaluate the first IF clause to be true assigning a discount of $10.00. Since it's a part of our lot of analysis we want to see the flow when the value is 601. Let's right-click on L_ITEM_VALUE and do Modify Value. Let's make it 601 and click OK. Now let us hit Step Into again. So now the IF clause evaluates to false, and it goes to the ELSIf clause. Hit Step Into again. Since we modified the l_item_value to 601, the ELSIf clause of l_item_value greater than 600 is true taking it inside the ELSIf clause. Let's hit Step Into again. As we can see in the Data tab, L_DISCOUNT is assigned a value of $20.00. Let's create a breakpoint at l_discount_ratio by clicking in the left tab. It gets added to the breakpoints in the Breakpoints tab. Let's right-click and Edit. Let's go to the Conditions tab. Here we can put in the condition without the WHERE clause using the variables the session can see at this time. Let's put l_discount equal to 0. So what we are saying is that stop at this breakpoint only if the value of l_discount variable is 0. Since l_discount is 20, the condition will be false, and the debugger will not stop over here. Let's click OK. Let's hit Resume. The debugger does not stop at the breakpoint at l_discount_ratio, and the execution completes. This is how we can set conditional breakpoints. So in this demo we saw the effects of not compiling a program unit in the debug mode, did a lot of analysis, and also created a conditional breakpoint and saw its effect.

Summary

Knowing how to debug is an essential skill for any programming language. In this module we talked about various ways to debug our PL/SQL code. DMBS_OUTPUT is a built-in package with several subprograms available for debugging like the DBMS_OUTPUT.PUT_LINE procedure. This is a very simple way to debug, but makes us modify our code to inject the DBMS_OUTPUT statements every time we need to add more debugging information. You can use this for small unit tests. DBMS_UTILITY is another useful built-in package. We talked about the FORMAT_ERROR_STACK function, which like the SQLERRM function gives the error code and error message, but it allows for longer message lengths of up to 2000 bytes. FORMAT_ERROR_BACKTRACE is another important function of this package, which gives us the line numbers and the program unit names in the call stack propagation as the error occurs. You should put these calls in every exception block. Finally, we took an in-depth look at SQL Developer Debugging capabilities in terms of setup, permissions, various debugging views, and navigating options. This is the most powerful and flexible way of debugging, and you should use this as your preferred debugging option.

Course author

    
Pankaj Jain
Experienced technologist, with expertise in various aspects of software development lifecycle, architecting software solutions and software development.
Course info

LevelIntermediate
Rating
(59)
My rating
Duration4h 1m
Released20 Nov 2014
Share course

