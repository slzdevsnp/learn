
Code School
Oracle Performance Tuning for Developers
by David Berry

This course introduces developers to the fundamental principles of performance tuning when using Oracle as their backend database.

Start CourseBookmarkAdd to ChannelLive mentoring
Table of contents
Description
Transcript
Exercise files
Discussion
Learning Check
Recommended
Why Performance Tuning Matters

Introduction

Hello, my name is David Berry. Welcome to Pluralsight, and this course on Oracle Performance Tuning for Developers. Just about every modern application, in some way, shape or form, uses a database as its back-end data storage. And for many of these applications, Oracle is the database of choice, and that is no doubt the reason you are here. In some way, you work with an application that uses an Oracle database and you want to learn more about how you can achieve better performance with Oracle. In almost every system, the database is a major component of the system and has a significant impact on the overall performance of that system. But other aspects of performance tuning are important as well. A poorly performing database and data access layer will almost certainly doom an application to failure. I've seen instances where a single missing index was causing a sequel statement that should have run in sub-second time to take over 60 seconds to run. And I think we can all agree that very few users will be satisfied with any function that takes a minute or more to execute. So for a system to be considered functional, and to be accepted by its users, good performance in the database is a must. For that reason, I like to think about performance not just as an overall system engineering goal, but as a usability attribute. Normally, when we think about usability, we're thinking about things like interface design and how to make tasks easy for the user to accomplish. But if the system that we design is not very responsive, this will not be a usable system. Performance is also a productivity issue. Many of us work in corporate IT departments, and we develop systems for internal customers to use. If our data access layer performs poorly, and this leads to overall system sluggishness, the productivity of our users for the system is impacted. When you have thousands of workers at lower productivity, this not only looks bad for your system, but isn't very good for the bottom line. Finally, writing an application that performs well is important for cost considerations. And this is especially true for database performance. If the application you write doesn't take advantage of the proper constructs in Oracle and performs slowly, you may be looking at upgrading server hardware to support the higher amount of resources your application uses. If you are running Oracle in a cloud-based environment, your cloud provider will charge you based on the amount of CPU, memory, and other resources that your database uses. So here, it is even more important to develop efficient, well-performing applications, because you're directly charged for these resources that an inefficient application uses.

Performance Tuning is Not Magic

When I talk to people about Oracle Performance Tuning, I get a lot of interesting comments. Many seem to think that Oracle Performance Tuning is some sort of black magic, based more on sorcery than on engineering principles. Others simply regard their Oracle database as a black box, and don't know much about what happens in Oracle processes or SQL statement, or why things run fast or slow. Mostly what I find though, is a lack of understanding of how Oracle works and what drives Oracle performance. And this is unfortunate, because without an understanding of how this major system component works and behaves, it is much harder to design and deliver a system that performs adequately for its end-users. If you are not a pilot, sitting in the cockpit of an airplane can be a little bit overwhelming. There are all kinds of instruments, gauges, and controls that you have no idea what function they perform or what information they are providing you. But to the trained eye, these instruments provide critical information about what is happening and what the pilot should do to control the aircraft. In a lot of ways, this is very similar to Oracle. Oracle is a highly instrumented system. It makes a wealth of data available to us about what is happening inside of our Oracle database server, how a SQL statement is going to perform, and what is causing the statement to perform slowly. What we have to do is learn to read and understand this information that Oracle is giving to us. When we do this, we can take a very analytical approach to performance tuning, and have a good idea about the corrective action we should take when we do encounter a performance problem. The question always comes up, who should be responsible for database performance tuning? In many organizations, this has traditionally been viewed as a DBA responsibility. It is my belief though, that application developers should be skilled in database performance tuning and indeed should take the lead in performance tuning of their applications and the data access layer. There are several reasons for this. First, the application team knows data in the database very well. This includes how the tables are related, what data they contain, and how the data is distributed throughout the system. Second, they understand what the application is trying to do and how data flows through the system in order to accomplish each function. And finally, the application team is familiar with what the user needs and expectations are for each function. So, they are in a better position to evaluate alternative approaches to performing a task and the associated performance trade-offs. Now, it is possible for a DBA to be familiar with all of these items as well. And it is usually the application team who has the best perspective on each one of these elements. And in order to conduct really effective performance tuning, you need to understand not just SQL statements and tables, but what the application is doing, how it is trying to do it, and how all of the major components interact together. The second reason why you as an application developer should be well-versed in database performance tuning, is that you want to integrate the ideas of performance tuning and testing into your development process, and not wait to complete these tasks until the end of your project. By keeping performance in mind from the start of your project, you'll be able to design your application to avoid known performance pitfalls. You'll also understand how to examine the performance of each of your components individually, so you can get a measure of their performance early on in the project. And, designing performance in from the start allows the application team to take end-to-end responsibility for the experience that their application provides. What is needed then is to give application developers the training and tools that they need to create well-performing, responsive applications. And that's what this course is all about. To do this, the course is divided into four major sections. The first section will familiarize you with the basics of Oracle architecture and what you need to understand from a performance perspective. There's a lot of material already written on the subject of Oracle architecture, and much of this material dies right into defining details right away. Our aim though, will be to teach you the most important parts for what you need to know in order to performance tune your application. We'll also talk about the performance metrics and how we measure performance. Measuring performance goes past just looking at how long a SQL statement took to run. So we'll discuss the metrics you should be looking at and what they mean in this section. Finally, we'll talk about connecting to Oracle. Connecting to Oracle is something that we do all the time, but seldom think about. So, we'll talk about some of the best practices for our applications to use when we do connect to Oracle. Next, we'll move into a discussion around tuning individual SQL statements. The first topic will be on the use of bind variables in your SQL statements, and why this is so important with an Oracle database. We'll then move in to how to generate, read, and analyze execution plans for your SQL statements. An execution plan is how Oracle tells you how it is going to execute your SQL statement. So knowing how to read and interpret these results is critical for when you need to troubleshoot a slow SQL statement. The next section of the course will deal with Indexing. Having a properly indexed database is probably the most important factor in the overall performance of your data access layer. So we'll go through indexing in-depth. This will cover everything from the different types of indexes, how Oracle evaluates if it can use an index, to some more advanced capabilities that Oracle provides to address some of the special situations that may come up. Finally, we'll talk about some indexing strategies for your application and go through some of the common reasons why Oracle may not be using an index that you've created. In the last section of the course, we'll talk about how you can monitor what Oracle is doing in real time. This is invaluable, because you can see what statements are performing poorly while your application is running, and you'll know exactly where to target your tuning efforts.

Performance Tuning Approach

I want to take a moment to share with you my approach to performance tuning, and the approach I will take throughout this course. It is my belief that it's important not just to provide a list of performance do's and don'ts in this course, but to help you understand why Oracle works the way it does, and how this impacts performance. To have mastery of any topic, you have to understand the details, and this is especially true with performance tuning. Knowing what criteria that Oracle is evaluating with, looks at your SQL statements, and what basis it is using to make decisions, will help you better troubleshoot performance issues that you may encounter. And, it will help you be more proactive about designing in good performance from the start in your applications. If we think back to the image of the cockpit of an airplane that we saw earlier in this module, would you really want to get on an airplane with a pilot who only knew how to fly on autopilot? Or would you feel better if your pilot understood the principles of flight, aeronautical engineering, and thoroughly knew the performance in handling characteristics of his aircraft? I think for all of us, that is an easy choice to make. The approach you should take to Oracle should be the same. If you understand something about Oracle architecture and how Oracle uses memory, the life cycle a SQL statement goes through when it's processed and how an index works, you'll be much better equipped to analyze and solve performance issues that arise in your application. In this course, I'm only going to use basic tools that I can be assured of, that everyone has access to. Among these are SQL Developer, SQL Plus, and the V$ dynamic performance views built into Oracle. Tools that I will not be using or discussing include the Oracle SQL Tuning Advisor, the Oracle Automated Workload Repository or AWR, and any third party tools like TOAD. The main reason that I will not be discussing these tools is because not everyone has access to these tools. For example, the SQL Tuning Advisor and AWR are separately licensed product, and it doesn't do you very much good to watch a course about tooling that you don't have access to. The second reason is that these tools all contain functions to automatically analyze SQL statements, and in some cases, even the entire workload from a database can recommend a performance tuning solution. And that is all well and good, but the point of this course is to learn those fundamentals for ourselves, not learn how to press a button. We want to be able to interpret this data for ourselves, and make sure that the suggested course of action from any tools that we use is the course of action we that we should take. To go back to our airline pilot analogy, we want someone who knows how to fly the plane when the autopilot is turned off. This brings me to my final point about tooling and the approach to take to performance tuning. Even today, the best performance tuning tool ever invented is you, a knowledgeable thoughtful technology professional who can analyze different information about what is happening in a system and come up with a creative solution to address these issues. Tools will never know the business problem your application is trying to solve, or how they might re-factor a table, or some functionality to take an entirely different approach to a problem. All they can do is run a set of predefined rules. But you can provide a level of critical thinking that no tool can ever provide. And it's this critical thinking that's really going to make the difference in the performance and the quality of your application.

Performance Tuning Principles

There are two performance principles that will come up over and over again throughout this course. So, it is worthwhile to introduce these principles now so you can keep these in mind throughout the course. The first principle is that in order to improve performance, your primary goal should be to have Oracle process less data. Oracle is built to handle large volumes of data, this is true. But what it is really built for is to have the ability to quickly eliminate all the data that doesn't apply to a particular operation, and quickly locate the data that does. If Oracle or any database has to read millions of rows off of disk, store this data in memory, and then sequentially scan through all of this data to perform some sort of comparison against the data, this is going to take not just time, but a lot of system resources on the Oracle server. And this is going to be reflected in how long it takes to complete your SQL statement. Contrast this to if you could use index operations, such that rather than loading and processing millions of rows of a table, you could directly go to the hundred or so rows that interest you, and just load and process these rows. Clearly, the second example is going to be much faster. And so that is what you want to help Oracle do, is to efficiently and quickly locate the data of interest we need for a particular SQL statement without having to look through tens of thousands of rows that are of no interest to us. This brings up a corollary to our first principle, and that is we want to minimize any amount of wasted work that our Oracle server does. If you ever have done any reading about lean manufacturing or lean software development, you know one of the ways to reduce cost is to eliminate the wasteful processes that aren't producing any value. The same is true in Oracle. We don't want to read millions of rows from a table when 100 rows will do. We want to write our SQL statement so that Oracle has the best chance to take advantage of our data being cached. So our data doesn't have to be reloaded from disk in an expensive disk operation. And we want to do things like use mind variables, so our SQL statements don't have to be reparsed each time they're submitted, wasting precious CPU resources. By intelligently designing our application, we can eliminate a lot of the rework and unnecessary work that goes on in our database. And that helps Oracle get the answers we need faster and have more processing power available to do the productive work we need Oracle to do. The second principle in database performance tuning is that you want to take a very analytical, numbers driven approach to the problem, and let data drive your decisions. Oracle gives us a lot of data about how a SQL statement is running and how the database is running as a whole. So, you want to look at this data to see what insights it gives us into where the problem is. When we do decide to make a change, like rewriting a SQL statement or adding an index, we can use this data to compare before and after results to know if we've improved the performance of our operation, and by how much. Another reason that it is important to take an approach based on data is that everyone's application and database are a little bit different. This could be the amount or distribution of data in your Oracle database, or perhaps a special use case that you have in your application. But by taking an approach of MeasureIT, you will be confident that you're making the right decision because that decision is based on data. To summarize, we'll use an analogy. Think of watching a world-class athlete perform. There's no wasted motion, no wasted effort or energy. This is how you want to write your data access code. Minimize the amount of disk I/O, memory, and CPU that is wasted, so that all of your effort is going into the productive work of running your SQL statement. And just like world-class athletes take a very scientific and data-driven approach to their training, you want to take the same approach to your performance tuning efforts. In this course, we'll learn the skills you need to accomplish both of these goals.

Sample Database

During this course, the SQL statements and demos you will see me use are from a sample database of a fictional university. This database has a table named Applications, where we keep data on students who have applied to our university, and a table named Students for students who have enrolled. We also have a table called Course_Offerings, and this table keeps track of the courses and sections that are offered in each term. And finally, there's a table called Course_Enrollments that keeps track of what students have enrolled in what courses and what their grades are. And of course, we have numerous smaller tables, like Departments, Courses, Terms, Degrees, and Degree Requirements, that support all these other larger tables. When working with a database, it is nice to have an ER Diagram of the database, so you can visualize the relationships between various tables. And so if you're the type of person that likes to look at an ER Diagram, you can download an ER Diagram for the sample database at my blog using the link that is shown on the bottom of this slide. If you want to load the sample data for this course, you can do so following the steps on this slide. These steps are also included as a Read Me file in the downloads for this course, so you don't have to pause the video and write these down, they'll be in that demos file. Loading the data is accomplished using the Oracle Data Pump utility. If you already have a data directory set up in Oracle, then you don't need to create a new directory as in step 2 and 3. Just make sure that the schema user that you want to use has permissions to that Oracle directory object as shown in step 3.2. Finally, the name of the user that I used was student. So by default, this is where Oracle wants to import the data. If you want to use a different username to import the data, then you need to make sure to include this remap_schema parameter when you call the impdp utility. Otherwise, you'll get a whole bunch of nasty errors. We can briefly demonstrate how to load the data. I'm here in a PowerShell prompt as I'm using Windows, but you could also use a DOS Command prompt. Or if you're on UNIX, you'd be using a Command Shell. In any case, the steps remain exactly the same. I'm in this directory, and then I'll run a directory command to show you that I do have my data file out here that I want to load. The first thing I need to do is I need to log in to Oracle as a SYSDBA user, so I'll do that using SQL Plus. If you're on a personal machine, this is a step that you would do. If you're on an Oracle server, say, that you're at work, these next couple of steps are probably steps that your DBA would complete for you. So I'll go ahead and login now. And now, what I want to do is I want to create the Oracle directory object. What this is, is it's a pointer in Oracle to the directory on the file system where Oracle can import and export files. There we go. And now, I need to grant permissions to the user who is going to import the data. And in this case, the name of that user is student 3. So I'll go ahead and do that now. That is all the work that needs to be done as a SYSDBA user, so I'm going to go ahead and log out of Oracle. And now, I'm going to run the impdp utility which is part of the Oracle data pump import. So here is my command, and when we look at this command, the first item that we see, we see the username up here, student 3, this is the username that we're going to log into Oracle as. And then we see this remap_schema parameter. And the reason that I need that is because I'm going to import the data as a different user that I exported the data as. So when I exported this data, if you download the demo file for this course, that data was as a student user. If you're using anything other than a username student, you need to include this remap_schema parameter as I've done here. Student will go in the first position because that's the user that the data was when I exported it. And then in the second position, you'll put the name of the user where you're importing that data to. In my case, that is student 3. So I'm going to go ahead and I'm going to run this command. And it does take about 90 seconds. So I'll stop the video and then come back when the import is finished. So there we go. Now our data is imported, and if we log in to Oracle, we'll be able to see our tables over here. And there they are. So that's all it takes in order to import the demonstration data into a database, so you can work with it and experiment with it however you would like.

Oracle Architecture and Performance Basics

Introduction

Hello, my name is David Berry. Welcome in this module on Oracle Architecture and Performance Basics. In this module, we are going to cover some of the foundational concepts you need to be familiar with in order to understand Oracle Performance and to effectively performance tune your application. There are three primary areas this module will cover. First, we'll discuss some concepts around Oracle Architecture and especially around how Oracle uses memory. If you're going to write effect Oracle applications, it is important to know a little bit about how Oracle's architected so you can better understand why Oracle works they way that it does. Second, we're going to discuss different Performance Metrics. Usually, when we think of performance of a function or of a SQL statement. We're just thinking about the amount of time it takes to perform that operation. Well, this is important. There are some other Performance Metrics that we should understand that provide more insight into how efficient our statement is and better guide our tuning efforts. Finally, in this module, we're going to talk about performance testing your application and especially about what makes a good Performance Test Database. You want to make sure to have a good Performance Test Database available, so that you can have confidence that any results you obtain from your tuning and testing efforts will translate into your production environment.

System Scalability

In working with an Oracle database, it is useful to understand a little bit about the context in which an Oracle database operates with respect to other application tiers. Today, most line of business applications are developed as web applications or make use of some sort of Sirius orientated architecture. If we think about how these applications or web services are deployed, what we probably have is some sort of cluster of servers. And each server is running a copy of the application, whether that be a web application or a web service. It doesn't matter if this cluster is composed of physical or virtual servers. The point is that we have multiple servers to handle the load. Having a cluster of servers to run our application like this serves two purposes. First, if one server crashes or has to be taken out of the cluster for maintenance, we have redundancy and our application doesn't go down just because we lost a single server. Second though, our cluster is an example of horizontal scaling. That is to support more users or more load rather than a single very powerful server to handle that load, we employ a large number of smaller servers to handle the load. If our load increases, we handle this by adding additional servers into our cluster. At the database tier, the story is different. At least as of today, when I am recording this. Generally, what we do is engage is vertical scaling. That is we have a single very powerful machine that serves as the database server. This machine has many times the amount of CPU and memory of any of the servers in our web cluster and is also connected to some sort of very fast disk, like a Storage Array Network or SAN. If we need to support a higher load, then we upgrade our database server to add additional CPU's, add additional memory or even expand our storage area network so that the data is distributed across more disks. One of the main reasons for this is that the data in the database has to be consistent. That is the data has to be in sync. In keeping that data in sync between multiple servers is a very hard problem to solve. Now Oracle isn't just used as the back end database for a line of business applications that may have some sort of interactive front-end. We also have to consider Batch Applications that run very high volume workloads and need to finish within a specified processing window. And we have Business Intelligence Applications, which read and process large amounts of data. The common threat amongst all of these uses though, is that we usually have a very large, very powerful Oracle server that is used as the back-end of these applications. And we are starting to see more movement in this area where you are able to build database clusters using multiple servers, as opposed to one very large powerful server. But for right now generally, you will see vertical scaling used in the database tier. So you'll have a very powerful machine as your Oracle server. As a result of needing this port, a large number of concurrent users and be able to retrieve data for these users in a very efficient fashion or to support very data intensive applications like Business Intelligence Applications. Oracle is what you would consider a memory intensive application. The server that Oracle is running on will have many, many times the amount of memory that a PC or even a web server has. And Oracle will essentially consume all of the physical memory that is not devoted to the operating system. What does Oracle do with all that memory? First of all, Oracle makes extensive use of caching. And secondly, Oracle has to have working areas of memory to perform operations like joins or sorts in order to fulfill the SQL statements that it is processing. So, let's look into how Oracle uses this memory in more detail.

Oracle Architecture Overview

If you open a book on Oracle or take a look at the Oracle Concepts Guide, you are likely to encounter a diagram that looks something like this. That outlines the major memory structures used in Oracle. This diagram is actually a simplified version of the diagram that is found in the Oracle documentation. But it includes the most important pieces that we need to understand as developers. You can roughly divide the memory used by Oracle into two sections. The first section is the System Global Area or SGA as it's often referred to. This is a memory structure that is shared across all users and all processes in Oracle. So when you here SGA, think of data that can be shared and reused amongst users and processes. The second section of memory is the PGA or Program Global Area. The PGA is memory that is dedicated to each individual session or current login in Oracle. When you hear PGA, think of data that is private to a log in session. Because each segment of memory in the PGA is dedicated to a particular login session. Of these two, the amount of memory devoted to the SGA is typically much larger. To give you an idea of the size of an SGA. Oftentimes, the SGA will use 80% or more of the amount of physical memory on the Oracle database server. So if your Oracle server has 96 gigabytes of RAM, you may very well have an SGA slice of 75 or 80 gigabytes. Which at the least at the time of recording of this video is a very large memory structure. The amount of memory used by the PGA is much smaller, but can become a concern if you have very large members of sessions that connect to Oracle. So as developers, we want to design applications that efficiently manage our connections to Oracle, so we aren't wasting memory on the Oracle server. The three most important areas of memory in Oracle to understand as a developer are the Buffer Cache, the Shared Pool and the PGA associated with each login session. So in this section, we'll concentrate on understanding how these three memory structures work. As developers, when we are writing SQL, we most often think in terms of rows. How many rows is my query going to return? I need to insert this many rows into the table or how many rows did my delete statement impact? Rows are important to Oracle as well, but Oracle is really working in blocks. A data block is the smallest level of granularity that Oracle will store data in. Block sizes can range from 2 kilobytes to 32 kilobytes with 8 kilobytes being the default size. When you think of a block, think of it as a container for data. A block can contain data for either a table or an index. But it is probably easiest to understand if you think of a block that contains table data, as is shown here. Each block is going to have a Hlock Header, which is going to contain some administrative data about the block. Like if this is a table or index, what is the name of the object that the block contains data for? And a directory of rows contained in the block. Typically, the Block Header is going to take somewhere around 100 bytes. Sometimes a little more, sometimes a little bit less. The rest of the block, if this table block is going to contain the data for the rows contained within the block. As shown here, each block will typically contain many rows. Oftentimes hundreds or even thousands of rows. So what Oracle is doing is trying to first locate the block that your data is in. And then locate the row inside of the block. You also noticed in this diagram that there are some gray areas within the block and those represent free space. When you create a table, you can use the PCT free and PCT use parameters to control how much free space is left in a block. But generally, it is a good idea to have some free space for a couple of reasons. First, we might update a row. And as a result of that update, the row may take up more space. If we have some free space at the end of the row, then the row can simply expand into that free space. If we didn't have any free space, Oracle would have to move this row somewhere else in the table maybe to a different block. And that would cause additional IO operations to occur, so that isn't very efficient. Second, having some free space at the end of the block allows some room for Oracle to insert new rows as they're added. Later in this module, we'll talk about the Performance Metrics of logical IOs and physical IOs. Both of these metrics are reported in terms of blocks read. For now though, let's look at how Oracle reads the data that it need to.

Buffer Cache

Whether your SQL statement is a select update, insert, or delete. Oracle is going to have to locate the data block or blocks that it needs to operate on. To do this, Oracle is going to look in an area of memory known as the Buffer Cache. The Buffer Cache is a very large cache of data blocks that Oracle maintains in memory for quick access. And this includes data blocks for both indexes and tables. Just like you might have a cache in an application that you write that keeps the most recently and most often used data in memory. That is what the Buffer C`ache is for Oracle. Being able to cache data blocks is extremely important to the performance of any database and Oracle is no exception. Even though there have been tremendous gains in the performance of disk systems over the years, it is still generally many orders of magnitude faster to read a block from memory than from disk. As such, the Buffer Cache will be far and away the largest component of the SGA. So when you run a SQL statement, Oracle will check the Buffer Cache for the blocks that it needs and read any of the blocks it can from the Buffer Cache. If there are data blocks that are needed that are not in the Buffer Cache, the Oracle will go to disk and execute physical IO operations to read those blocks into the Buffer Cache or they can be utilized by your SQL statement. Oftentimes, the statement will get some of the blocks it needs out of the Buffer Cache. But also requires some physical IO operations to occur, as well. Well, there is always going to be some physical IO required from a performance perspective. We always want to write SQL statements that have to do as little physical IO as possible, because physical IO is so much slower than reading from the Buffer Cache. You may have experienced this yourself at times. If you have ever executed the same SQL statement twice in a row, you've probably observed that the first time you rang your statement, it may have taken a second or two to execute. But on the second execution, you got your results back almost immediately. There are a couple items at work here. But the biggest factor is the Buffer Cache. The first time you executed your SQL statement, Oracle probably had to perform some physical IO to get the data that you needed. But when you executed the statements the second time, the data was already in the Buffer Cache. So Oracle could return your results much faster. The size of the Buffer Cache is finite. After all, there is a finite amount of physical RAM on your database server and swapping the Buffer Cache out to disk would defeat the entire purpose of having an in memory cache. So Oracle manages the box in the Buffer Cache using a least recently used algorithm. Meaning, the oldest and least frequently used data blocks will get cached out when oracle needs to move new blocks into the Buffer Cache. Oracle manages all this for you. Just know that data blocks are always being moved into and out of the Buffer Cache as need be.

Shared Pool

The second area of memory that we want to look at is the Shared Pool, which is also located within the SGA. Whenever you execute a SQL statement, Oracle first has to parse that statement. Check to make sure the tables and columns referenced in the statement exist. Check permissions on database objects and create a plan for how Oracle's going to execute the statement. Well, this process happens very quickly from the individual statement. Remember, Oracle is processing hundreds if not thousands of statements every second. And so it is beneficial to cache as much of this information as possible. So that if a SQL statement is executed multiple times, Oracle can simple look up the information it needs rather than having to re-compute it each time. This is what the Shared Pool is all about. First, there's the Dictionary Cache. This is an area of memory where Oracle caches information on database objects, like what columns a table contains or what indexes are available on a table. Then there's the Shared SQL Area, which caches SQL statements and their execution plans. In a later module, we'll talk extensively about the Shared SQL Area. Is how you write SQL statements can have a dramatic effect on the performance and scalability of your application. For now, understand that every SQL statement you execute, the Oracle optimizer will go through steps to determine the most efficient way to execute the statement. Those steps are known as an execution plan and Oracle will cache this plan in the Shared SQL Area. So that if the statement is executed again, it can just use the cache plan and not have to repeat this computation. Finally, there's an area of memory known as the Reserved Pool. There are times when Oracle needs a large contiguous block of memory in either the Data Dictionary Cache or the Shared SQL Area. But because of ions moving in and out of both caches, such a continuous block of memory is not available. So the Reserved Pool exist as an area memory that Oracle can borrow some memory from to get a continuous block of memory that it needs for a short term operation. This is nothing that you, as the developer need to worry about. Oracle handles all these internally. It is simply included on this slide for the purpose of completeness.

Program Global Area

The last area of memory that we're going to take a look at and that is important to us as application developers is the Program Global Area or PGA. Whenever an application connects to Oracle, Oracle creates a session for the connection, much like an operating system will create a login session if you were to connect to a remote server. This session in Oracle has memory associated with it, and this memory is part of what is called the program global area. Now unlike the areas of memory that we just looked at in the SGA which are shared across all users and all processes in Oracle. The memory associated with an Oracle session is private. This block of memory will only be used by its associated login session. What does Oracle use this memory for? There are two primary areas. The first area is the SQL work area which just like the name implies is a work area for any operations that need to be performed by SQL statements executed by this session. So if a SQL statement has to perform a sort this is the area of memory that Oracle will use to perform that sort operation. If a SQL statement has to create a hash table in order to perform a hash join operation, again, it is this block of memory associated with this individual session that will be used to perform this work. The second area of session memory is what is known as the Private SQL Area that contains information relevant to SQL statements being run by this session. This includes information like the bind variables used in a SQL statement and state information about the SQL statement itself. What is important to realize is that every connection to Oracle does take up some memory on the database server. The amount of memory used depends on a number of factors. If you're just executing very simple SQL statements. Then the memory associated with this log in session will be relatively small. If you have queries that are doing large has joins and sort operations though, these log in sessions will take up much more memory. We'll talk more about this in the next module when we discuss connection pooling, but what you want to take away from this discussion is that every connection to Oracle does use some memory on the Oracle database server. So you don't want to open up a lot of unnecessary connections if they're never used by your application. because you're going to be wasting memory on your Oracle server, and this is memory that Oracle could use for other purposes.

Oracle Architecture Wrap Up

Let's summarize the important takeaways of what we need to understand about Oracle architecture and especially how Oracle uses memory. The first takeaway is that Oracle makes extensive use of caching to improve performance. This includes caching not just the data that your sequel statement may need to read or update, but also administrative information like table and index definitions and execution plans. Oracle contains a sophisticated algorithms to manage each cache, and as a result you can achieve a much higher throughput because Oracle's able to minimize how often it needs to reform expensive operations like reading data from the disk or re-computing an execution plan. The second takeaway is that we as developers, as much as possible, want to create applications that can take advantage of how Oracle catches data. In the module on bind variables in this course, we'll discuss why using bind variables maximise the statement and execution plan reuse, and why this is so critical for performance. And then, throughout much of the rest of this course, we'll talk about how sequel statements actually execute and how we can use indexing strategies to cut down on the amount of data a sequel statement needs to access. By cutting down on how much data a statement needs to read and process, we increase the likelihood that the majority of the data blocks the statement needs are already going to be in the buffer cache. And this will result in improved performance. The information presented here is just the proverbial tip of the iceberg as far as Oracle architecture is concerned. There are numerous books that have been written over the years on the subject, but for a good start I recommend the Oracle Database Concepts Guide. You can access in either PDF or HTML format at the following links. The guide is very informative and is also very approachable in terms of readability so it is a good document to have in your library to further your understanding of how Oracle works.

Performance Metrics

The goal of this course is to help you write SQL statements and data access code that run faster and are more efficient. To do so, we need to ask ourselves what are the types of performance metrics that we should look at while we are tuning our applications so that we can objectively know that we are improving performance? The first performance metric to discuss is the performance metric that most people are most familiar with and that is elapsed time. Elapsed time is simply the amount of time that statement took to execute. Elapsed time is an important metric because this is how our end users think of performance. They will say to us that a certain webpage needs to load in three seconds or less or the report they're trying to run is taking twenty minutes and that's too long, and to the surprise of no one, it's always our goal to minimize the amount of elapsed time that a SQL statement or indeed any operation takes. Think about it, when was the last time one of your users asked you if you could make something run slower? When we are tuning SQL statements, though, while minimizing the amount of time each statement takes is our end goal. There are other metrics that turn out to be more useful. And the reason for this is that the amount of elapsed time that a statement takes is really a product of a number of other performance metrics and factors. That is elapsed time tells you the final result, but it doesn't tell you why the operation took that long. So we need to be able to look at other performance metrics to figure out what is driving performance. Secondly, we have to be careful when we're looking at elapsed time results, as they can be impacted by a number of external factors that are beyond your control as an application developer. For example, how long a statement takes is dependent on how busy the database server is when a statement is run. If a statement is run during peak load, or while a database backup is being taken. And the statement is naturally going to take longer. But this isn't in response to anything that we're doing in our application code or anything that we have control over. So we want to look at elapsed time as an end result, but for actually understanding what is driving the performance of the SQL statement, you want to dig deeper and look at other metrics. Probably the best metric to look at for judging statement performance is the number of logical IOs that a statement performs. Quite simply, the number of logical IO operations is the number of read operations that a SQL statement performs from the Oracle buffer cache. If the buffer cache does not contain the data that the SQL statement needs,aAnd there will also be an associated Physical IO operation for the statement. But no matter what, when Oracle needs a block of data, a logical IO will occur. So, in this way, the number of logical IOs give you a good indication of how much data Oracle is having to process in order to execute your SQL statement. The amount of data Oracle has to process in order to complete your SQL statement will affect the performance of the statement in a number of ways. First of all, if you have a high number of logical reads, it's highly likely a significant portion of the data blocks that your statement needs won't be in the buffer cache. And therefore Oracle will have to go to disc and read this data off of disc. As we all know reading data from disc is much slower than reading data from memory especially a large amount of data. Second Oracle is going to have to process the data for all of those blogs this could be filtering the data to look for only the rows of interest. Joining the data to another data set or sorting the data. But regardless this is going to take both time and CPU to perform these operations. Intuitively we know that any time we have to process a larger data set no matter what technology it is that we are using this is going to take longer and use more resources. And this same principal is true in Oracle. The other fact to consider when evaluating the number of logical IOs a statement is having to perform is the fact that this is a consistent metric. That is if you run this statement over and over again you're going to get the same number. And this number isn't going to be affected by things like other users on the system or what data happens to be in the buffer cache at that moment, and so when you start tuning a SQL statement. And you'll see a change in the number of logical IOs performed. You know that this change in the number of the logical IOs is in response to the change that you made in your tuning efforts and not in response to some external event. And in performance tuning, this is very important. Historically, Oracles use the term consistent gets rather than logical IOs. To refer to the operation of reading a data block from the buffer cache. Recently, however, Oracle has started to use the term logical IOs more often. And the term logical IOs is a little bit more standard across different database platforms, so that is the term that I'll be using in this course. Just know when you do read the Oracle documentation or see the term consistent get's used in some of the Oracle tools or statistics this is just another term for logical IOs. So don't be confused by that. So how do we see the number of logical IOs that a statement is performing? There are a couple of ways that Oracle makes this data available to us. First, in tools like SQL plus and SQL developer, we can use the autotrace command to execute a statement and get a detailed performance report, and included in this report are the number of logical IOs. In fact, in SQL developer, we'll even get a breakdown of the number of logical IOs by operation performed in the execution plan. So we can target what specific operations are reading and processing the most data. All of this is demonstrated in detail in the module in this course all on statement level performance tuning. Every time Oracle executes a statement,. Oracle will also keep detailed statistics of the execution of that statement and these statistics are summarized and available to you in a series of views known as the V dollar sign views. Most notable for us is the V dollar sign sequel status view. According to data from this view, we can easily see what in the database are performing the most logical iOS on average. And then target these statements for performance tuning. You can also see other statistics, like the average amount of CPU time used, the average elapsed time a statement took, and the number of times a statement has been executed. How to query this information out of the V$SqlStats view and other related views. Is covered in the module named Monitoring Oracle Applications. In either case we'll be able to a get a good read on how much data a statement is processing. And then target the correct statements for performance tuning based on this data. Let's take a moment to talk about how CPU is used by sequel statements and Oracle in general. Oracle servers handle a large volume of requests, so it's not unusual to see 80 or even 90 percent of the CPU being consumed on the database server at any time. Of course, if the amount of CPU used on the server goes to 100 percent, then most likely we have some processes that need CPU resources that aren't able to get it, so these processes will have to wait until CPU becomes available. And so like any other resource, we want to make sure that we're using CPU as effectively and efficiently as possible so we aren't introducing unnecessary bottlenecks into our system. There are two main areas where a sequel statement uses CPU resources. The first of these areas is in parsing the sequel statement. Every SQL statement submitted to Oracle must be parsed and have an execution plan created. And as it turns out, this is a CPU-intensive process. There are also some implications around getting access to the shared SQL area, but if you aren't using bind variables correctly, you will encounter latching issues on your Oracle server and this will cause a lot of CPU resources to be wasted on the database server. In the module on bind variables this is discussed extensively. But in short you want to make sure that you're always using bind variables in your application. Otherwise you're going to be wasting a lot of CPU resources. The second area where CPU is used is the actual processing of queries. Every time Oracle has to perform a comparison, a join operation or sort a data set, this take CPU. The more data that is involved in each of these operations, the more CPU will have to be expended in order to complete the operation. So here again, we look at both the amount of CPU being used but also the number of logical reads. Because if we can reduce the size of the data being operated on, we'll save CPU. To wrap up this discussion of performance metrics, while our ultimate goal is to reduce the amount of elapsed time it takes for each statement to run. What we really want to do is look at other performance metrics in order to guide our tuning efforts. When we look at other metrics, like the number of logical IOs a statement is performing, they are really helping understand how efficient the statement is. If a statement is having to read and process a lot of data, this statement is going to have to perform a larger number of physical IO operations and use more CPU. All of which will take more time. If all of the data is needed in the final result set, that is okay. And that might be the best that you can do for that statement. But if Oracle is having to read and process through a bunch of data that ultimately won't be used, this is wasteful and inefficient. It is inefficient in the fact that this extra work will make your SQL statement take longer, and force your user to wait. And it is inefficient because it is consuming additional resources on the Oracle database server. So this might cause a performance bottleneck that causes other statements to run slower than they should in your application. And so, as you start to think about performance tuning you want to start to think in terms of statement efficiency rather than purely how long a statement takes to execute. As it is, statements that are efficient and only use the minimum amount of data that they need that deliver the best performance.

Performance Tuning and Database Size

Let's take a few minutes to discuss some test environment considerations for where you tune and test your sql statements. And, for where you will perform in the application level performance testing. Not having an appropriate test environment set up to do your performance tuning and testing is one of the most common mistakes that is made when trying to performance tune an application. So let's see what the important considerations are so you can make sure that the performance tuning that you do on your application is as effective as possible. One of the primary considerations that you want to keep in mind, is the size of the database that you're using to test your applications and perform any performance tuning activities. Too often times,. What happens is that both in our development and test regions. The database used in these regions contain only a fraction of the amount of data that is contained in production. Well this may be sufficient for application development and functional testing, it doesn't work for any sort of performance testing or tuning. And so the result is that your application runs very fast in your development test regions. Only to encounter performance issues when it's deployed to production. There are a couple of reasons behind this. First of all, hardware has gotten fast enough these days that for a small database almost every statement runs relatively fast. If most of your tables just contain just a few thousand rows, it may be that Oracle is able to keep the entire database in memory. So you never incur any disk access, which, of course, is highly unlikely once you move to production. Also, modern processors have gotten fast enough that even if Oracle has to sequentially scan through a few thousand rows in a table, this is not big deal. But when the same SQL statement runs in production, now you're scanning through hundreds of thousands or even millions of rows and this is a problem. There is a second reason it worked though and this is the fact that Oracle may have actually used a different algorithm to execute the SQL statement between the two regions. When I say that Oracle use a different algorithm, don't worry. This isn't a malfunction in Oracle or any sort of defect. It is actually designed behavior where Oracle is evaluation the most efficient to process your SQL statement, but this can and will vary depending on the data in your database. So let's understand why this is. As application developers, we are most familiar with a programming paradigm known as Imperative Programming. This is the paradigm that we use in languages like C, C++, C sharp and Java. In imperative programming, we are not just defining what we want our program to do. We are defining how our program will perform that action. For example, thinking of a sorting algorithm. They are just saying sort this array of data. We are also providing the implementation of the algorithm to use. Whether a shell sort, a bubble sort, quick sort or some other algorithm. The important point though, is that we are not just specifying the result that we are going to get back. But also, each step in the algorithm to be performed. So regardless of the input data that's provided, the algorithm remains the same. When you write SQL, you're using a programming paradigm called Declarative Programming. In Declarative Programming, you are only specifying the result that you want. It is up to some other entity to determine how specifically to achieve those results. In Oracle, what this means is that your SQL statement. Let's say, a query is saying, I want to get back these columns from these tables and here are my where are clause. This is the matching condition to use to determine what rows I should get back. What happens then is that the Oracle optimizer examines your query and determines the most efficient way to perform that query. This could be scanning the table directly for matching rows. Or could be by first using an index to find the rows and then reading the table. If you have a join in your query, the Oracle optimizer will choose amongst several options for the best way to join the data together. The important point though, is it is the Oracle optimizer that is making the decisions about the specific steps in the algorithm to use to perform your query and to do so in the most efficient manner. And what steps are used may differ depending upon on the data that is involved. What this means is that in order to conduct effective performance tuning and testing, we need to create a test environment that effectively replicates our production environment. This environment needs to contain a similar volume of data as our production environment, as well as a similar distribution of data. Because the distribution of data can also affect how Oracle will execute your SQL statement. Otherwise, if we run a SQL statement in two different environments that have differing data. Either by the amount of data or the distribution of data, you're probably going to get different answers from each environment. Both answers will be correct with respect to the environment that they were run in, but the results won't translate from one environment to the other. By having a test environment that mirrors production, we can be confident that any tuning or testing we do in this environment will be representative of the results that we would get in production. So when we analyze a SQL statement or application is running or we evaluate an indexing strategy that we are considering, we know that the results we're getting in our test environment will translate to our production environment. And this is the whole point. We want to have a test environment where we can get a good read on the performance of our application. And then work to improve our performance by trying out different solutions. So it's absolutely critical to have a test environment where we can perform these type of activities.

Building Effective Performance Test Databases

Now that we understand why it is important to have a full size test environment that accurately represents our production database. Both in terms of the amount of data and the distribution of data. Let's turn our attention to how we create such a database in one of our test environments. There are two main approaches that you can consider. The first approach you can take is to copy data down from your production environment into one of your test environments. There are a couple of advantages to this. First, there are tools built in to Oracle like the Data Pump utility that are well-equipped to import and export large amounts of data. Second, from a development point of view, we know that we are getting data that matches the data that we have in production. Because after all, we are using our production data in one of our test regions. And finally, if you work with your DBA, it isn't hard to create some scripts that can automate this procedure so you can refresh your test database on a regular basis. So what do you want to be careful about? Most importantly, this is your production data. And you have to be careful as it could contain sensitive information that really isn't appropriate to have in a Test Environment. There may indeed be requirements around auditing of who has access to this type of data or about limiting access to a small subset of individuals and this tends to be hard to do in a test region. So you'll probably be confronted with the problem of needing to scramble or mask some of the data. This could include items like Social Security Numbers, bank account numbers, date of birth and perhaps even items like email addresses and phone numbers. And for some applications like health care. This list may be much, much longer so the entire approach may not even be feasible. So if you decide to go with this approach, make sure to work with your IT security professionals to determine what steps need to be taken to protect any sensitive information before you copy this data down into your Test Environment. The second approach you can take is to use one of the commercially available data generation tools. You can configure these tools to point at your database schema and give them information about the type of data to generate and the appropriate ranges of that data. And they will go about and generate any amount of data that you need them to. The advantage here is that now, you don't have any sort of risk of information disclosure from your test environment, because all of your data is synthesized. You can also generate any amount of data that you need. So if you're a small company or a start up and you only have a small amount of data in production, but you're expecting some rapid growth. You can use these tools to synthesize a large amount of data, so you can conduct performance tests prior to that growth occurring. The down side of these tools is obviously, there's a cost associated with them and you need to acquire the tool and install it. And secondly, you will need to invest some time in order to configure the tool and make sure that you're getting the right distribution of data for your database. Overall though, using a data generation tool is a probably the better route to go. If for nothing else and it minimizes the risk of any sort of accidental information disclosure. Information security and the need to protect sensitive information, continues to become a bigger and bigger issue for all of us. And so for that reason alone, it's probably worth the time and effort to start learning about the data generation options that are available.

Performance Tuning Database use Scenarios

So now that you have built a good performance test database that accurately replicates your production database, let's discuss how you can use this database to make your development process more effective. First of all, when you're developing your application, you're going to have SQL statements that you want to get an idea of their performance and how Oracle is processing the statement. So this is the first use of your Performance Test database. You will want to take these statements and then generate execution plans and actually execute the statements in this full sized database to get a performance baseline. How to do this is covered in detail later in the course in two modules. The first being named statement-level performance tuning and the second being execution plans in-depth. So what you'll want to do is apply the techniques discussed in those two modules to better understand how these statements are going to perform. If you run a statement in your Performance Test database and the statement has a high cost or is doing a lot of IO, you probably are going to want to engage in some performance tuning. It's going to involve rewriting parts of the statement or modifying some of the indexes or perhaps even redesigning how your application is going to perform a task. And so this is the second use of this Performance Test database is so that you can engage in this type of performance tuning, while you're still in development. It is in this sense that performance tuning is a highly iterative process. You're going to run your SQL statement and get some baseline performance values. This would include the statement cost, the baseline number of logical reads and an estimate of the amount of time it takes to run that SQL statement. And then you're going to engage in some performance tuning. Let's say, for example, adding an index. And then we're going to execute our statement again and measure the impact of the change that we made. This allows us to judge how effective our change was and if there's more that we should do or if this change is good enough. And then you keep iterating in this fashion. Sometimes, a change that you'll make will have no impact. You may add an index but for some reason, Oracle doesn't use the index. So then you head back to the drawing board, reanalyze your statement. And maybe come up with a different index that you can use. Now if all this seems a little bit overwhelming, don't worry. That's what the rest of this course is about. And in fact, we'll go over in detail what makes a good index. What makes a bad index and why Oracle will or won't use an index. In addition to tuning individual SQL statements, you also want to conduct Component Level and application Level Performance Testing. And of course, you want to do this against your test database that reflects what your production data looks like. This serves several purposes. First, it should validate any statement level tuning that you did. And second, it exercises the entire component. So if there are other pieces of data access code that were not tuned at a statement level, you'll get an idea of how that code is performing. Finally, you'll get to see the entire component working together and this might indicate that you need some more performance tuning. It maybe that you are comfortable with the performance of each statement individually. But after seeing a component that needs to run four or five of these statements together, the overall performance is less than what you need. What we are trying to do is to identify any performance issues that we might have early in our application development life cycle. So that if we need to make any changes, we allow for sufficient time in our schedule to make these changes. And we avoid having to rework major components of our application at the last minute. So as you work through designing and developing your application, recognize places where you have complex SQL statements. Statements that run against tables with large numbers of rows, search type functions or areas that have had past performance problems. And then prioritize these statements and components to be analyzed and tested early in your project life cycle, so you can identify and address any issues that may come up. In an ideal setting, once you've finished with coating your application and it moves into a test phase, you would have an opportunity to conduct Formal Performance Testing. Typically, what this involves is some software that will distribute agents across different client machines. And each of these agents simulate a certain number of users or transactions running against your application. Many organizations have specialized teams that are responsible just for helping you conduct this type of performance testing. Performing a formal performance test can provide you with very detailed insight into how your application will behave, before it's deployed to production. By working with your DBA and using the information in the module on monitoring Oracle applications, you can get a good feel for what areas of your application are performing well. And what area still needs some tuning to occur. And this, of course, is very valuable when we can identify and fix these issues in a a full size performance test environment rather than finding them after we deploy to production. Even if you are not conducting Informal Performance Testing, it can still be very valuable to conduct at least some if not all of your functional testing for your application against a full size database that reflects your production data. As you're QA staff or business users test the application, they're going to get some feedback about how different functions perform. While it is true that the app isn't being tested at full load, it is still better to have some feedback about the performance rather than none. And again, you can identify some areas of high risk in terms of performance and make sure that your testers hit these areas. First, you'll get some direct feedback from them if a function that they were testing didn't meet their expectations in terms of performance. For example, they may tell you directly that a certain function took 20 seconds to complete and that was too slow. But second, you can use some of the techniques presented in the Monitoring Oracle Applications module. So look into Oracle itself and see if any statements your application is running are using up an excessive amount of resources. These statements may perform okay in test, because you don't have very many users and the database isn't under much of a load. But when you move to production, these statements will be a problem. While this is not as thorough or as detailed as conducting a full performance test, you will still get some insights into how different functions in your application perform. And it will hopefully help you identify any of the more significant performance issues that you might have. So far, everything we have talked about in terms of performance tuning and testing has been with regards to Proactive Performance Tuning. Unfortunately though, no matter how thorough our testing, there are times when performance issues occur any running production application. You're going to need to troubleshoot these problems and fix them. This is what we would call reactive performance tuning. Obviously, we would like to avoid these situations as much as possible, but it does happen. Sometimes these are urgent issues where you have a function in your application that is essentially unusable because it runs so slow. And sometimes, it is more of a chronic situation. Where a function has been getting slower and slower over time and users are starting to complain. In either case, we're going to have to engage in some performance tuning. So these types of situations provide one more use for your full size performance test database. Once you identify the suspect SQL statement or function, you can tune test any fixes you have in your test database rather than in production. In order to fix the issue, you may need to try out several different ideas. And it is always preferable to do this in a Test Environment rather than a Production Environment. This is especially important, because if you have to do something like build a new index or maybe drop and recreate a new index with new columns on a very large table. This can take a significant amount of time and system resources. So we'd like to only do this once in production, as to minimize the impact on end users of our application. In summary, we want to make sure that we have a full size database available to us during the development and testing of our application. This gives us a place to analyze and tune SQL statements and test out the performance of components in our application as they are finished. And most importantly, we can be confident in the results that we get we get. Because we have a test database that accurately reflects our production data. We also want to make sure to be incorporating performance testing early in our project life cycle. This is all about risk management. You want to identify areas of the application that could be at risk for performance issues and get a measure of where we stand early, so we can address those risks early in our project if we need to. So don't leave performance tuning and testing until the very end of your project. Try to analyze and test both statements and application components, as you work on them. When our project does move into a formal test phase, ideally we would like to conduct a formal performance test and simulate the load or application it will under in order to identify any bottlenecks that we may have and get those addressed. If however, you're not conducting a formal performance test in this way, then you should look to make sure that any functional testing you perform is against a full size database. This will at least give you some level of assessment of the performance of your application. Finally, keep in mind that performance tuning and testing your application is really about usability. We all know from our own experiences that applications that respond sluggishly or take a long time to perform critical business functions, don't provide a very good user experience. So the real reason we go through all this trouble of performance tuning and testing our applications is to make sure that our applications aren't just functional for our users. But they're more usable, as well

Connections and Connection Pools

Introduction

Hello, my name is David Berry. Welcome to this module on connections and connection pooling. I want to take a few minutes to discuss how an application connects to an Oracle database. Connecting to Oracle is one of the most fundamental tasks and is one of those tasks that we rarely give a second thought to. However, there are some practices that you should follow in order to make sure that your application doesn't encounter unnecessary waits as a result of not managing connections correctly. An important aspect of achieving the highest performance possible is to use connection pooling. When your application uses a connection pool, you keeping number of physical connections to the database open at all times. And then when your application code needs a connection to the database, it will borrow one of these connections it has already opened from the connection pool. After this block of code has finished executing the data access calls it needs to, it will return the connection to the pool so that it can be used by a different block of data access code. The advantage here is that you're able to reuse physical connections to the database server. And this is much faster than establishing a physical connection to the Oracle database server every time you need to execute some data access code. Establishing a TCP connection across the network, allocating the PGA memory on the Oracle server, and other associated overhead all takes time. So being able to check out a connection that already exists is much faster. What happens if a piece of data access code in your application tries to check out a connection from the connection pool, but all of the connections are currently in use? The connection pool will recognize this situation and it will go ahead and open up a new physical connection to the database, add this connection to the pool and return it to the requesting block of data access code. Conversely, if there are a number of connections in the pool that haven't been used in a period of time, the connection pool will go ahead and close these connections in order to conserve resources on the database server. In this way, the size of the connection pool will vary up and down depending on the amount of data access activity occurring in your application. It is important to know, that there will be a connection pool established for each unique connection string that you're using in your application. So if your application connects to two or more Oracle databases, or perhaps connects to two different schemas on the same Oracle instance, in both these cases, each connection string will have its own connection pool. Each connection pool will have a minimum and maximum size. And these are just like they sound, the minimum and maximum of physical connections the pool will maintain to the Oracle database. If your application has a period of time where the usage of the app is very low and not very many connections are needed, then the connection pool will contain code that will close unused connections and you would see the number of connections in the pool go down over time. If your application is idle for a long period of time, say, in the middle of the night when everyone has gone home, then you very likely would see the connection pool would only contain the number of connections specified in the minimum pool size. As we just discussed, if your application requests a connection from the connection pool, but all existing connections are currently in use, the connection pool will automatically establish a new physical connection to the Oracle database and return this connection to you. And so in this way, the number of connections in the pool is dynamic. During busy periods, the number of connections in the pool will increase to help max the load. During slow periods, idle connections are automatically closed. What is even better is that all of this is managed for you and there's nothing you have to do in your application in order to take advantage of this. There is one other scenario though, that you should be aware of and that is when the connection pool is already at its maximum size and all of the connections in the pool are in use. What happens then, when another piece of data access code requests a connection from the pool? The answer is that the thread running this piece of data access code will have to wait until a connection becomes available. So in the example we see on the screen, once one of the other threads finishes with its connection and returns it to the pool, this connection will now be assigned to our waiting thread. If no connection becomes available within a specified period of time, called the connection pool timeout, then the connection pool will throw an exception on the thread waiting for the connection and you will need to handle this exception in your application code. There are some important implications here for how you use connections and how long you hold on to connections in your application code once you've checked a connection out from the pool. But the general rule is that you only want to keep a connection checked out from the pool as long as you need to and no longer. Otherwise, you could cause other threads in your application to block while they're waiting for connections. Let's take a look at a couple of examples of connection pooling in action to understand why this offers a performance benefit.

Demo: Connecting With and Without Connection Pools

I have a very simple demonstration program in Visual Studio and all this program is going to do is open up a connection to Oracle. And then it's going to run a query to get the session identifier of this connection. And it's going to do this a total of five times in a row. For this program, if you don't include an argument on the command line, the program will use a connection stream that has connection pooling turned on, which is actually the default in .NET. If you include an argument of NOPOOL, then we'll use a connection string that has connection pooling turned off. If I scroll down to the method that's actually performing the data access, you can see that I have some code that is just going to print out how long it took for our data access code to get a connection object, and this value will be in milliseconds. Looking at these numbers, we'll be able to evaluate the difference between using a connection pool and not using a connection pool. So I'm going to go out to a PowerShell prompt and I'm going to run this. And the first version I'm going to run is without connection pooling. So here we are. We see that we got the first connection. It took a little bit over a second and a half, and then each subsequent connections were slightly quicker than that, but were still running about a second each. So let's go ahead and run this with connection pooling enabled, which is the defaults, so we can compare our results. And here we are. Again, the first connection takes about a second and a half, a little bit more than that, but notice the subsequent connections take almost no time at all. What is happening here is that for the first connection, we had to pay the overhead of setting up the connection to Oracle. And in this case, that took a little bit over a second and a half. But then, once we have a connection and we are able to just check that connection out from the connection pool and reuse it over and over again, getting the connection is almost instantaneous. All we're doing is getting a reference to that preexisting connection in our client application and we don't have to actually pay the cost to establish that new physical connection out to the Oracle database server. In the first run of this program, what was happening is that every time that we got a connection, we had to go out and establish a physical connection to the Oracle database server. And this was taking about a second each time. So clearly, using connection pooling can give us a big boost in terms of performance because our application code isn't having to sit around and wait while we connect to the database. So in pretty much all situations, we want to make use of connection pooling.

Using a Connection Pool in .NET

So how do we use connection pooling in our applications? If you were a .NET developer, you're most likely already using connection pooling since it is built into the Oracle driver and completely transparent to you as an application developer. I have a segment of some data access code and all this code is doing is opening a connection to the database, performing some data access work and then closing the connection. When the program executes this line right here, even though the name of this method is Open, what is really happening is that you're borrowing a connection from the connection pool. It may be that the connection pool does indeed have to open a new physical connection to the database in order to execute this statement. But most times, what this line is doing is just going to grab an available connection from the connection pool and hand it back to your code. Then, when you call Close on your connection, you're not actually closing the physical connection on the database. What you're doing is returning this connection to the pool so it can be checked out by another thread in your application. If we were using, using statements, rather than a try-catch-finally block, then when .NET executed the using statement, this is when the connection will be returned to the pool. So if you're a .NET developer, that's all there is to it. You probably already have code that looks like this in your application, and so, you're already using connection pooling without even thinking about it. If you want to control some of the parameters of the connection pool, you can do so through the connection string that you use. The parameters that you see listed here are taken directly from the Oracle.net developer's guide. These are just the ones that relate to connection pooling. Generally, the defaults just work fine, so if you don't specify any of these parameters, Oracle will just use these default values. One parameter that you may be interested in is that, by default, the maximum pool size is 100. Again, this maximum applies to each connection pool. So if your application connects to two different Oracle databases, each pool can have a total of 100 physical connections in it. If you do have an application, say, a very busy web application that needs more concurrent connections, then this is the parameter that you would want to modify. We also see that there is a parameter for the minimum pool size and you might be tempted to set this to a larger value. In some cases, this can be a appropriate, but I would discourage you from just arbitrarily setting this to a high value. If there's a period of time where there's little to no activity in your application, the Oracle driver will still keep this number of connections open to the database. And if you have a lot of unused connections, this is going to be wasteful in terms of memory up on your Oracle database server. This is especially true if this minimum pool size is repeated across multiple machines, like servers in a web cluster. In practice, as the need for connections increases, the Oracle driver opens more connections. And then, as the need for connections decreases, it will check every five minutes for connections that have been idle to see if it can decrement the number of connections in the pool. If so, it will use the parameter, Decrement Pool Size, to determine how many connections it should close. And in practice, this works very well. The defaults do a good job of matching the number of connections in the pool to the needs of the application, so you rarely have to modify these values.

Using a Connection Pool in Java

In Java, things work a little bit differently. What you need to do is to download the Oracle Universal Connection Pool library and use this library that provides the connection pooling functionality to use. There're some other open source examples out there, but in this module, I'm just going to cover the use of the Universal Connection Pool library. It is a separate download from Oracle and you can use the link provided on this slide or simply go to Oracle's website and look in the Download section and then look under Developer Tools. I've also provided a link to the Java doc for this library, so if there are other aspects of the library you are interested in, you can review the documentation provided by Oracle. What you're going to do is have some sort of method that's responsible for initializing the connection pool, probably called in an initialization routine of your application. In this case, I'm handing the name of the connection pool, the JDBC URL, the database, user name and password that will be used by the connection pool into this helper method. The pool name is not actually used in making the connection. It's just nice to have so that when you have multiple pools, you have some sort of friendly identifier that can be used to tell what pool is what. We use a PoolDataSourceFactory object to create a new PoolDataSource object. And then on that PoolDataSource object, we set all of the appropriate parameters so that the pool data source will know how to connect to Oracle. And this includes the name of a class that will act as a factory class for creating connections. Down below here, we then have an opportunity to set some of the parameters for our pool. So you can see, I've decided to set an initial pool size of 5, a minimum size of 1 and a max of 100. And then this method is returning a PoolDataSource object. Now, what is important is that in your application, you need to keep a reference to this PoolDataSource object so your data access code can get a reference to this object because we'll use this PoolDataSource in order to get connections to the Oracle server, as we'll see in the next slide. Here we see a segment of some data access code that needs to get a connection to the database and there are two important lines in this segment. The first of these is the second line in the code fragment and the idea here is that in some way, we're getting a reference to the PoolDataSource object that we created in the previous slide. In this slide, I'm showing this as calling a method that would probably be doing some sort of look up by the name of the pool, but there are other ways you could do this, just in some way, you have to get the reference to that PoolDataSource object. And then, on the PoolDataSource object, we're going to call GetConnection. And this is going to get a connection from the connection pool for us. Just like in the .NET code, this may require opening a new physical connection to the database. But if so, this is all done behind the scenes. In most cases, the pool data source is going to return us a connection that's already in the pool, just not being used. And then we're going to perform our data access as normal. When finished, we call Close on the connection, and just like before, this doesn't actually close the physical connection, but it returns the connection to the pool. So we can see, it's very easy to use a connection pool in Java just like it is in .NET. And there really is no reason not to use one when making connections from your application code.

Connection Pool Best Practices

When using connections from connection pools, which most likely you are, there're a couple of best practices to keep in mind. The first is, during the time when you have a connection, keep your code focused on data access and only hold onto the connection as long as you need to. What you don't want to do is to get a connection and then perform some other time-consuming operations, like a bunch of business logic, calling a web service, or writing a file or anything like that. You want to stay focused on the data access that needs to be performed. You may very well need to perform some complex logic on the data that you're reading or perhaps call a web service to perform an operation on the data you just queried. But all of this can come after you've performed your data access and return the connection to the pool. You want to treat database connections as a somewhat scarce resource and a resource that is expensive to create. As we saw in the prior demonstration, it took about a second to open up a new physical connection to the database and that is actually a pretty typical value. If you have pieces of data access code that hold onto database connections longer then they need to, it stands to reason that more of the connections in the pool will be checked out at any one time. And as we have discussed, if all the connections are in use and another request comes in for a connection, then the pool will have to respond by opening another physical connection to the database. From an applications point of view, whatever business operation is calling the data access code is now incurring the cost in terms of time for needing to open this new connection. In addition, because you now have more open physical connections, you're also going to be using more PGA memory on your Oracle server because each connection to Oracle uses up some memory on the database server. Finally, if your application is busy enough that you're at the maximum pool size, you could see segments of your data access code block while they wait for a connection to become available. This means your user is experiencing a delay in performing whatever operation they were trying to perform because a connection isn't available. But the root cause is that you have some other piece of code that is holding onto a connection longer than it needs to, that it doesn't need any more. Maybe this other thread is waiting on a slow web service to return or doing some other operation. So what you have effectively done is to introduce a bottleneck into your application and the effects of this bottleneck will be felt in multiple functions across the application. So be a good steward of your connections and make sure that while you have a connection, the code is short and focused on data access. And this will make sure that your application is making the most efficient use of the connection pool as possible. There are occasions that we run a query in our application code and in response to the results in that query, we may need to execute additional SQL statements against Oracle. One example of such a situation is shown here in the slide. I have a method where I want to get a list of courses for a term and in addition, I need to look up some information about the department that is offering the course. This code itself is not a very good example of how you would write this code. For example, most departments will offer many courses, but in this code, we'd be querying that same department over and over again. But for our purposes, this code is just here to illustrate a point. We're in one data access routine. We have a connection open to the database and we need to execute additional statements. And so in this case, we might call a method like this. And this would look up the department info, map it to a Department object and return the data to our first method. The problem with this code the way that it is shown is that it's actually going to use two connections to Oracle in order to do this. The connection in the top method and in the bottom method are separate and distinct connections. So from a connection pooling perspective, now we have one logical operation in our application that is using two different connections from the connection pool in order to complete. So if we had a lot of code like this, our connection pool would seem to be much busier than it really should be because each time this is happening, we're using two connections rather than just one. And by now, you can guess the problems that are associated with this. We're probably going to have to open additional physical connections to Oracle. And as we know, this will take longer for whichever threads end up having to open those connections. And we're much more likely that we're going to be close to our maximum pool size. These will effectively double the number of connections it takes to get this segment of work done. So if we have a really busy application, we may encounter some situations where the pool is at maximum pool size and some of our threads end up blocking, waiting for a connection to become available in the pool. The Oracle drivers, both for Java and .NET contain support for something called multiple active result sets, or MARS. This is available by default. There's nothing that you have to do to turn this on. And what this allows us to do is to pass the connection object down to the child method, as shown here. And you can call multiple SQL statements on the same connection. This is the encouraged way to do things. Now this code is only using one connection, so we're more efficient from a connection and connection pool perspective. Second, if this code was in a transaction, both queries are running on the same connection now, so they will run in the same transaction and the results you get back will be logically consistent. Whereas before, they wouldn't have necessarily been so. Now, you might look at this code and think that you might still need a QueryDepartment method that only takes department ID as a parameter so you can call this from your business logic layer. And if that's the case, you can easily accomplish that with a little bit of method overloading. Just know that any time you have a data access method call another data access method, what you should do is have the first data access method get the connection and hand the same connection down to the child methods to use. This will make sure that the entire logical operation is only using one connection to Oracle, which is the most efficient in terms of performance and not opening unnecessary connections, and is also the right answer from a transactional processing point of view.

Demo: Blocking Connections

I have a second demonstration set up and what we are going to look at is what happens when, first, all of the existing connections in a connection pool are already in use. And second, when we are at the maximum pool size, what happens when more requests for connections come in. So let's take a look at what this program is doing. First of all, you need to know that I have the minimum pool size configured to two connections and the maximum pool size set at four connections. To start with, I have a method and this just connects to the database and gets the connection pool initialized. Calling this method up front is really here just to make sure the test is repeatable and we don't have any effects from setting up the pool in later connections that we're going to make. I put some delays in the program to make sure the output's a little bit easier to read, so we're going to wait a few seconds and then we're going to use parallel task library to run this data access method I have called RunTimeConsumingOperation. And if we scroll down, we can take a look at that. We see this method is just querying the session ID again, but I've also added a thread wait in here to tell the thread to go to sleep for ten seconds while it has the connection open. If you aren't familiar with parallel task library or the Thread.Sleep command, don't worry. Just know that what this is doing is that I'm running two different data access routines in parallel, so on separate threads, and the Sleep command is simulating these operations taking ten seconds. So each time this method is called, it's going to get a connection from the pool and then it's going to hold on to that connection for a little bit over ten seconds. Let's go back up to the main body of the program now. We are then going to wait again to make things a little bit easier to read, and then we're going to run this time consuming data access operation again on two additional threads. And since the first two operations from above have not finished yet, these two operations should result in the need to open up two new physical connections to the database. Then, we're going to wait again and this time, we'll wait a little bit longer. And the reason why is I want to make sure that operations three and four that we just started get their connections and those operations get started. And one of the operations that I'm about to start, operations five or six, doesn't slip in and grab one of the connections first. So at this point, we have four operations that are all running and they'll all have connections checked out from the pool, so we'll be at our max pool size. Now I'm going to start two additional operations, operations number five and six. And what we should see is that these two operations end up blocking as they're waiting for connections. So let's go ahead and run this program and see what we get. All right, we're in PowerShell, so let's run this program. Those are the first two threads that we're able to get connections out of the pool. These are the second two threads which had opened new physical connections, and now threads five and six are blocking. And now the first two threads have finished, so threads five and six were able to get connection. We see a little bit of delay there and the program's finished. So let's go ahead and discuss these results. This program really acted just like we expected it to. The first two threads were able to get connections out of the connection pool, so they were able to get these connections almost instantly, only about ten milliseconds, and then proceed on with their work. The second two threads requested connections from the pool, but no existing connections were available, so we see some delay there. And the cause of this delay is the need to open new physical connections to the Oracle database server. In this case, this was taking about two seconds. The real interesting part is when we get down to threads five and six. At this point, we see that these threads block because there're no connections available and the pool is already at its maximum size. So each of these threads spent four seconds doing nothing but waiting for a connection to become available. There're a couple of lessons learned here. One, you want to stay away from being anywhere close to the maximum pool size. Otherwise, you could end up having that blocking occur. But two, make sure when you have a connection in your code, you're only holding the connection as long as you need to. You want to get your work done and get the connection returned back to the pool so another thread can make use of that connection. It doesn't have to resort to opening more connections or waiting for a connection to come available.

Summary

Let's wrap up our discussion of connections and connection pooling with some final thoughts. First off, connection pooling is your friend. By keeping a few physical connections to the database open and then sharing these out as logical connections on the client side, we save the overhead and expense of opening a new physical connection each time. From an application perspective, this eliminates the delay that tends to run about a second in opening up physical connections to the database, and this is a big deal. If we think about your typical Web page in a Web application, we generally want load times to be in the two to three second range, and hopefully no more than five seconds. If we need to expend a second to acquire a database connection, we've already used a significant part of our time budget. So, being able to avoid this is a substantial savings, and connection pooling is easy to use. You don't need to implement your own connection pooling strategy. There're already existing connection pools in both .NET and Java that will meet all of your needs. Second, when you're using connection pools, make sure to only keep the connection in your code just as long as you need to and then return the connection back to the pool as soon as you are finished with it. Just make sure that there will always be connections in the pool for other threads to use. Third, when you are performing one logical operation in your code, you just need to use one connection, the top level of the data access block for this logical unit of work, get a connection from the database, and then pass the reference to this connection to all of your other data access blocks that need it. This avoids checking out multiple connections from the pool to perform one work item. And again, this will make sure that the pool can maintain a supply of free connections to lend out to other threads. Finally, keep the minimum size of your connection pool small. Yes, there is a cost to open a new physical connection to the database, but the connection pool will quickly right-size itself for the workload that your application has. If you set an artificially high minimum, then you'll have a lot of idle connections during slow times and this consumes resources on your Oracle database server, especially memory. So these resources are now not available to be used by other activities that might need them, say, for example, some batch processing that you have going on in the middle of the night. Overall, connection pooling is easy to use and is something that you're probably already using. Just follow these few simple rules and that will make sure that your application avoids any unnecessary waits as you connect to Oracle.

Bind Variables

Introduction

Hello. My name is David Berry. Welcome to this module on using bind variables in Oracle applications. One of the most important practices in writing applications that use an Oracle database is to always use bind variables in your SQL statements. While easy to do, far too often this practice is not understood, or overlooked. The advantage of using bind variables is that if a query is executed multiple times, Oracle does not need to expend resources to create an execution plan each time but instead can reuse the original execution plan. This saves a lot of resources, especially CPU resources on the database server because most applications execute a relatively small number of statements over and over again. Just as important, when Oracle is able to reuse an execution plan you avoid creating contention around some segments of shared memory in the Oracle database. If this contention does occur, it creates a bottleneck around how fast SQL statements can be processed throughout the entire database. Which not just increases the amount of time it takes to process each statement, but also can greatly diminish the overall throughput of your Oracle database. Finally, using bind variables also provides protection against SQL injection attacks. And while this is a security benefit rather than a performance benefit, it is still an important consideration. Especially because SQL injection attacks always seem to be near the top of the list of application security threats. In this module, we are going to explore not just how to use bind variables in your application, but also to help you gain an understanding of why their impact on performance is so great.

The Shared SQL Area

If we return to our diagram of the life of a SQL statement, we are reminded that for every SQL statement Oracle executes, Oracle must have an execution plan which tells Oracle all the individual steps that must be performed in order to process the query. This execution plan is created by the Oracle Optimizer and is shown in step two of this diagram. It is primarily this step that we will focus on throughout this module. We'll start this module by understanding this process in depth. When the Oracle Optimizer creates an execution plan for a SQL statement, it must examine all of the different ways that it can process that statement in order to come up with the most efficient one. The first part of this process is for Oracle to gather all of the information about the tables involved and any indexes that could potentially be used from the Oracle Data Dictionary. To read this data, internally the Oracle Optimizer will execute over 50 queries against data dictionary objects and possibly more depending upon the complexity of the SQL statement it is attempting to analyze. Then the Optimizer takes all this data and evaluates all of the ways in which the SQL statement could be performed. This part of the process includes evaluating database statistics in order to make decisions about if an index should be used, how to perform any join operations, to what order the table should be joined in. The final phase is to take the most efficient plan and store it in an area of shared memory known as the shared SQL area. In real time this process happens very quickly. However, parsing SQL and determining the best execution plan does tend to be CPU intensive. And you also have to remember, Oracle isn't just processing a single SQL statement at a time. It is processing hundreds or perhaps thousands of statements every second. Taken together, parsing all of these SQL statements can take up a significant amount of system resources, especially CPU. The shared SQL area is a segment of shared memory in the database where Oracle caches execution plans. You can think of the shared SQL area as being similar to a large hash table. For each SQL statement, Oracle will calculate a unique key by using a hashing function and then store the parsed SQL and the execution plan in the shared SQL area by that key. The amount of memory allocated to the shared SQL area is finite. So Oracle manages the shared SQL area using a least recently used algorithm. This means that execution plans that haven't been executed recently will be removed from the cache in order to make room for new incoming plans. Therefore, at any given time, the contents of the shared SQL area will vary. Frequently execute statements will most likely always be present. In SQL statements that are only executed a few times a day tend to have a shorter life span and be removed from the cache. When a SQL statement is submitted to Oracle and the Oracle Optimizer needs to determine an execution plan, we will first examine the shared SQL area to see if an execution plan already exists for this SQL statement. To do this, Oracle will use the same hashing function, to calculate the hash key on the text of the submitted SQL statement. And then, much like you would in a hash table, performing a lookup by this hash key in the shared SQL area. If a matching execution plan is found, Oracle can simply read this execution plan out of the shared SQL area and continue on processing the query. If a matching execution plan is not found, then the Oracle Optimizer will go ahead and generate an execution plan for the SQL statement. The actual details of the comparison are a little bit more in-depth than this because Oracle has to account for things like two tables with the same name but located in different user schemas. But this covers the essential of how the shared SQL area works. Why is Oracle cache execution planned in this way? The short answer is that generally applications contain a relatively small number of unique SQL statements and they just execute these same SQL statements over and over again with different parameters each time. By caching SQL statements and their execution plans, we see two benefits. First, we save computing resources on the database server, because we don't have to regenerate an execution plan each time. And this frees up these resources to perform other work in our database, like actually processing our SQL statements. Second, we'll actually see each SQL statement run slightly faster after its first execution. The reason for this is because looking up an execution plan in the shared SQL area is faster than regenerating a plan from scratch. Oracle's not the only database to make use of caching SQL statements and their execution plans in this way. While some of the implementation details vary, you will find the same strategy used in all the major relational database systems in use today. When Oracle is able to match an incoming SQL statement to an execution plan that is already in the shared SQL area, this is called a soft parse in Oracle terminology. If Oracle has to generate a new execution plan for a SQL statement, this is called a hard parse. You will see the terms soft parse and hard parse used consistently throughout the Oracle documentation, books written about Oracle, and blog posts. So it is important to be familiar with these terms. There will always be some level of hard parsing that needs to go on for an Oracle database. After all, there will always be a first time when a statement is submitted to Oracle, and on that first time Oracle will have to hard parse the statement. The idea though, is to minimize the amount of hard parses you do and maximize the amount of soft parsing that goes on. In module seven we'll exam how you can use V$ sign views to tell how much hard parsing your system is doing and what the ratio is between hard parsing and soft parsing. Being able to monitor the ratio on a system-wide level gives you a good idea of the health of your Oracle database and lets you understand if there are any bottlenecks forming due to a high hard parse count. But for now, understand, it is hard parsing that we want to avoid and make sure that Oracle can soft parse as many of our statements as possible. There are a few loose ends about the shared SQL area that we need to spend a moment to wrap up. First, you might be wondering about the size of the shared SQL area and how many statements can be cached? The answer is that the size will vary from system to system, based on parameters, like how much memory is available in Oracle, some configuration parameters set by the DBA. But generally speaking, the shared SQL area is sized sufficiently that it can contain several thousand SQL statements and their execution plans. Second, it is important to know that the shared SQL area is shared across all users and all applications that are using a single Oracle instance. So if you have multiple applications that are using a single Oracle database as a back-end, all those applications are using the same shared SQL area. This has implications we'll discuss in the next section. Because if one application is doing a lot of hard parsing, it can by itself create a lot of contention around the shared SQL area and this will, in turn, affect other applications that are using that same Oracle database. Third, if you regenerate statistics on a database object, whether a table or an index, Oracle is smart enough to invalidate any execution plans that use that object, and remove those execution plans from the shared SQL area. The reason why, is that now that new statistics are available, the Oracle Optimizer may determine a different execution plan is more efficient. So we don't want to keep that old execution plan around. This is something that Oracle manages for you internally and no action is required on your part. Finally, there's a way for a DBA to manually flush the shared SQL area while Oracle is running and remove all of the cached SQL statements and their execution plans. Something that you probably never want to do on your production server. But it can be useful in test environments if you're running some sort of performance test and you want to make sure that you're starting from the same point each time.

Contention and Latch Waits

There's a second impact to having a large amount of hard parsing going on in your Oracle database. This impact is more subtle, but just as serious to consider. Oracle is a multi-user system, and at any given time there are many different SQL statements being processed by Oracle. If multiple processes are all hard parsing SQL statements, then each of these processes needs access to update the shared SQL area. And herein lies the problem. So if you have multiple processes, they're all accessing and modifying a shared data structure at the same time. That data structure will become corrupt, and the shared SQL area is no exception. Therefore, in order to protect data integrity, Oracle needs to synchronize access to the shared SQL area amongst all these different processes. If you've ever done any multi-threaded programming, you're already familiar with this concept. And object that needs to accessed and modified by multiple threads has to be protected, such that only one thread is accessed in the object at a time. In C sharp this is done using the lock keyword and in Java we use the synchronize keyword. As a result, the compiler creates a critical section of code and a serialization construct is created to make sure that only one thread can be running in that section of code at a time. In this way, the object is prevented from becoming corrupted because only one thread can be in the critical section of code modifying the object at any time. In addition, while the thread is accessing a critical code section, no other thread can read data from the object. Just make sure that no other threads read the object while it is being modified and potentially receive incomplete or inconsistent data. All of these same concepts apply in Oracle. In Oracle, we use the term latch to describe the serialization construct used to synchronize access to a shared resource. The process that currently has access to the shared resource has the latch. A process that is waiting to gain access to the shared resource is said to be undergoing a latch wait. There are a number of resources that need to be shared across processes in Oracle and therefore there are many different latches. For now though, we'll concentrate on the latch used to synchronize access to the shared SQL area. Understanding multi-threaded operations, how locking works and why it is needed is a very difficult subject area to understand and understanding the shared SQL area is no exception to this rule. Therefore, we'll start out with an analogy to help us understand what's happening here. Let's imagine a scenario where we have two different processes that are going to be writing to the same file. As you can see here, we have those two different processes on the left-hand part of the slide and they're going to write the text of some famous speeches into our file over on the right-hand part of the slide. What we would expect is that we'd have the text of the Declaration of Independence in the file, then some sort of delimiter between the two speeches and then the text of the Gettysburg Address. Both processes are writing the file at the same time and they're not aware of what the other process is doing. This is probably not the result we're going to get. One possibility is that the first process would write out its data. But then this data would be overwritten by the text from the second process. In this case, the updates from the first process are simply lost, the data doesn't exist anymore. This is clearly not what we want. We're writing out some important financial data rather than the text of some famous speeches, so it clearly not be an acceptable solution. Another scenario that could occur is that the writes from the two different processes are interleaved together and so we end up with some text that looks like this. Again, this is clearly not what we want, because now our file is pretty much unreadable. It's going to be impossible for any process reading this file to figure out what the original data was really supposed to be. The issue in both these scenarios is that the activities of these two processes aren't coordinated in any way. There isn't any mechanism to coordinate access to our file. It won't take long until two, or many even more processes try to write to the file at the same time and then the file is sure to become corrupt. So how do we solve this problem? We need to introduce some sort of synchronization mechanism, to make sure that only one process can write to the file at a time. A very simple example of this could be creating a Lock file. Maybe something as simple as HistoricalSpeeches.txt.law. When a process comes along and wants to write to the file, it first checks to see if this Lock file exists. If not, it will create the Lock file, and then it can proceed to write to the file. If another process comes along and wants to write to the same file, it checks to see if the Lock file exists. Seeing that 1 does, now Process number 2 knows that another process is already writing to that file and Process number 2 knows that it needs to wait. Well this mechanism is relatively simplistic. It does illustrate how we can introduce some sort of locking mechanism, to make sure that only one process is writing to the file, and we can preserve the data integrity of our file. So far, we've talked about the situation where two processes are trying to write to the file at the same time. However, the same scenario applies if one process is trying to write to the file, and one process is trying to read the file. The situation here is that the process trying to read the file, will likely re-date it as incomplete or inconsistent, if it tries to read the file while another process is writing to it. So in this case, the process that is trying to read the file also has to honor the lock file. And by seeing the lock file, it knows that it needs to wait to read the file, until another process is finished writing the file. This analogy is very similar to what happens in Oracle when multiple processes are trying to hard parse SQL statements at the same time. Just like in our file example, we can't have all these processes writing to the shared SQL area at the same time. So this is where Oracle introduces a latch, to make sure that only one of these processes is modifying the shared SQL area, at any given time. All the other processes that need to access to shared SQL area, will experience a latch wait. In this way, Oracle is able to protect the shared SQL area from becoming corrupted, like our file did in the file example. The trade off is though, that waiting on a latch to become available, is in and of itself a very expensive operation. So let's take a look at this. When an Oracle process tries to acquire a latch but is unable to, the process that is waiting for the latch does not go to sleep and then check again after some period of time, or go into a queue where it simply waits. Instead, the waiting process undergoes what is termed Latch Spinning, where it stays on the CPU and continually checks over and over again if it can acquire the latch. The reason why Oracle does this is because going to sleep and giving up the CPU, and then getting back onto the CPU has a high cost. So if at all possible, the process wants to continue to loop and see if it can acquire the latch. So what is happening here is even though that the process is waiting, it is consuming CPU while its looping. And this is CPU that isn't really doing any useful work. At some point, usually after 2,000 iterations, the waiting process will give up trying to get the latch and put itself to sleep for a short period of time. When a process gives up the CPU like this and is replaced by another process, this activity is known as a context switch in operating system terminology. If you've studied operating systems, you know that context switching is something that you want to avoid, this is a lot of computational cost in switching from one process to another on the CPU. After a brief period of time, the process will wake up and will again be switched back into running status. At this point, it will try to acquire the latch again. And if the latch is unavailable, say a different process has acquired the latch while our waiting process was sleeping, then the Latch Spinning starts all over again. The waiting process will loop, consume CPU while trying to acquire the latch, and then potentially go to sleep again. And this cycle will continue until the process is able to acquire the latch. What should be clear from this discussion is that contention is something that we want to do everything we can to avoid. First, because of how processes will spin while undergoing latch waits, we're going to end up using a lot of CPU while these processes loop on the CPU. And this is CPU resources that are just wasted, not available to do other useful work in our database. The second impact of contention, is that we're really introducing a bottleneck into our system. Statements will only process as fast as they can gain access to the shared SQL area. And as we have discussed, if we have a lot of hard parsing going on, now the responsiveness of these statements is impacted, because they have to spend time waiting to get access to the shared SQL area. One of the side effects of this bottleneck, is that we understand that a system will only run as fast as its slowest component. So it doesn't matter how many CPUs we have, how much memory we add, or how fast our SAN is. It is this bottleneck that is going to limit the performance and the throughput of our system. What we have here is the computer science equivalent to sending all of our SQL statements down a one lane road. It doesn't matter that we have a 12 lane super highway on the other side. Our ultimate throughput is limited by this single lane road, that is gaining access to the shared SQL area, and all of our SQL statements need to travel down this road. So clearly, if we can design applications that avoid contention in the database, we want to learn how to do this. And we'll cover this in the next section.

Matching SQL Statements

In the last few segments, we've come to understand the affects that excessive hard parsing can have on the performance of your system. Let's now turn our attention to how to write SQL statements that will be soft parsed rather than hard parsed, and how we can incorporate these statements into our applications. For an incoming SQL statement to be matched to a SQL statement and execution plan already in the shared SQL area, the two statements have to match exactly. This includes column order, the order of predicates in your WHERE clause, and even extends to white-space characters in the statement, and what characters are in upper and lowercase. You can think of this as if you called String.Equals on the two strings of the SQL statements, one string being the incoming SQL, and the other string being the statement in the shared SQL area. If String.Equals would return true, then the statements match. In effect, what we have here is a character by character comparison of the two strings. And only if each and every character matches are the two SQL statements considered to be equivalent. Let's take a look at a few examples of this. On this slide, we have two queries that are each retrieving grades for a student in a particular term. And in this case, the student and the term is the same between these two queries. These two queries are exactly the same, except for the fact that in the first query, the name of the view is in all lower case. While on the second query, the name of the view is in all upper case. For the purpose of determining if the two SQL statements match, character casing counts. And so Oracle would regard these two SQL statements as different statements, and would hard parse each one. In this slide, we have two SQL statements that are very similar to the one shown in the last slide. And once again, we're querying the view for grades of a particular student. Notice however, that the order of columns is different between the two queries. So even though these two queries are in fact returning the same columns and therefore the same data, the fact that the column lists are in different orders means that Oracle will regard these two queries as different, and hard parse each one. It is important to consider though, that when we write an application, that we define the SQL our application uses in some sort of string in the data access class. And then we'll use this string almost like a template that we plug values into and generate the SQL we're going to run against the database over and over again. So generally speaking, we don't see issues where a statement is run once in uppercase and the next time in lowercase and things like that. It's just important to be aware of, that Oracle does match these statements exactly. And you do want to pay attention to all of the little details like character casing, column order and spacing, especially if you have any code that dynamically generates SQL. What is more important for us to focus on, is that using literal values directly in SQL statements, causes the statements to differ, and therefore be hard parsed. Let's see what we mean by this. Consider the two SQL statements shown on the slide. Once again, each of these statements is querying the view to get student grades for a particular semester, in this case the Fall of 2013. The only difference is that they are querying the grades for two different students. Query on the top is getting the grades for student 12345, and the query on the bottom is getting the grades for student 67890. Functionally, these two queries are exactly the same. They are returning the same columns in the same order. They have the same predicates in the WHERE clause. The only difference is which student grades will be returned for. However, since these queries are directly specifying the student ID in the WHERE clause as a literal value, Oracle will see this two queries as separate and distinct queries, and will hard parse each one. The reason why is because we have specified the literal value of the value we want in the query directly. And again, if we called String.Equals on these two strings, we would get false. So in the eyes of Oracle, these two statements will be regarded as different. This applies not just to the student_id field, but to any literal value that can vary in any predicate in the WHERE clause. In this case, I've changed the query so that in both instances, we're retrieving grade data for student 12345. The first query is getting grades for the Fall of 2013 term, and the second query, for the Spring of 2013 term. Again, these two queries are functionally equivalent, we're just varying the term in which we are retrieving grades for. However, Oracle will again see these two SQL statements as different statements, and hard parse both of these. You can start to see the road we are headed down here. If we write our SQL statements with literal values like this, every time a different student logs on to our web application to check their grades, or a student checks their grades for a different semester. We're going to end up with what Oracle considers a unique SQL statement, and wind up hard parsing that statement. And as we've talked about, that is the situation we want to avoid. What we need to do, is to rewrite our query using Bind Variables. To do so, we're going to replace the literal values in our WHERE clause with this syntax that represents a Bind Variable. In Oracle, the syntax that is used is a colon character followed by the name of the Bind Variable. So you can see in this statement, we're specifying two Bind Variables, one for student_id, and one for the term_code. When Oracle parses this statement, it sees the bind variables in the text of the SQL statement, and not the actual values. So therefore, the text of the SQL statement is the same each time. Consequently, the statement will be soft parsed on each time it is run after its initial execution. What we want to do is to use a bind variable for any value in the WHERE clause that can vary on different executions of the statement. In this case, we would expect that both the student_id and the term_code would vary as the statement is run on behalf of different students looking at their grades in different terms. So both of these values need bind variables. When you submit a SQL statement that uses bind variables, you will also need to provide the actual values that Oracle will need to plug in when it executes the statement. Let's take a look at some application code to better understand how this is done.

Using Bind Variables in Applications

I'm in Visual Studio, and we're taking a look at some C# code. This code is part of the demonstration we'll run in a few minutes to help us understand the performance implications of using bind variables. This is a simple data access method that takes in two parameters, a student_id and a term_code. And it's going to return a list of course grade objects, which is just an object that encapsulates all the grade data inside of it. Looking at lines 23 through 25, we see the text of our SQL statement. And notice that the WHERE clause, we aren't specifying the student_id or the term directly, but we do see our bind variables. Then, as we move down and look at lines 28 through 31, this is where we set the values of our bind variables. On our OracleCommand Object, there's a parameters collection. And what we'll do is, we're going to new up two OracleParameter Objects. Those objects are going to contain the name of our bind variable, and of the value. And then we're going to add those objects to that parameters collection on the OracleCommand Object. At a minimum, you need to specify the name and the value in the OracleParameter Object. There are other items that are available though for you to set. And these include things like the data type, and the size of precision for numeric values. If you want to go ahead and set those you can access them either through some overloads on the constructor, or through some properties. So let's go ahead and set a data type right now. So student_ parameter, we actually want to use the Oracle DB type property. And this is an integer in our database, so there we go. There are a couple of other things that you should know about using bind variables in your application. First of all, you need to add an OracleParameter Object to the parameters collection on the command for every bind variable that is defined in your SQL statement. If you forget one, your program will throw an exception when it's run. And the message you'll get will be something along the lines of, insufficient values to bind into your statement. Second, you need to add the parameters to the parameter collection in the same order that they are listed in your SQL statement. That is, when Oracle matches the parameters up with the SQL statement, it actually does so positionally, it's not looking at the names. So if I were to go ahead and reverse the order here, and I'll do that right now, I'm going to put term_code first. This code here actually would throw an exception. Now there is a parameter that you can set on the Oracle command that says I don't want to bind by position, I want to bind by name. And the name of that parameter is bindbyname. And you just simply set this property to true, and then this code will be okay, Oracle will match these up by their name and disregard the position, and this code will run just fine. I realize this is C sharp# code, and not everyone is a C# developer, but generally speaking, things are going to work in a very similar fashion in all languages. In Java, what you'll want to do is you'll want to use the prepared statement class, and on the prepared statement class there are going to be methods to add parameters onto that object. In other languages consult your documentation for how to add parameters into your SQL statement. But generally it's going to look something like what we have here. I want to take a moment to contrast this code with some data access code that uses literal values. So I'm going to jump over to the other class called literal values DAO. This is the same method as we had before, where again we're passing in a student_id and a term_code. and you'll see that the major differences are in the SQL statement that it's defined on lines 25 through 27. Here, we're inserting the values past in directly into our SQL statement using string concatenation. And then it's this SQL with literal values that's included that's being submitted to Oracle each time. You can clearly see here that as we pass in different parameters to this method, then we would create a different SQL string and submit this SQL string to Oracle each time. There are a couple of problems here. The first issue is that as we've talked about in this module, each one of these SQL statements is distinct and ultimately going to result in a hard parse up in our Oracle database. The second problem, though, is that anytime we are forming up SQL using string concatenation and inserting values passed into us directly into our SQL, we're vulnerable to SQL injection attacks. When we use bind variables, the Oracle driver will automatically escape any special characters that are in the value that's passed in. So our SQL statement is protected against any values that were submitted with malicious intent. With literal values though, that string is just passed on through to the database and this poses a very serious security threat. Now using bind variables doesn't mean you shouldn't implement other good security practices like validating your input in your application. But they do offer one more level of protection. There are a couple of final items that we need to cover. First, pretty much all of the major object relational mappers in use today, already use bind variables implicitly. So, if you were using entity framework or in hibernate. Your application is already using bind variables and you should be in good shape. If you are using an ORM, you might be tempted to think that bind variables is a topic you don't need to worry about. However, while new development might be taking place using an ORM, a lot of times we have to support existing applications in our jobs. And you want to make sure that these applications that are just using plain old ADO.net or JDBC are using bind variables as well. And if they're not, you want to get those applications uplifted. One reason why that we just covered is the security concerns around SQL injection attacks. The other reason is that if you have one or two of these applications out there in your organization these applications by themselves can use up a lot of resources on your Oracle server. And cause contention around the shared SQL area. So, in that way, these existing applications that don't use good practices can impact the performance of all of your other applications that are using that same Oracle instance.

Sample Application to Compare Approaches

We're in Visual Studio and I have a simple console program that I've written to demonstrate the performance difference between using bind variables and using literal values. This program does is uses Parallel Task Library and Dot net. So we get multiple threads and each one of these threads is going to execute a query and retrieve some student grades a thousand times. What we are trying to do in this program is to simulate the situation where we have multiple concurrent users connected to Oracle and they're all running queries at the same time, just like what you would have in a real system. In this demonstration, we're just going to use four concurrent threads. But even at this relatively low number, we're going to be able to see the impact of hard partitioning and producing contention around the shared sequel area. We aren't going to go through this program line by line, but I do want to highlight a few key points before we actually run the demonstration. The first part of this program looks for a single argument on the command line and based on that argument, we're going to use either the literal values DAO or the bind variables DAO and these are the two DAO objects that were shown in the last section. So, ultimately what I'm going to do here is we're going to run this program twice. Once for each DAO. Moving down a bit, on lines 44 and 45, there is the important part of our test. We're going to do a couple of things here. First we are going to clear out the shared pool of any previously existing execution plans, by running an alter system flush shared pool command. The whole purpose of our test is to see how many statements are hard parsing. So, we don't want our results to be impacted by whatever data might already be in the shared pool. So, we'll clear this data out and make sure we're starting from the same point each time. The other thing that we are going to do is to snapshot the values of some key performance metrics on our Oracle database. We'll capture these values again when we finish running our test queries and then by taking the difference of the values we captured, we can see how many resources we were using in some of these key areas. We could do this with stats pack just as well. Though in this case, I'm just choosing just to capture some key metrics that we want to look at for our test. Statspack captures a wide range of items and while this is good for when you aren't really sure what's causing the problem, the reports Statspack produces are rather lengthy. For this demonstration it's just a little bit easier to read if we focus in on a few key metrics. So, let's take a look at what we're capturing and to do so, we're going to go over to this other DAO object test setup DAO. The first item that we are capturing is the amount of CPU that is used by Oracle. And we do this by querying this article out of the v$ sys-time model view. Next, we want to capture some data around the hard parsing that's going on in our system. And we can do that by querying the v$ sys-stat view. The three statistics that you see here will tell us the total number of statements hard parsed, the amount of elapsed time to hard parse the statements, and the amount of CPU time used in hard parsing these statements. The final set of metrics you want to collect is around the shared pool latch. And we can do this by clearing the view v$ sign latch. What we're trying to do here is see if there is any contention that is showing up around this latch. We'll collect four parameters. The first being the number of latch gets. Just the number of times that a process requested a latch. The second parameter is latch misses. This is the number of times that a process requested a latch but failed to acquire the latch. So, when we see latch misses, we know that we're going to have some process which enter latch spinning. And of course, this is something we want to avoid. The third parameter is latch sleeps, which is the number of times the process was spinning, waiting for the latch, but reach its limit in the number of times it would loop and then put itself to sleep. Finally, we have the number a spin gets and this is the number of times a process that was undergoing latch spinning was able to acquire the latch while spinning. Let's head back over to the main program and look at a couple more things. Lines 48 through 54 iterate through our list of students and for each list, they will kick off a separate task which will start querying grey data from the database. It is in line 52 where the task actually was started. And when we call StartNew in Parallel Task Library, we will kick off a new thread to execute that task and return immediately. So, in this way, we're able to get multiple threads all running at the same time, and this helps us simulate some concurrent activity, just like what we would have in a real system. We'll wait for all of the tasks to finish on line 55. And then, after this is done, as we scroll down here, you see that we're going to capture the values of our performance metrics again to get the after values and then at the end of the program, we're going to print all of these values out to the console. So, let's go ahead and run this program a couple of times and see some data that we're going to get so we can evaluate what's going on. I have a PowerShell prompt open to the directory where the exe is located. And now all I need to do from this prompt is to simply run the program by typing in the name of the program and then that command line argument. And so, I'm going to start this off by running the program for literal values first. This should take about 12 seconds and so we were just going to sit here and wait for it, I'm not going to stop the video. We'll just go ahead and wait for this to finish so there may be a few seconds of dead air here. Okay, here we go. So, the data is finished I'm going to reserve any discussion of the data, we see it took 14 seconds here in elapsed time to complete. The only thing I'm going to say right here is, you do see that the database CPU is 37 seconds, whereas it only took around 14 seconds to complete. And the reason why here is because Oracle is running on my laptop, which is a quad-core machine. And so, what we see for most the time that this program was running, it actually was using multiple cores in Oracle in order to get its work done. So let's go ahead and run the program for bind variables. And see what the data is there. And then we can discuss the data. So, this should not take quite as long. All right. And here we go. So we see that the bind variables program took around 6 seconds and it took less CPU. What I'm going to do is I'm going to take these values here and I'm going to copy them out and I'm going to put them back into a Powerpoint slide so it's a little bit easier to compare them side by side.

Evaluation of Impacts of Bind Variables

The first thing I want to say is that the data we're looking at here, we're really only looking at to draw some general conclusions from. Don't take these numbers to be absolute. What these numbers are intended to show though is that there's a significant performance difference between using bind variables in your application and not using them. And that's why it's important to invest time in learning how to use bind variables. So, the first thing we notice is the test using bind variables executed much faster overall. Taking only about 6 seconds when compared to 14 seconds for literal values. What seems to be driving this number are two items. The amount of CPU consumed and contention around the shared sequel area. We immediately notice a big discrepancy in the amount of CPU used for each test. For bind variables we're using just 9 seconds of CPU. And only 40 milliseconds of the CPU was used for parsing SQL. So, this tells us the CPU in the bind variables test was pretty much all going to executing our query. On the other hand, the literal values test, we're using quite a bit of CPU, 37 seconds worth. And the main driver of this is the parsing process, which is using 27 seconds worth of CPU. Oracle doesn't give us a breakdown to understand what amount of this 27 seconds of CPU was being used for parsing. And what amount was due the last spinning. But from our perspective it doesn't really matter. What we see here is that we're really wasting a lot of CPU resources up on the database server. And this is a CPU that can't be used for other more important tasks. Moving on to the data on the shared pool latch. You see that even with only four concurrent processes running, we're already seeing some contention around this latch. We have almost 15,000 times where are process attempted to acquire the latch and failed and therefore, the process had to start spinning. As a result of the spinning processes, we see almost a hundredfold increase in the number of latch get requests. And what this is is the spinning process is looping over and over again trying to acquire the patch. We also see 78 occurrences where a process was not able to acquire the latch while spinning and it had to put itself to sleep. And it would be correct to assume that this number would only go higher as we introduce more of the current processes. To wrap up, there are a few take aways we can draw from this data. First of all, this demonstration only had four concurrent processes and we can already see that we are causing contention. If you think of a typical production environment, you probably have a lot more than four concurrent users. So, this is why it's important to make sure to use bind variables. Because as we go to higher user accounts, this contention is only going to get worse. Second, we also see that the failure to use bind variables does indeed consume a lot of system resources and these are resources that are not available to other applications. So, whenever possible, you want to identify and correct these applications that don't use bind variables. Otherwise, we're going to adversely impact the performance of applications that are using the same Oracle database. Third, another way to look at the data is that when we're using literal values, we're only able to run about 280 queries per second. Using bind variables, we average running 667 queries per second. What this is really speaking to is the scalability of our system. Failure to use bind variables is going to put an artificially low ceiling on the number of users your system can support. And again, this is a problem that adding additional hardware won't solve. Since we are constantly challenge to build systems that scale to support larger workloads and more users, we want to make sure that all of our applications are using bind variables. In summary, there really isn't a reason not to bind variables. They're easy to implement and they're an important practice in making sure that your application is scalable and makes efficient use of resources on your Oracle server.

Statement Level Performance Tuning

What is an Execution Plan

Hello, my name is David Berry. Welcome to this module on Statement Level Performance Tuning in Oracle. You may have encountered a situation where you have a SQL statement and this statement is running much slower than you expected. And by running slowly, the statement is really affecting the usability of your application. What you need to do is to figure out how to get this statement to run faster. But maybe you aren't quite sure where to start. If so, this module is for you. In this module, we're going to cover how you can have Oracle give you the execution plan it is using to process your SQL statement. This execution plan is all of the individual steps that Oracle is performing in order to complete your SQL statement. By understanding each of these individual steps and the metrics Oracle provides with each one, you'll be able to quickly identify which aspects of your SQL statement are in need of performance tuning. From here, you'll be able to take some corrective action. Which could mean rewriting parts of your query or adding an index. In any case though, you will know where to concentrate your tuning efforts. So, let's get started. Imagine that you wanted to drive from Chicago to New York. How would you get there? What roads would you take? Most of us would probably use a tool like Google Maps and type in Chicago to New York. And in an instant, we would be presented with turn-by-turn directions of the fastest route. But, how does a tool like Google Maps figure out what is the fastest route amongst all of the choices? There are any number of different roads that you can take. Is the route hat is the shortest distance the fastest? Would you take the freeway or would a local highway be a better choice in some areas? How do you account for traffic and road construction? If you think about it, what Google Maps is doing is very similar to what Oracle has to do when we submit a SQL statement. We're telling Oracle what we want. I need these columns, from these tables, that match these conditions. This is our way of saying, I want to go from Chicago to New York. Oracle is looking at data like the number of rows in a table, what indexes are available, and how selective these indexes are in order to make its decisions on how to perform our statement. This is just like Google Maps looking at factors like what roads are available, speed limits on those roads? And how to account for traffic and road construction? In the end, both tools come up with a plan. In Oracle, this plan is called an execution plan. And it tells Oracle the most efficient way to process your SQL statement. In Google Maps, the plan is turn-by-turn directions that will get you from Chicago to New York in the shortest amount of time. You may not be familiar with the term execution plan. Or you may have heard the term but wasn't really sure what it meant. What an execution plan is, is the collection of individual steps Oracle has to perform in order to execute your SQL statement. Each one of these steps is a basic operation. Like reading from a table, reading from an index, or performing a join. There are a limited number of these basic operations, about 200. Don't worry though. You don't need to memorize what all 200 do. By knowing the 10 to 12 most frequently used operations, you'll be well equipped to understand most execution plans. Each step in the Execution Plan will also contain information about how expensive Oracle estimates that step to be. This value is calculated based on database statistics and takes into account both the CPU and IO cost of that individual operation. This cost provides us two things. First, it lets Oracle make a decision between performing a step two different ways. That is, is it faster to use an index or just read the table directly? Second, because we get a cost value for each step, we can easily see which steps are the most expensive ones. So, we can focus our tuning efforts there. By understanding the execution plan a statement is using, you can quickly zero in on what operations are making a particular statement run slow. So, let's take a look at how to generate an execution plan as this will help give us a better feel for what an execution plan is really all about.

Getting an Execution Plan

I'm in SQL Developer and I have a SQL statement here in my text editor and I want to get the execution plan for this statement. Right off the top, I want to mention that in SQL developer and other Oracle documentation, you're going to see the term explain plan. What explain plan means is that you want to have Oracle run an operation and have Oracle explain what the execution plan is that it would use to process the SQL statement. So for our purposes, saying explain plan or generate the execution plan are synonymous. So, don't be confused by the slight variation in terminology. The first thing I want to do is show you how I have SQL developer configured to display an execution plan. To configure what data is shown, you first want to go to the Tools Menu and then down here to Preferences. You then want to click on Database, open this up and then find Autotrace/Explain Plan. The configuration for the execution plan is on the right in my version of SQL Developer which is SQL Developer 4. And what I want to do is show you that I have four columns checked. Cardinality, Cost, Object Name, and if I go down to the bottom here the Predicates Column. You can experiment with turning on other columns, as they may provide a little bit of additional information. For example, by selecting the IO_COST or CPU_COST columns, and those are up here at the top, you can break the cost out by those two factors respectively. However, most of these options just alter the way that we're viewing the data in some way. But it's all the same data. But do experiment, as everyone's preference is a little bit different in terms of how they like to view an execution plan. So, let's go ahead and exit out of this. And, then in your tool bar, you're going to have a button that looks like this. I'm going to circle it on the screen here. And, the tool tip on that button is going to be Explain Plan. The location of this button has moved around a little bit in different versions of SQL Developer. So take a look and see where it is on the version that you have. Clicking on this button will instruct SQL Developer to send the current SQL statement to Oracle and get the execution plan back. If you prefer keyboard shortcuts, the F10 key is the shortcut key and that does seem to be consistent between different versions of SQL Developer. So, let's go ahead and generate our execution plan. And again, when we do this, Oracle is not actually running our SQL statement, it's just generating the execution plan and returning it back to us. This is useful to know in a couple of scenarios. First, if you have a DML statement like an update or delete, getting the execution plan does not actually run those statements. So you aren't affecting any data. Second, if you have a very resource intensive query, again that query's not actually running when you get the execution plan. So, you're safe in getting the execution plan to start your tuning efforts and you don't have to worry about impacting other users of your database.

What Does an Execution Plan Contain?

So, before we get into analyzing this execution plan, let's take a moment to describe what each one of the columns means in the execution plan. The first column is the Operation Column. And that tells us the operation that Oracle is performing in each step. These are operations such as reading a table, reading an index, performing a join, or doing a sort. And there are variations on each of these operations. Later in this module, we'll go through all of the operations you see here, and some of the other more common operations, so you can understand what Oracle's doing in each step. The second column is the Object_Name Column and this tells us the name of the database object, whether a table or an index, that Oracle is accessing in this step of the execution plan. The third column is Cardinality and what this is is an estimate of the number of rows Oracle expects to return from the step. This estimate is based on the statistics Oracle has for the table or indexes involved in each step. It's a useful column to look at in order to get an idea of the relative size of the result set Oracle's producing from each operation in the execution plan. The fourth column is Cost, and this is the estimate of how costly Oracle thinks this operation will be to perform based on the amount of CPU and IO resources the operation will consume. One thing that is important to note is the cost value you see on any line is inclusive not just to the cost of that operation, but also inclusive of any suboperations. So, for example on this merge join operation, we see that we have a cost of 208. That is not the cost of the join alone, but that's the cost of the entire subtree and the join operation together. And you can see this if you, if you collapse the merge join operation. And so, you can see that that is inclusive of all of those operations underneath it. The final two columns which I'll scroll to the right so you can see both of them, are Predicate Columns. These allow you to see a couple of things. First of all, by reading the Predicate Columns, you can correlate conditions that you've specified in your where clause or in a join condition to an operation and the execution plan. For example, I see on this operation here a predicate for the specific student ID, and we recognize this condition as a condition that we specify in our where clause. If you look at some of the other values in the predicate columns, you'll recognize the join conditions that we specify in our SQL statement. The second piece of information that Oracle provides you, is how Oracle is filtering your rows. If you have a value that is in a filter predicate, what this means is that Oracle is first reading all the data that it needs, and then applying the filter operation after the data has been read. If you have a value that's in an access predicate, this means Oracle is able to use the condition Specify to drive the read operation in order to get the data it needs. Thereby, only reading and processing data that matches the conditions specified in the access predicate. Later in this module, we'll get into access and filter predicates in more detail.

Reading an Execution Plan

So, let's go ahead and work on reading what this plan is trying to tell us. And I'll scroll back over to the left here so we can see it. I also want to mention at this time that I've intentionally dropped an index on one of the tables for the purpose of demonstrating some aspects of reading an execution plan. So, what we want to do is we want to read the execution plan from the inside out. So, we want to start at the innermost operation and work our way out from there. In this plan, we'll have a choice of two operations. But we'll start with this operation here. A table access full of the course enrollments table. What this operation is doing is reading all the records out of the course enrollments table. And if we scroll over so we can see our filter predicate again, we see that a condition is being applied so we only get the records for this particular student. You see that Oracle expects this operation to return approximately 25 rows. The estimated cost of this operation is 206. Which is pretty high, and we'll come back to the cost of this operation in a moment. The other operation, which is at the same indent level, is that Oracle's performing an index full scan of the PK grade's index, which is the primary key on the grades table. In an index full-scan operation, Oracle's going to read the data of the entire contents of the index in order, which is important because that data is in sorted order. Now, reading an index doesn't give you the actual data in the table, only whatever columns are in the index and the row IDs that point back to the corresponding rows in the table. So moving outward, we see that the next operation is a table access by row ID. And what is happening here is that Oracle's taking the row IDs that it got from the prior index operation and using those to look up the actual data in the Craig's table. At the same level as this table access by index row ID, we see a sort operation that is occurring over the data we read from the course in Roman's table. And by looking at the access predicate, we see what is driving that sort is the join condition on the grade code column. So, the big picture of what is happening here is that Oracle is reading the data that it needs from the course enrollments table and then sorting the data by grade code. At the same time, Oracle's also reading all the data out of the grades table in sorted order, by use of an index scan operation. And then looking up the corresponding rows. As a result, Oracle has two data sources. Both in sorted order by grade. And that allows Oracle to do a merge join operation between these two data sources which is what we see in this operation here. So, our output from this operation is the course enrollment data for this particular student joined with the grade data. Let's go ahead and collapse this node to make things a little bit easier to read. You see at the same level as the merge join, Oracle is reading data from the course offerings table. And the way that it is doing so is first by conducting an index range scan of the index, ix course offer term code. And it's using the supplied term code FA2012. Which is for the Fall of 2012. In order to drive this operation. What Oracle has done is it look at our where clause and it's determined that we want all of the course offerings for this particular term. And it's noticed that we have an index on the course offerings table that can be used. So, Oracle is looking up all the matching values in this index and then using the results from the index, using a table access by index row ID, to get only the rows it needs from the course offerings table. Moving outward again, Oracle is going to take these two data sets and join them together by making use of a hash join operation. We can see the join criteria here, listed in the access predicates. So, what we have now is a data set that's a combination of the courses our students are enrolled in, the grade information and the information of the course offerings table. So again, let's collapse this node and make things easier to read. And now we see we have just a couple of operations left. Oracle's going to read the data from the courses table. And this will bring in information like the course title and the number of credits and so on. And how Oracle is doing this is by performing a full table scan of the courses table. Then, to join the data into our prior data set, Oracle is using a hash join operation. And finally, we have a sort operation and this is in response to the order by clause specified in our SQL statement.

Analyzing an Execution Plan

The total cost that Oracle's estimating for this statement is 222, and if we expand everything back out You see there's really one operation that is driving this total cost, that's the full table scan of the course enrollments table. And so this is what an execution plan really provides us. It breaks our SQL statement down into the individual steps that article's going to perform, so we can understand how those steps fit together. And we can also see which of those steps may be costly, so we can focus in on those steps and try to improve their performance. This is just like performance tuning that you would do on any other piece of software. Where you were trying to identify the operations that take the most time and resources. And then try to find an alternate way of performing those functions, because these are the operations that drive the overall cost of the algorithm. In this case, the answer's pretty straight forward. We need to have an index on the student ID column in the course enrollments table. This will let Oracle search the index for the student we're looking for. And then it can just access the rows for that student. We don't need to read through the entire table in order to get the data for just our one student. So let's go ahead and add that index now. There we go. Our index has been added. And now let's go and regenerate our execution plan and see what we get. So we see right away, our cost for the overall plan has dropped significantly. It's all the way down to 25. So this is telling us that by adding this index, our SQL statement is much more efficient and should perform much better. We're going to look down through the plan here. We would notice that the full table scan operation is gone. Replaced by an index operation and a subsequent row look up. And we see here that this index range scan operation and its associated row look up is much more efficient than the prior full table scan. And this is the essence of statement level performance tuning. You start by getting the execution plan of the SQL statement that isn't performing in the way you want it to. Examine that plan for what it's doing and then make some sort of modification. Whether adding an index like we did in this case, modifying existing index or maybe rewriting part of the statement. Then you generate the new execution plan and see if the cost of the plan has improved and what may have changed in the plan. This is an iterative process that you continue to follow until you're satisfied that your SQL statement is performing the way that it should.

Autotrace Introduction

There is a second capability that Oracle provides that is very useful for tuning SQL statements and that is the Autotrace Capability. This feature is available both in SQL Developer and SQL Plus. The output in each one looks a little bit different, so I'll demo with use in each tool during this section. The most important difference between generating an execution plan and using the autotrace feature is that when you run a SQL statement using autotrace, Oracle will actually execute your SQL statement. So we have any DML statements here testing be aware of this. And be prepared to roll back your transaction or restore your data in some fashion, because this DML statement will run. Likewise, if you have a very expensive query you're running that statement will run. So you will be consuming system resources on your Oracle database server. For this reason, it's usually best to confine the use of the autotrace feature to your test environments. What is useful about the Autotrace Capability is that Oracle will report back to a number of statistics from the execution of that SQL statement. For tuning of SQL statements, the most interesting values that come back are the number of logical and physical IO's. In addition, you will also get some statistics around the number of sorts performed, both in memory and that require the use of disk. And some information on the number of packets needed to be sent over the network. Let's review for a moment the concepts of logical IO versus physical IO. When your SQL statement needs to read a block of data, and that block of data is already in the Buffer Cache. This results in a logical IO operation. That is Oracle is just reading the block from memory. When your SQL statement needs to read a block of data that is not in the Buffer Cache, Oracle first has to read the block of data from disk into the buffer cache. And this is a physical IO operation. Then Oracle will read the block of data from the Buffer Cache in a logical IO operation. So in this case, the result is both a physical and logical IO operation being performed. The IO operations occur any time Oracle needs to read a block of data to process a SQL statement, regardless of whether the block of data is part of an index or part of the table. What the number of IO operations give you an idea of is how much data Oracle's having to read and process for any given SQL statement and for each step in a SQL statement. And you will also see the term consistent get used throughout the Oracle documentation. The term consistent get is synonymous with the term logical IO. Historically, Oracle has tended to use the term consistent get more often. But over the last two years, we've started to see the term logical IO used more often with Oracle. Just know that these two terms refer to the same thing. In the example that we are getting ready to do, we'll see that the metric is labelled consistent gets. So don't be confused by this. Now that we are straight on terms, what does all of this mean in terms of tuning our SQL statements? What we really want to look at is the number of logical IO's or consistent gets, that our system is performing. And as we tune our statement, we want to see this number go down. Reducing the number of logical IO's, means that we're reducing the amount of data that Oracle needs to process in order to process our SQL statement. Quite simply, processing an array of 1 million elements is going take a lot longer than processing an array with a thousand elements. And the number of logical reads gives us a good feel for the amount of data that our statement is having to process. So if we can do something like write a more restrictive where clause, introduce an index or modify an index to be more selective. We're going to cut down on the amount of data Oracle has to process. And this will show up as a reduction in the number of logical IO's. Now you might be thinking that we want to reduce the number of physical IO's, especially since reading from a block of disk is several orders of magnitude slower than reading a block from of memory. And this is true. We do want to the reduce the amount of physical IO that Oracle has to perform for any statement. However, by reducing the amount of logical IO, we will inherently be reducing the number of physical IO's as well. The fewer blocks our statement needs, the more likely it will be that those blocks will already be in the Buffer Cache. And if we do need to go to disk, it will just be to read the few blocks that are necessary, not to read an entire table of data.

Autotrace Example in SQL Developer

We are in SQL Developer and we have the same SQL statement as before. And in this case, I went back and dropped the index that we added in the last segment. So we can see the impact of that full table scan in terms of logical reads. To execute a statement and get the autotrace results, you want to click this button here. And the tool tip of this button will be autotrace. Again, this button has moved around a little bit in the different versions of SQL Developer. So look around for the correct tool tip in the version of SQL Developer that you have. You can also use the F6 key as a shortcut key to run a statement with autotrace. Just want to say, one more time an autotrace will actually execute your statement in Oracle. So be mindful of that. Let's go ahead and execute the SQL statement with the autotrace capability and see our results. You can see here that we have a new tab in our result area and the name of this tab is called autotrace. And the data in this tab looks very much like our execution plan. And that's because when you run an autotrace, one of the things that it does is give you back the execution plan that was used to run the SQL statement. And that's what we have here. However, in this tree structure, we do have some additional data that we did not have before. First, we have a column, LAST_CR_BUFFER_GETS. This is the number of logical reads that Oracle is performing for this step. Like costs, this value is inclusive not just as the line item that it's on but of any operations in the sub-tree of this operation. So looking at our statement here, we see that the full tables scan operation resulted in 741 logical reads. Which is far and away higher than the other operations in our statement. What this tells us is a couple of things. First, Oracle's having to comparatively read a lot of data in order to perform this operation. And then, there's a lot of data for Oracle to process to go through to get the rows that it needs. The second new column that we see here is called LAST_OUTPUT_ROWS. And what this is? Is the number of rows that were actually output as a result of this step. Remember that the cardinality column tells us how many rows Oracle estimated would be output from this step. The last row's column is the actual value. So in this case, for our full table scan operation again, we see that we're getting 40 rows back. And when we match this up with the number of blocks that Oracle had to read, we see that we're not being very efficient here. Because we had to read a lot of blocks just to find those 40 rows. Just like the cardinality column, the value of the last output rows can give you a good idea of the result set produced by each operation. If you see a large result set in one operation and then in the parent, the numbers of rows decrease dramatically. This would indicate that you have an operation producing a lot of rows, only to be thrown away in the next step. So you'd want to investigate if perhaps you could use something like a more selective where clause in the child operation, so it doesn't have to produce so many rows that are ultimately going to get thrown away. In SQL Developer, we also get this panel down here. Which gives us stats for our session for the time that our SQL statement was executing. And so I'll move this up so that we can see the results that are in here. I will warn you upfront. These stats are actually a little bit off. For example, if we look at the consistent get. For example, if we look at the consistent gets in this lower pane, we see that we have a value of 773. Whereas in our tree structure above, what we have is a value of 769. What is happening here is the query that Oracle uses to fetch the data from the V dollar sign my-stat view is running in the same session. And it is picking up some of the recursive queries Oracle has to run in order to process your SQL statement. For example, the query to find the columns on the table. Consequently, the statistics for the statements get intermingled and that's what you're seeing in this lower pane down here. So you can look at these values, but treat them as approximations. They are close approximations but nonetheless, they're a little bit off. Not just from the data in the upper tree here, but also from what word SQL Plus reports or what will be reported in an Oracle trace. Speaking of SQL Plus, let's go and see how to run an autotrace in SQL Plus. Because SQL Plus does a nice job of formatting the data, which makes a few things a little bit easier to see.

Autotrace Example in SQL*Plus

I'm in SQL Plus and start off with, we want to do two things. First, you want to set your line size to a value so the output doesn't wrap. I usually find that a value of 120 works, so let's go ahead and do that. And the second thing you want to do is you want to enter the following command. Set autotrace traceonly. What this tells SQL Plus to do is to go ahead and run the SQL statement. But just show the autotrace output and don't actually print the results of the SQL statement to the screen. Having done both of these, let's go ahead and enter in our SQL statement and see what we get. We see here that we get three items. The first two items are the Execution Plan and the Predicate Information. And this is much the same information as we saw before, it's just formatted a little bit differently. The third block of data that we get is this section down here and this is giving us some of the overall query statistics. So one of the first things we see is our number of consistent gets and these are our logical reads. And this is the same number as is shown before. We also see the number of physical reads, Oracle has had to perform for this SQL statement and we see that this value is zero. Meaning that all of the data Oracle needed was in the Buffer Pool. Oracle didn't actually have to go out and get any data from disk in this case. Typically though, you will see some physical reads and the larger the numbers of logical and physical reads that have to be performed, the slower your SQL statement is going to run. So you'll want to investigate these, if you have some high numbers in these two rows. Moving down, we see we have some information about the amount of network traffic this SQL statement generated. There were 1,150 bytes that were sent back to the client by Oracle. And in this case, there were two network round trips. So if you have a SQL statement that's returning a lot of data back to a client, you might want to take a look at this data. Because obviously, if a lot of data has to be sent over the network resulting in a lot of network round trips. This is going to have an adverse impact on performance. Next, we have some information about the number of sorts that Oracle has performed. Oracle had to perform two sorts and both of these were able to be performed in memory. If you see that Oracle had to go to disk to perform a sort, this is where you want to be concerned and investigate what is happening. What's going on is that Oracle had a data set that's so large in these cases that it can't be sorted in memory, so it has to go out to disk in order to perform the sort. Obviously, the performance of using a disk in a sort operation is going to be much slower than doing an in memory sort. What you want to do is look at why Oracle has a data set that's so large that it can't be sorted in memory. If you can, trim this data set down to contain fewer rows, so Oracle can perform the sort in memory. Finally, we have the number of rows processed. And in this case, this is the number of rows that the SQL statement returned. So you can see, there's a wealth of information that Oracle makes available to us about what our SQL statement is doing. By looking at the cost of each step in an execution plan, the number of rows that are produced by each step and the number of IO's that need to be performed. We can quickly tell which steps are driving the performance of our SQL statement. And when we do make changes to try to improve this performance, we have a lot of data which we can go to look at and judge how successful those changes were.

Data Size and Execution Plans

I want to take a moment and discuss the impact of the amount of data and the distribution of data in your database for any execution plans you generate. And specifically, what I'm getting at is the size and contents of your database in your development and test environments as opposed to your production environment. The way the Oracle optimizer calculates the cost of various operations is based on database statistics. These are the statistic that tell Oracle how many rows are in a table. What is the distribution of values within a column. And how selective an index is. These statistics are in turn based on the data that is present in your database. That is the actual data populating your tables. So if you have different data between two different regions, the Oracle optimizer will probably give you different execution plans. Each execution plan is correct for its region, but because the two regions contain different data. The answers you get will not translate between those two different regions. What often happens in an organization is that the production database is a full size database. However, the databases used in development test regions are scaled down versions, while all of the schema objects are the same. These databases may only contain a few thousand rows of data. So in this case, if you take a SQL statement and get an execution plan in dev or test, it is quite likely that this execution plan will not match the execution plan you would get in production. And the reason why is because the size of the data is different between the two regions. Meaning the Oracle optimizer is going to make different decisions. Let's see an example of this. I'm back in SQL Developer and what I have done is created a new table, course enrollment small. And this table only has about 4,500 rows in it. So we're simulating the scenario we might have in a development region or even a test region. We have only a small subset of the data loaded. Other than the number of rows in the table, everything else is the same. So the same columns, the same indexes and so on. So let's generate our execution plan and see what we get. You see that we have the execution plan and it has a cost of 3, so much lower than what we had before. Notice as well though, the steps in our execution plan are very different than before. Where before the innermost and therefore first operation was an index range scan on the IX course enrolls student ID index. Now Oracle is first looking at the grades table. In fact, even though the course enroll small table has a index on student ID column, Oracle is choosing not to use it. Instead, deciding that a full table scan is more efficient. We also notice that Oracle's no longer performing has join operations, but instead is using nested loop joins. The point here, isn't to go through each individual difference and explain why Oracle made the decisions that it did. But to illustrate that the amount of data that you have in your database makes a difference in the execution plan that will be generated. The difference is not just in the total cost Oracle reports for the SQL statement, but what operations are chosen and how Oracle arranges these operations are also impacted. You want to make sure that you have an environment available that is representative of your production database, both in terms of the amount of data and the distribution of the data the database contains. It is this environment where you want to conduct performance tuning and testing. This includes generating and analyzing execution plans and any performance tuning for a statement that you might do. As we've just seen, if we're in an environment where our data differs significantly from what we have in production, we're at risk of a couple of things happening. First, we may not detect a performance issue that exists because we don't have enough data or the right data to see how Oracle's really processing the statement. And so consequently, if you are not doing performance testing until the end of your project, you could end up with a lot of last minute rework or worse yet, if you don't do any performance testing at all. You may be working to do some emergency fixes once your application has deployed the production. And of course, this is something we want to avoid. The second rescue run is that in getting a different execution plan, know what the statement is going to be using in production. You might make some incorrect decisions about how to tune your SQL statement. And so the tuning you do may only be partially effective or not effective at all. Again, this is a scenario we want to avoid. Because all of our time is valuable and we want to spend our time and energies working on the most important things.

Summary

In this module, we've looked at the definition of what an execution plan is. How you can generate an execution plan and how to read that plan. We've also looked at how to use the Autotrace Capability to get even more detailed information about the performance of our SQL statements. And we started to touch on how to interpret these results. Being able to read an execution plan and read its results, forms a foundation of how to tune individual SQL statements. In the next module, we're going to dive even deeper into execution plans and understand what the most common operations do. So when you see them in an execution plan, you'll know what Oracle is doing. We'll then wrap up our survey of statement level performance tuning, by looking at some of the common approaches taken to improve the performance of individual SQL statements

Execution Plans in Depth

Introduction

Hello. My name is David Berry. Welcome to this Pluralsight module on Oracle Execution Plans In Depth. In the previous module, we introduced the Concept of Execution Plans. And how to Read an Execution Plan. In this module, we are going to understand some of the most common operations, performed within an Execution Plan In Depth. By understanding, what these operations are doing, you'll be better equipped to interpret, what an Execution Plan is telling you and apply this information to tuning your SQL statement. We are going to cover, these nine operations in this module, as these are the most common operations that you will encounter. At the time of this recording, there're close to 200 different operations that can be performed. But most of this operations are relatively uncommon. Understanding, the nine operations shown here, will give you a solid foundation for understanding most of the Execution Plans you will encounter. If you do run across another operation in your Execution Plan, you can always type the name of that operation into you favorite Search engine to get a full description of what that operation is doing. And in these cases, you will usually find that the concepts presented here will help you understand what these other operations are doing.

Table Access Operations

One of the main categories of Execution Plan Operations are the operations used to access data in tables. These are important to understand, because how Oracle retrieves and subsequently processes, the data needed by your SQL statement is one of the primary drivers of performance. But you want to keep in mind, is that you want to structure your SQL, where Oracle can retrieve and process as little data as possible. This is simple common sense. If Oracle has to retrieve a large number of rows. Process through, all of these rows and end up throwing most of them away. This is going to be inefficient, and that inefficiency will show up in the response time of your SQL statement. The first operation, we will take a look at is a Full Table Scan. In Execution Plan output, you will see the words reversed so it reads Table Scan Full. But pretty much, everyone in the database community refers to this as a Full Table Scan. In this scenario, Oracle is going to read every block for the table, and process each and every row, in every block. If some of the blocks for the table are in Oracle's buffer pool, then Oracle can read those blocks from memory. For all of the remaining blocks, Oracle will need to read those off of disk, causing physical IO to occur. The need to perform, this physical IO and especially, because this potentially could be a large amount of physical IO is one of the primary reasons. Why a Full Table Scan is generally regarded as the most expensive operation that can be performed as part of a SQL statement. If a Table takes up, several thousand or tens of thousands of blocks on disc. It is highly likely, that Oracle will have to read a majority of those blocks from disk. As we know, reading data from disk is comparatively very slow, when compared to reading blocks from the buffer pool. And if we need to read several thousand blocks off of disk, this is going to have an adverse impact on performance. If Oracle does need to read data from disc for a Full Table Scan, it will perform what is called a multiblock read, that is Oracle doesn't just read one block at a time, but just like the name implies will read multiple blocks for the Table on each IO operation. Reading multiple blocks together in this way will be more efficient. However, if the Table is large, there will still be a significant IO cost to performing the Full Table Scan. The other aspect to consider, is that when Oracle is performing a Full Table Scan, it has to process through all of the data in the Table row by row. So, a Table has a hundred million rows. This is clearly going to take some time. The picture to have in your mind here, is a sequential scan type operation. The picture to have in your mind here is a sequential scan type operation, just like if you were in a language like C# or Java and you were iterating through each element of a very large array. If the array that we're iterating through has 10 million or a 100 million elements. Not only is that operation going to take quite a bit of team. But we're also going to expand quite a bit of CPU working our way through an array of that size. These same principles apply in Oracle. For a large Table, we want to avoid a Full Table Scan operation because what is really happening is a sequential scan of the data or Oracles having to examine each and every row on a Table. There's an important distinction though that, for Small Tables performing a Full Table Scan is perfectly fine and may even be the most efficient operation. You will notice on many of your smaller Lookup type Tables that even when an index is available, Oracle will often just perform a Full Table Scan of these Tables to get the data that it needs. What is happening here, is that Oracle knows that if the Table is small enough. It is more efficient, just to read the Table and find the day that you need rather going to an index first, and then correlating the result from the index back to the Table. I mention this, because Full Table Scan operations are one of the main things you look for, when troubleshooting a poorly performing SQL statement. However, for Small Tables performing a Full Table Scan is common and probably the most efficient choice. So you don't need to expend effort, worrying about why Oracle is not using an index in these cases. The next operation to look at, is a Table Access by Index Row ID. In this case, Oracle has the Row IDs of the rows, that are needed probably from an Index Lookup Operation that preceded this operation. By having the Row ID, Oracle is able to first, access the specific block. And then, directly locate the required row inside the block that contains the needed data. Oracle can view this because an Oracle Row ID, contains information both about the block where the data is stored. As well, as the exact location of the Row inside of the block of data. So you can see, this is going to be very efficient because Oracle's able to directly go to the Row that contains the Data that it's looking for. As before, if the needed block is already in the buffer pool, Oracle can simply fetch the block from memory. If the block's not in the buffer pool, Oracle will need to perform physical IO in order to retrieve the block. However, since we're only retrieving blocks that contain the Data we need, this is a much more efficient operation than a Full Table Scan. Even, if Oracle does need to read some blocks from disk, the number of blocks to be read are significantly fewer than in a Full Table Scan operation. When you are reading Table operations in Execution Plans. You will often, see a Filter Predicate listed along with the operation. In this example, we see a Filter Predicate on the Zip Code column, being applied in the Full Table Scan operation. What a Filter Predicate is telling you, is that Oracle is first reading the data and then applying the filter to the data in order to only keep the rows that are needed for this operation. So in this case, we don't have an index on the Zip Code column in the Applications table, so Oracle is having to read all of the rows of the Applications table and then is applying the filter to only keep the rows from Zip Code 90017. Filter Predicates are not only applied to Full Table Scan operations. But also can be applied to Table Access by Row ID operations. In this example, Oracle is using an index on the students table to find the records with this particular ZIP Code. However, the index as defined is only on the ZIP Code column. So in order for Oracle to then find all the students with the last name of Jones in this ZIP Code. Oracle first has to read the corresponding rows in the students table by use of a Table Access by Row ID operation. And then apply a Filter Predicate in order to find only the rows with the last name of Jones. It is normal to see some Filter Predicates on table operations in this way. After all, you're not going to have an index on every column. In this case, Oracle is using the index on a Zip Code column to narrow down the rows, that it needs to examine to a small subset. And then apply a Filter Predicate only to the small subset of rows in order to retrieve the specific rows that it needs. So when you see a Filter Predicate listed in an Execution Plan, keep a couple of things in mind. First, the Filter Predicate is being applied after Oracle has read the rows. And Oracle is applying this predicate to all of the rows in that data set. Therefore, we want to make sure that the target data set, and Oracle is applying the Filter Predicate to, is as small as possible. So the Filter Predicate is being applied to a Full Table Scan. You should probably, investigate the situation to see if an index can be added, so that Oracle can access only the subset of the data that it needs to and not every row in the table. If a Filter Predicate is being applied to a table accessed by a Row ID operation, then the predicate is only being applied to a subset of rows. This is normal behavior as Oracle's applying the filter condition only to the smaller subset of rows after it's performed an Indexed operation.

Demo: Table Access Operations

Let's take a look at some Table operations and Execution Plans in SQL Developer. I have a simple query here, that you might have in an application if you are searching for a particular applicant, in this case by their Zip Code and by their Last name. And currently, there are not any indexes on the Applications table. So I'm going to execute an Auto Trace on this statement, and let's see what we get. So we see, we are indeed, getting a Table Scan Full operation. A couple of things we noticed is that, we are, indeed, reading a large number of blocks. In this case, over 1700. This is what we expect, because we do have a Full Table Scan operation and, of course, we know Oracle has to read through all the blocks for the table. We also see, that this query has a relatively high cost, which again, is something that we expect based on the fact that Oracle's having to do a sequential scan through all the data to find the rows of interest. If we scroll out to the right, look at the Filter Predicates, we see the conditions of our ware clause are listed as Filter Predicates, which as we know, means that Oracle's first reading all of the rows and then it's applying this as a filter. Finally, looking at the Last Output Rows column, we see there's exactly one row that is being matched in our ware clause and that's being returned by this operation. So, all the data here is what we expect on a Full Table Scan operation. Oracle has to sequentially, scan through all the rows in order to apply the filter it needs to, and the result of this is a high number of logical IO operations, and ultimately a high cost. This situation is not just limited to select statements. Let's perform and update statement with this same ware clause and we'll observe the same behavior. And here again, we see we have a high number of consistent gets or logical reads, a high cost for the operation, and again, Oracle lists or ware clause as the Filter Predicate, if we scroll out here to the right. So, no surprises here. It doesn't matter, if you're querying data using a select or performing an update or delete statement. The problem that faces Oracle is the same. It has to scan through, all the data in the table in order to find the rows of interested and for a table of any size, this is going to be an expensive and inefficient process. What I want to do now, is I first want to roll back from our update statement to make sure that we didn't impact any data. So I'll go ahead and do that. And now, I'm going to create an index on the Applications table on the Zip Code. So let's go ahead and do this. Okay, and now we have an index and we'll be able to demonstrate a Table Access by Row ID by running this same query up here. So once again, I'm going to run an Auto Trace on this query and we'll see what we get. So as we would expect we see Oracle using the index that we just created. What is happening is that Oracle is using the Index operation to get back all of the Row IDs of the rows that have this Zip Code 11201. It's specified in our ware clause. And if we look at the left output rows column for our Index operation, we can see that this operation returned 29 rows that have the Zip Code of 11201. In the next segment we'll talk more about Index operations, but what comes back from this Index operation is not the rows themselves, but the Row IDs. These are the values that point to the locations of the rows in the table. And this is where our next operation, the Table Access by Index Row ID comes in. This is where Oracle is taking those Row IDs and looking up first, each block that contains the data and then the exact row inside of the block and that's what's occurring in this step right here. We see that, the Table Access by Index Row ID lists 31 logical IOs, but remember, this is inclusive of the sub three and therefore the two IOs that were performed on the index. So, there are really 20 blocks being read on the table. What this tells us, is that those 29 rows that have the matching Zip Code are randomly distributed throughout our table and, so Oracle has to go read 29 different blocks. Still though, this is much more efficient than before where we were having to read and process over 1,700 blocks. Correspondingly, we see that the cost of this operation is much lower. Now, we aren't done at this point. In this case, our index only told us what rows had a Zip Code of 11201. But, we have a second condition in our ware clause to fulfill. And that is to find the applicant with the Last name of Rivera. To do this, we'll scroll out to the right here. And we can see that the filter predicate is being provided here on the Table Access by Index Row ID operation. And so this is where Oracle's applying that second condition in the ware clause. So we'll scroll back to the left here and what Oracle is doing, is we're first using the index to find the rows with the matching Zip Code, looking up those in the table, and then we're applying a Filter Predicate to those 29 rows that had the matching Zip Code. And the end result is we locate the one row that is of interest to us. What you're seeing here is a typical scenario. You're not always going to have an index that matches every column in your ware clause, but if Oracle can use an index, it can first trim down the data set to a small subset of rows. And then apply the Filter Predicate only against those rows. So overall, we see that when we're doing a look up of just a subset of rows and a Table Access by Row ID operation,. The entire Execution Plan is much more efficient.

Index Lookup Operations

In this section, we're going to look at the most common Index Lookup Operations. These are the operations, where Oracle is traversing the b three structure of an Index to find the Row ID's of the rows that match a condition specified in your where calls or the join condition. These operations tend to be very efficient, so we want to favor these operations where possible. The most common, Index operation is an Index Range Scan. You will see an Index Range Scan used, whenever Oracle is using a non unique Index to perform an Index Lookup Operation. You will also see, this operation used if you are accessing a range of data on a unique index. Either by using, the between keyword, or greater than or less than operators, to specify of the data in you ware clause. The range scan and the name indicate that this operation can and most often will match multiple rows. For example, in this slide querying to the table using the last name of Nelson will result in multiple rows being returned. What this means is that this operation will return multiple row ID's, each one pointing to a row of data that matches the index lookup operation. To perform this operation Oracle is going to traverse the B-tree structure of the index. Working its way down to the leaf nodes at the bottom of the index. Inside of the leaf nodes there will be the index key values. And for each index key a row ID that points to the location where the corresponding data is found. Having the row ID's, Oracle can then use a table access by row ID operation, to access the data it needs and as we've just seen in the prior section, this is a very efficient operation. The data in the Leaf Nodes of the index, which are these nodes at the bottom of the index, are stored in sorted order. So once Oracle's at the leaf block level it can traverse the index horizontally to scan for any data that it needs. If you think about an index on a date column which is the example shown here. You might have a query which is specifying a date time range. What Oracle will do is traverse the index vertically to find the Leaf Node with the first date in the range. And then scan through the leaf nodes sequentially, until it finds the last date time in the date range. Index operations like this are very efficient because this tree structure is usually only three or perhaps four nodes tall. In this way, Oracle only has to read a couple of blocks that contain these branch nodes at the top and perform a couple of comparisons. And then Oracle's already at the leaf node level where it can quickly scan through the data in order to find the rows of interest. Consequently any time you have a table of any size you want to make sure that your SQL statement is using index. By using an index Oracle will only need to read a few blocks to find the appropriate keys in the index and then read only the blocks it needs from the table. Which, again is a relatively small number. By using an index, Oracle is minimizing both the amount of data that it may need to read from disc and the amount of data it has to process. The result is a SQL statement will execute very quickly in terms of response time and be very efficient in using system resources. If you have a unique index on your table and you specify the value that you need in your Where clause. Oracle will perform an index unique scan. Once again, Oracle will traverse the bee tree structure of the index. But in this case, Oracle knows because the index is unique, it is only looking for a single index key and therefore, a single row ID. Once Oracle locates this information, it can go ahead and perform a table access by row ID. If you ever query data in a table based on the primary key this is the operation that Oracle will use and this operation is why primary key queries are so fast. Traversing the index structure is typically only three or four IO operations, and then there will be a single IO operation to get the actually row data from the table. So Oracle is able to locate the row with a minimum amount of effort. Earlier in this module we saw the term access predicate used with index operations in an execution plan. Whenever an index range scan or an index unique scan is used, you will have an access predicate associated with these operations. When Oracle is using an access predicate. It is using a condition in the Where clause or a joined condition, in order to traverse the index and then only read the blocks it needs to. In situations where multiple rows could be returned like a non unique index or a Where clause for a range of values the access predicate also determines the start and stop points of what blocks Oracle has to read in the index. This is why an index operation is so surgical. Because Oracle is reading and processing a minimal amount of information. Rather than the filtering of data going on after data has been read, the filter is being applied up front. This not only saves IO operations both logical and physical IO. But also save CPU because Oracle has many fewer rows to examine. Index operations can also have filter predicates applied. Just like in table operations filter predicates are applied after the data is read from the database object. In this case the index. Since filter predicates are applied after data is read. They do not reduce the amount of data that must be read for the operation. However, a filter predicate on an index operation will only apply to the data read in that index operation. So if an index operation contains both an access and a filter predicate, the access predicate will first be applied to only return a small subset of the index values that match the access predicate. And then the filter predicate will run only on this subset of values, not on every value in the index.

Demo: Index Lookup Operations

We're back in SQL Developer, and let's take a look at some of these index operations. We have the same query in the editor that we've used quite a bit throughout these last couple of modules. So we'll run an auto trace so we can evaluate some index range scan operations. We have a couple of different index range scan operations in this execution plan. But we'll focus on this one here. The operation against the IX course enrolled student ID index. What we see here is that on this index operation. We are able to find the row pointers of the rows we want in just three IO operations. These three IO operations represent Oracle traversing the B tree of the index to get to the leaf node level. And then read the leaf box that have our row pointers in them. If we scroll over to the right we see that Oracle is using the condition on the student ID field we specified in our Where clause to drive the traversal of the index. So now that Oracle has the row pointers it can perform a table access by index row ID, which is the operation right above the index range scan and go get the data it needs. We often see these two operations together and in this case we can see that Oracle's able to find the 40 course enrollment records for this student by performing only 11 total IO operations. Three on the index and eight on the table. To give you some perspective, the course enrollment's table has about 250,000 records, and those records are stored in 768 blocks on disc. So by locating the 40 rows that are of interest to us, in just 11 IO operations, we have a very efficient operation and that's reflected in the cost. When we ran this query in the prior module, without the index, the cost of finding these 40 rows was 206. And now we see that it's all the way down to nine. Let's look at another index operation. And that is an index unique scan operation. So to do that, we'll run this query which is looking up a student in the students table by their email address. And there's a unique index on the email column in our students table. So what we are saying here is that all students have to have a unique email address at our university. We don't allow any students to share email addresses, at least with other students. So let's go ahead and run the auto trace. So taking a look at what we get here, we see our index unique scan operation and this operation is only performing two IO operations to find the row ID of the row that it's looking for. So on just two IO operations, Oracle has traversed the index, found the row ID. And then there's one subsequent IO application in the table access by roll ID operation right above it, for Oracle to read the roll of data. And so we can see, when you have a primary key value or other unique index value that can be used in your SQL statement. This is going to be very, very efficient. There are typically only two or three IO operations to traverse the index structure and then one additional IO operation to get the data for that row. So anytime you can access data by a unique key like this this is going to be your most efficient option. The last item I want to demonstrate is where we have an index range scan operation that is using both an excess predicate and a filter predicate. To do so I'm going to use this query that was used earlier in the module where we were searching for this applicant with the last name of Rivera from the 11201 zip code. What I've done is I've created a new index on the applications table that you see up here. And this index has these three columns in this order. That is last name, then first name, and then ZIP code. So we'll go ahead and perform the Autotrace for this statement, and we see that Oracle is now using this new index in our execution plan. Let's slide out to the right here so that we can see the predicates. And what we see, is Oracle is supplying both conditions to the access predicate. In the module in this course on indexes, we'll talk more about column order in indexes and why column order matters. But as it turns out, Oracle can use the last name column to access this index. But then we don't have the first name column, which is the second column in the index. And so what Oracle is going to do is it's going to go ahead and use the last name to get all of the index keys that have the last name of Rivera. Then it's going to apply the zip code as a filter predicate to those index keys, which we can see out here. As listed in the filter predicates column. If zip code was not part of the index Oracle would first have to read the table rows and then apply the filter. In this way Oracle is applying filter as part of the index operation which saves us from having to read the blocks for the table. The point thought is that in a index range scan operation, it is possible to have both access and filter predicates. What we see in this case, is that we had conditions in our where clause that match the first and third columns in the index. The condition that match the first column or leading edge of our index, became an axis predicate. And the condition that matched the third column was still able to be used on the index operation, just as a filter predicate.

Index Full Scan Operations

In this section we're going to take a look at index full scan operations. These are operations that will read the entire index. So they're not as efficient as simply traversing the tree structure of an index and finding the rows of interest. But since an index typically contains fewer blocks than a table, there are situations where Oracle can take advantage of this. And reading the entire index to get the data it needs, rather than performing a full table scan is more efficient. The next operation to discuss is an index fast full scan operation. In this operation, Oracle is going to read the entire index, and is going to read the index in an unsorted order. What this means is that Oracle is not traversing the tree structure of the index. It's just reading all of the blocks of the index as fast as it can. And so in this sense, an index fast full scan operation is more closely related to a full table scan operation than to the index operations we've just looked at. What is happening is that Oracle has determined that it can get all of the data it needs from the columns in the index. And it doesn't need to subsequently go and read more data from the table. In this way Oracle is using the index as a type of lightweight version of the table. Since an index contains only a subset of the column in the table, it will consumer fewer blocks on disk than what the full table does. So Oracle is using that to its advantage in reading the index. Because this saves IO operations over performing a full table scan. Like a full table scan, an index fast full scan can use multi-block reads. To reduce the number of physical io operations that have to be performed for any blocks that are not already in the buffer pool. On the other hand, while an index will contain fewer blocks than a table, an index by itself can still be a rather large structure, and so while a fast full scan of an index is more efficient than performing a full table scan. It is much less efficient than an index range scan, as you are still potentially processing thousands or tens of thousands of blocks. The final index operation we are going to discuss is an index full scan operation. Note, this is different from the index operation we just discussed and indexed fast full scan operation. In an Index Full Scan operation, Oracle reads the entire contents of the index in sorted order. The typical reason for doing this is that there's an order by clause in the query and every column in the order by clause is contained in the index. And the order of the columns in the order by clause matches the order of the columns in the index. A second scenario is that a query contains a group by clause and that all of the columns in the group by clause are contained within the index. Since Oracle is reading the index in sorted order it will use single block IO operations to read any blocks from the disc that are not in the buffer pool. Consequently, this going to be slower than an index fast full scan that uses multi block IO operations. This operation's going to be faster than a full table scan because again, the index will only contain a few columns and therefore, will be fewer blocks than reading the full table. Also, Oracle is getting the data back in sorted order, and so this saves Oracle from having to perform a sort once it has read the table. However, this operation is reading the entire index and it's doing so using single-block IO's. So overall this is going to be a relatively slow operation.

Join Operations

One of the major features of relational databases is their ability to join two tables together. Understanding how Oracle performs these joins is important to help us understand the performance impact of different join operations on your SQL statement. The first joint operation we're going to take a look at is a nested loops join. This is one of the most common joins used in Oracle. Especially when you're joining smaller datasets together. In a nested loops join you have two row sources. On the slide these row sources are shown as tables but these two row sources don't necessarily have to be tables. They could be any data set that is the result of a prior operation like a previous word joint operation. What Oracle will do is it will designate one of these row sources as the driving source. Usually but not always this will be the smaller of the two data sets. Then Oracle will iterate through each row in the driving source. Checking the inner source for matching rows. The name nested loops joins comes from the fact that when Oracle is iterating of the driving table, this is like the outside loop in a nested loops algorithm. So in this outside loop, Oracle will get a row from the driving table. And then for each row, we'll execute an interlude. Looking for rows that match the driver row inside of this loop. If matching rows are found, then Oracle will output the joining row into the output dataset. From a performance standpoint, this works well at the size of the driving table, shown as the external row source is this pseudocode is small. It is also important that for each row Oracle processes in the outer loop, Oracle is able to efficiently look up matching values in the inner row source, like by using the selected index. Where the nested loops join runs into trouble, is if both data sets are large or if there is not an efficient way to look up a row from the external row source in the inner row source. If there's not an index on the inner row source that can be used, then Oracle will have to perform a sequential scan of all the data in the inner row source. If this inner row source is small, that will be acceptable for performance. But if both our row sources are large, the number of comparison operations that will have to be performed is the number of rows in the inner source multiplied by the number of rows in the outer source. And as you can see, these numbers get big in a hurry. So, does really the size of the two row sources and the ability to efficiently perform a look-up in one of the row sources that drive the performance of the nested loop join. The second join operation we're going to look at is a hash join. A hash join is the most common type of join used when you're joining two large data sets. What Oracle is going to do is take one of the row sources that needs to be joined, usually the smaller one, and build a hash table from this row source. The key for this hash table will be a value calculated from the columns in the join condition. Then, Oracle is going to iterate through the other rows and probe the hash table it just built for matching values. When a match is found, this row will be included in the result set. Oracle will look to perform a hash join when there is no suitable index that can be used in a nested loops join. Since no suitable index exists in order to probe one of the data sources, what Oracle is doing is building its own data structure in memory, the hash table, that can be used to perform the join operation. And this is one of the keys for hash join performance. First, Oracle has to build the hashtable, so Oracle's expending processing resources to do this. Second, if a hashtable Oracle needs to build is large enough it may not fit into memory on the Oracle server, meaning that Oracle have to cache part of the hashtable to its temp table space. Consequently, Oracle will incur cost for physical IO during the joint process when it needs to read and write parts of the hashtable to disk. That being said, hash joins are often the most efficient join operation when large data sets are involved. Let's think of an example where we have two row sources, each containing 10,000 rows, and we want to join these two row sources. If Oracle was to perform a nested loops join, and there was not an index on the join condition that could be used, Oracle may have to perform up to 100 million comparison operations during the course of the nested loops join. I say up to, because internally, Oracle contains some optimizations to account for things like repeated key values. Still, if no index is available, the number of comparisons is going to be relatively high. Using a hash join operation will incur a cost to read through one of the row sources and puts data into a hashtable. But now when we iterate through the second row source we can perform an efficient hashtable look-up. We will only need to do this 10,000 times. And this is the essence of how a hash join works. You're investing some resources in building a hashtable up front but this allows the subsequent probing operation that must be performed to be done so much more efficiently. The final join type that we will look at is a sort merge join. A sort merge join is used when each of the row sources that Oracle is trying to join is in sorted order and that sorted order matches the join condition that is applied. In some cases, the rows in the row sources are already in sorted order because they have been read from an index. In other cases, Oracle will perform a sort against the rows in one or both of the row sources. Then once the rows have been sorted, Oracle can perform a merge operation between the two data sets. And this merge is very efficient because the rows are already sorted. The performance driver for the sort merge join is usually the sort operation. If the rows can be read from an index and therefore are already sorted, then Oracle doesn't have to perform a sort so this saves time and resources. If on the other hand, Oracle does have to perform a sort operation against one or both of the row sources, the Oracle optimizer has to account for this cost in its calculations when deciding whether or not to use a sort merge join. Obviously, it is more expensive to sort large data sets than small ones. And so if the data set is large and unsorted, oftentimes, Oracle will opt for a different join operation. When you're evaluating join operations in execution plans, what is it that you should look for? A good first question to ask is, which table will Oracle choose to access first. Because this table will drive any further join operations and the rest of your SQL statement. For this table that is driving the rest of the SQL statement, we want it to be as selective as possible. That is, to only get back the rows that we really need. If this driving table comes back with a lot of rows that are subsequently going to be thrown away when the table is joined to other tables, Oracle won't be able to use a nested loops join, which is very efficient at joining small datasets. And instead will be forced to resort to something like a hashjoin, where it needs to build a hashtable and memory. The second question to ask yourself is if there any indexes available on columns in the join condition, for both tables in the join. If there are indexes and these indexes are selective, Oracle can use those during the join process to look up matching rows between the two tables. As we have already seen in the prior section, any time data can be accessed through using a tree structure of an index rather than a sequential search. This is going to be much faster. A final question to ask yourself is if your SQL statement really needs to join to a table or not. Many times in my career when I've been asked to help tune SQL statements. I found complex queries joining to tables where it turned out that the data from the table wasn't even used in the application process that was running the SQL statement. There are also times that if you're using multi-column keys in your tables or perhaps that you have done some slightly normalization of your table structure. That you may be able to skip over joining to a link table and instead join directly to the table that has the data you need. So as with all other operations try to keep in mind the big picture of what your application and your SQL statement is trying to accomplish. And make sure that you really need to access each and every table in your SQL statement.

Tuning SQL Statements

So let's take a moment to put together everything we have learned over the last two modules and discuss some guidelines for performance tuning our SQL statements. One of the critical factors in performance tuning is to understand context. If you are writing something like a web application used in a customer service portal or a line of business application used internally by representatives in your call center, queries to your database must be lightning-quick. A few hundred milliseconds at most. First, because these users demand screens that will return within a couple of seconds, and second, because you're supporting a high volume of users. So no one SQL statement can run too long or else you'll have a bottleneck in your system. Conversely, if you are working on a batch application that runs in overnight hours, the parameters are different. Let's say you were working on an application that generates billing statements for customer accounts. You will probably have a query that will find all of the customer accounts that come due and need to have statements generated for today's date. And this one query may return 10,000 or more results by itself. You still want to tune this query to be as fast as possible, but depending upon the structure of your database and join conditions, this query may very well take 30 seconds or a minute to run. And this may constitute good performance based on the data being returned. And business intelligence systems are yet a completely different use case. Performance is important in all of these cases, but the context is very different in each one. For that reason, you can't make universal statements like every statement should return within 500 milliseconds. Or every query should perform less than 1,000 logical reads. While these rules might work in the first use case on this slide, they are unrealistic in the second two. So what are we to do? So to answer this question, let's ask a different question. When you look at a database schema, how do you identify the key tables in that schema? As it is usually these key tables that will drive the performance of your SQL statements. The first item I consider is what role the table is playing in the database schema. Often you're going to have a number of lookup type tables that contain a few hundred rows or less. These are usually never an issue. When I look at a schema for the first time, I try to identify the tables that contain the data that the application is collecting. These are the tables that contain customer information, orders, billing data and the like. The data in these tables isn't static. New data is being added to these tables all the time. These tables are almost always significantly larger than the other tables in the database schema. And, if you're going to run into performance issues, it's usually going to be on these tables. The second item I try to understand, is how do these key tables relate to each other? This is both in a database sense, in terms of understanding foreign keys between the tables, but also in a business sense. How does the business data in these key tables relate? And how does that drive how the user of our application wants to see the data? Most useful business functions will require data from two or more of these key tables. Somewhere within your application you're probably joining two of these key tables together performing a subquery against one of the tables from the other, or something similar. And it is in these areas when you have to combine data from two or more of these key tables that performance problems often crop up first because of the amount of data involved. But second, because the functions we are performing are more complex than just a simple read from a single table. So you want to identify these key intersections amongst your tables. Another method you can use to identify high cost statements is to look for statements that are outliers when compared to other statements in your application. If you have a line of business application with an OLTP database as a backend and most of your SQL statements have a cost in the teens but one statement has a cost of 50, the statement with the cost of 50 is the outlier and this is where you want to look. From here, you want to identify the highest cost operations within the plan, determine what those operations are trying to do and then see if you can tune those operations. As we have seen in this module, tuning the worst performing operation in a SQL statement can oftentimes lead to a dramatic reduction in the cost of that statement and ultimately, the time it takes to process that statement. What are the operations that are candidates to have a high cost? Anytime Oracle has to do a full scan operation, whether against a table or an index, you should be suspicious. You also want to look for operations that have a high number of logical IOs occurring, or operations that return a large number of rows. Especially if on a subsequent step many of these rows end up getting filtered out, or otherwise thrown away. What can you do in these situations? Often this is a sign of poor indexing, which is the topic of the next two modules in this course. You may need to add a new index that the statement can use, or it might be appropriate to modify an existing index, so that it can be more selective. Another option is to consider if it is possible to have a more restrictive where clause to your SQL statement. The more information you give Oracle, the more efficient Oracle can be at trimming down the number of rows it needs to look at and finding the relevant data faster. For example, let's say I have a customer's table with a million rows in it and I need to search for a customer with the last name of Johnson. If the data follows the distribution of last names in the United States, then I would expect to get back about 8100 results. So if I assume I am using an index, Oracle will need to read the data blocks that contain those 8100 results from the table. And even though were are initially using an index Oracle still has to do quite a bit of work to read the table data. If however I'm able to provide some more information like the person I'm looking for live in Colorado and their first name starts with G, Oracle can use this information to be more selective about the rows of queries from the index. And now I might only get back 25 or 50 results from my index operation. This is more efficient for Oracle, but it will also be more efficient for your user if they're being presented with 50 or fewer results to look through, not thousands. What is important is to think about what your SQL statement is trying to accomplish. There is some business function in your application that your SQL statement is designed to support. So ask yourself what the purpose of this business function is. What data does it need? What data is provided to this function as input? By understanding what function your application is trying to accomplish you have a better chance at writing a concise, efficient SQL statement that supports that function, but doesn't waste a lot of effort on items that aren't needed in your application. Just like application code where we want to keep methods short and to the point, we want to do the same thing with our SQL statements. The single responsibility principle applies here as well. Complex SQL statements are hard to test and debug and they often do a lot of unnecessary work. Plus, every table you join in or other function that you have in your SQL statement to perform is another chance for something to go wrong like missing an index. So keep your SQL statements short and to the point and you'll have an easier time ensuring good performance. As a final thought, don't underestimate the ability of a single costly SQL statement to negatively impact performance across your entire application. What your user feels is a business function that is too slow to be usable. What you see is a SQL statement that takes a long time to run. Maybe 20 or 30 seconds when it should only take 500 milliseconds. But there is a darker side to this story. A poorly performing SQL statement is often a very costly statement to execute in terms of resources. If you have a few users in the application function that runs this poorly performing SQL statement. These few instances of the poorly performing SQL statement may be enough to use up all of the CPU on the database server or cause enough disk IO to slow down other statements. This is no different than the situation on your PC where you have one application using 100% of your CPU or performing a large amount of disk access. Not only is that application unresponsive, but any other applications you are running at the same time also becomes slow and unresponsive. This same situation can occur in Oracle if you have a few inefficient SQL statements using a disproportionately high amount of system resources. So now that you have the tools, you can understand what your SQL statements are doing, and how you can tune them in order to maintain optimal performance. Well-tuned SQL statements save resources in your database and this can make your application more scalable. But most importantly, they provide an application experience for your users that is more responsive and ultimately more useable.

Indexing Essentials

Why Indexing Matters

Hello, my name is David Berry. Welcome to this module on Indexing Essentials. Having an effective indexing strategy is probably the number one factor in determining the performance of your database. If you don't have an effective indexing strategy, I can almost guarantee you that your database and consequently your application will perform poorly. Any SQL statements run by your application will have poor response times, and your application will use an excessive amount of resources on the database server. As critical as an indexing strategy is, you would be surprised to find out how many applications have an ineffective indexing strategy or no indexing strategy at all. So in this module and the subsequent modules on indexing, we're going to work to understand indexes in detail so you can effectively apply them to your applications database. And again, our end goal in all of this is for you to have a responsive scalable application, because these are the applications that are more usable for your users and easier to support. So why do indexes matter so much? To understand this, let's first understand how data in your database is stored on disk. In an Oracle table, data is effectively randomly distributed. That is, there is no implied sort order or anything like that. So the data needed for any SQL statement may be anywhere inside of the table. Next, if we think about the data that is required for any given SQL statement, this is typically just a small fraction of the rows in the table. In the university example that we've used throughout this course, we're just loading a class schedule or grades for one student where the course offerings for one department in one term. And it is usually this way that we have statements that don't need all the data in a table, but just the data that matches some relatively selective criteria. So if we put these two factors together where we only need a handful of rows of data, but the data we may need could be anywhere inside of the table. The resulting data access operation that we would have to perform is to read and process the entire table to find the data that we need. And when we say process, we mean conduct some sort of sequential scan operation. This would be like having a book without any sort of table of contents or index at the back, and even no chapter or section titles. To find the information you need, you'd have to read the book from cover to cover. And that just isn't very efficient. What we need is some sort of data structure that can help us navigate the data in the table, so we can more directly go to the data that we need and this structure is an index. Each index is going to be defined with an index key, which is composed of a subset of the columns of the table that are frequently used when searching for data in the table. With this key we'll store an Oracle row ID, which is a pointer to the exact location in the table that actually contains the data. And so what we can do is look up the key value in the separate data structure, the index, and then this will tell us exactly where we have to go in the table to find our data. So just like you would use an index in a book. First, you would look up the keyword that you're interested in, in the index in the back of the book and this would tell you what page you should turn to. In Oracle though, indexes actually work even better than this because index tells you not just what data block the data is in, but the actual location of the row in the data block. So this would be like knowing not just what page to turn to, but what sentence to read in which paragraph. Now, there is a cost to this, and that cost is the fact that an index is an additional data structure. So in order to get the performance benefits of having an index, we're going to have to duplicate some data in the table. When the index is created, there's a computation cost involved in this process in terms of IO to read all the of the rows from the table, CPU to process these rows and create the index, and then additional IO to write the index structure back to disk. And because the index is stored on disk, there is also a cost in terms of storage space. Then, of the index is in existence, there is cost to maintain the index. Any time a column and a table which is part of an index is updated, the index has to be updated as well in order to keep the index in sync with what is in the table. When a row is inserted or deleted from the table, again, the index must be updated. All of this work can't be deferred until a later time. The index has to be kept in sync with what data is in the table. Otherwise the index wouldn't be of much use in helping us find the data that we need in the table. So the index always has to reflect what data is in the table right now. Where all of this leads us is to the idea that we need to consider creating indexes as an investment. Yes, there will be a cost to creating an index and some overhead to maintain that index. But if this index can be used in thousands of other SQL statements a day to make those statements run faster then this is a worthy tradeoff. On the flip side of the coin, if we have an index that is rarely used in any of our SQL statements, we aren't getting much of a benefit out of that index, but we are paying the cost of maintaining the index. And just like in your stock portfolio, you want to recognize these bad investments and get out of them. The rest of this module is about helping you understand how indexes work so you can create effective indexes that will be used by your application. Once again, with this investment idea we want to make sure that we are creating investments that help our application to perform better. Not indexes that are poor investments and slow our application down due to the overhead they take to maintain. To do this, we'll first understand a little bit about the two most common types of indexes in Oracle. The standard B-Tree index and the Bitmap index. Then we're going to start talking about how indexes act, and what you need to understand to make sure they are effective. We'll do this by understanding how column order matters in an index. And move into a discussion about the selectivity of an index and why this is important and how you measure it. Finally, we're going to tie these two concepts together because they do have a relationship and it's important to understand the dynamics of this relationship in order to create good indexes.

B-Tree Indexes

The most common type of index in Oracle is a B-Tree index. The B in B-Tree stands for balance. And it is a balanced tree that a B-Tree index maintains as it's data structure, not a binary tree. A B-Tree index is a good general purpose index that lends itself to a wide variety of applications. It is far and away the type of index that you will use most frequently. A B-Tree index consists of a tree structure that looks similar to what you see on the screen. The first thing to understand is that in these nodes down here at the bottom of the screen, the leaf nodes, the data in the index is stored in sorted order. Why this is important, is that now we can build a tree structure on top of the sorted data and that is what is represented by the green blocks up here, the branch nodes. Within these branch nodes we see these boxes with values like A0 and A1 in them. And what these boxes represent is the value stored in the branched nodes about where the break points are within the data. So we have an operation that is traversing the index it knows from these break points which child node it should read next. And so what is going to happen is that we're going to traverse the index until we get down to one of the blue boxes at the bottom, one of the leaf nodes. The leaf notes contain the index keys and the associated row ID for the index key. So, once in the leaf node Oracle can quickly find the values of the index that match the condition that was passed in and the associated row IDs, which is what we're really after. In Oracle, a row ID contains information not just about the data block of where the data for the row is stored, but the actual location inside of the data block where the row is located. So once we have the row IDs we are after we can very efficiently find the data we're looking for. When Oracle reads and index there are three possibilities. The first possibility is that there are no index keys that match your criteria which means that no rows in the table match that criteria. And the result here is pretty obvious. We get no data back from this operation. The second possibility is that Oracle finds a single index key that matches the criteria used to read the index. This could be because there is just one matching row in the table or this is a unique index. So in this case, Oracle will read that key and return its corresponding row ID. The third and most likely outcome is that there are multiple values in the index that match the criteria being used. For example, you could have an index on the last name column. And for pretty much any last name out there, it's likely that there will be more than one record in the table for that last name. Or you could have an operation that is specifying some sort of range, like a date range. In these cases, Oracle will traverse the index to the first matching key and then read the index forward until it finds the last key that matches the access predicate for the index. And the reason that Oracle can do this is because the rows are in sorted order. Now, what happens if the index keys that match the criteria span across multiple leaf blocks in the index as shown here? That is no problem, because each leaf block contains a bi-directional pointer to the next leaf block. So once Oracle gets to the leaf block level, it can simply read the matching index keys, even if it has to jump over another leaf block. If you have taken any computer science courses in data structures during your academic career, you know that it's generally much faster to traverse a tree structure like this than to perform a sequential scan against a large data structure. In computer science we use the big O notation to describe the expected performance of an algorithm. For a balanced tree, which is what our index is, the average search will be performed in log n operations, where n represents the number of items and the data structure. In a sequential scan, which would correlate to a full table scan, the number of operations is simply n, the items in the data structure. So for a table that has one million rows, we can locate an item in a B-Tree structure in 19 comparisons. Whereas a sequential scan operation will take one million operations. And of course as the number of rows grows, the rate of growth of the log n algorithm is much lower than of the linear algorithm. And in Oracle, the penalty is actually worse than this because in our index we can use the power of the B-Tree structure such that we only have to read a few blocks off of disk, usually less than ten. Whereas in the sequential scan example you have to first perform a large amount of IO and then perform our search. And this is what B-Tree index is really all about. We're paying some overhead to build and maintain this tree structure, but once we have the tree structure, it can be traversed very quickly and efficiently to locate the pointers to the rows that we need.

Bitmap Indexes

A Bitmap index is designed to solve a completely different type of problem, and for that reason, works very differently than a standard B-tree index. A Bitmap index is designed for situations where you have low cardinality columns. That is, columns with just a few distinct values. Think of columns such as gender or marital status. We may have a value that defines the region that a customer is located in. Finally, consider the case where we have a column that maps to a predetermined set of bins. An example of this is age where we might convert ages into a bin that keeps track of who's between 18 and 34, 35 to 49, and so on. All of these are situations that a standard B-tree index will not work very well in because of the low number of distinct values. But this is exactly the situation a Bitmap index was designed for. In a Bitmap index what Oracle is going to do is to construct a two-dimensional array for the data being indexed. As it is shown here, there will be a row in the array for each row in the table. And then or each distinct value contained in the column that we are indexing, Oracle will create a column in the array. From here, what happens is Oracle simply marks a one or a zero in the array element based on whether or not the condition is true for this row. So as we see here, student number 1001 is a female. So the column labeled female in the Bitmap index is marked as true, or one. While the column, male, is marked as zero. Conversely, we see that for student ID 1003, who is a male, then the female column is marked as zero meaning false and the male column is marked as true. So what we have in our Bitmap index is really just a two-dimensional array of bits. And other than the simplicity of the structure there are a couple of advantages to this. First of all, this structure is very space efficient on its own, and secondly, bit arrays like this can be highly compressed making the structure even more space efficient. Consequently, bitmap indexes don't take up much space on disk and so they can be read very quickly by Oracle. What if the column that we're indexing in our bitmap index has more than two distinct values? That's okay. What we're going to do is that we're just going to get more columns in our bitmap array. So, as you see here. You've probably done some bending on ages to convert them into age ranges, and we have four different age range values in our table. These then translate into four distinct columns in our Bitmap index. And so if we have a relatively low number of distinct values and consequently a low number of columns in our Bitmap index, we're going to retain the advantages of the Bitmap index in terms of being space efficient for storage, and reading the index off of disk. Even if we have 50, or 100, or even a few hundred distinct values, this is going to be okay. But if we have a high number of distinct values, several thousand, or tens of thousands, then we'll have to have the same number of columns in our Bitmap index and those advantages go away. This is one of the deciding factors in choosing between a standard B-Tree index and a Bitmap index. And so if you have a lot of unique values then a B-Tree index is a better choice. But for low cardinality columns, columns that have just a few distinct values, a Bitmap index is a better choice. The real power of Bitmap indexes comes when we can join several Bitmap indexes together. Conducting bitwise logical operations between different bit arrays is very efficient. So while none of our columns, individually, has a lot of uniqueness, by having a bitmap index on each column, and then conducting a logical end operation. We can very easily answer questions like finding all the females who are married and in the age range of 35 to 49, which is a typical demographic question that one might want to ask. And again this is what a Bitmap index is really showing. A standard B tree index just isn't a very good fit for this problem. To get enough uniqueness, we'd have to include all of the columns in the B-Tree index, but this index then would be targeted at just this one individual query. If we decided that we wanted to use some different demographic information, let's say replacing the gender column with a column that reflected the highest level of education that this person has completed. Then the B-tree index would not work at all. But by using a bitmap index, it is possible and desirable to have many of these such indexes across the various columns. So we can easily perform these bitwise logical operations between the different bit arrays. And this is going to work very quickly and work very well. If on the last slide you were thinking that Bitmap indexes are designed for data warehousing environments, you are correct. It is a data warehouse environment that we often want to conduct these types of queries where we're aggregating data together, and often we're trying to drill down on data by using a combination of these low cardinality columns. So Bitmap indexes help us solve that problem and they do it very efficiently. However while Bitmap indexes work well for a data warehouse they should never be used in OLTP database. Any time you perform any sort of DML against the column that the Bitmap index is over the process Oracle has to undergo to update the Bitmap index is very expensive and very slow. So Bitmap indexes are not appropriate to use in a database that does any sort of transactional processing. Which means, pretty much, any database used for a line of business application. In a data warehouse environment, where you're batch loading the data, once a night, through some sort of ETL process, you will be fine. But if you try to use a Bitmap index in a transactional database, with any amount of inserts, updates, or deletes, you will quickly find yourself in a deadlock scenario. So don't use Bitmap indexes in your transactional databases, but reserve them for use in your data warehouse applications, where they're really intended to be used. Finally, you should know that Bitmap indexes are only available in Oracle Enterprise Edition, and not Standard Edition. You can also use them in Oracle Personal Edition. But this, of course, is more for just trying things out, not for running a production application. So, if you do have an application for Bitmap indexes, make sure that you're running the right edition of Oracle. The rest of this module is really going to focus on standard B-tree indexes. Because they are more common. And that is what you'll be using in a typical database that would form the back end for any business application that you might be developing. It is important however, to know what a Bitmap index is, so that if you encounter one, you'll know when it is appropriate to consider using one. And when you're better off on choosing a B-tree index.

Index Column Order Matters

Most of the time you're going to be dealing with composite indexes. That is, indexes that contain multiple columns. One of the most important things to recognize about indexes is that the order of columns in the index matter. On the slide we have an index with three columns. State, last name, and first name. So it is the combination of these 3 columns that make up the index key. The order here is important though. The first column listed is state. So this is what its known as the leading edge of the index. And Oracle will only consider using this index if the leading edge of the index. In this case the STATE column is included as a condition in the driving operation. Whether that be a WHERE clause or a joined condition. So if we have the following SQL statement, we see that we have a WHERE clause that contains conditions for last name and first name. Which are both columns in the index. But it does not contain a condition for the state column, and since the state column is the first column, or leading edge of the index, Oracle will not consider using this index to perform this query. So how can we fix this situation? One answer is that we can include the state column in the WHERE clause of our query, and now since our WHERE clause contains all of the columns in the index. Oracle will be able to use this index to perform the query. Now the issue that we have to consider though, so we may not have the state value available for us to search on. But we could write a user interface that would require the user to give us a value for the state, this wouldn't be a very usable application. Our user may only know that they're searching for Sam Harris. And they don't know what state he's from. So we need to consider some additional solutions. What we could also do is to reorder the columns that are in the index. Now the last name column is the first column in the index. So it is the leading edge of the index. Next comes first name. And, finally, state has moved to the third spot in the index. And so, now, we have our orignal query again. And we have conditions in the WHERE clause that match the first 2 columns of our index. And we can say, yes, Oracle would be able to use this index to perform the query. This brings up an important point. For any composite index, your WHERE clause does not have to contain each and every column specified in the index in order to be used. But you do want to have as many columns from the front of the index as you can in your WHERE clause. In this example, the index contains three columns. And in the WHERE clause of our query, we have the first two columns specified. So this index is going to be a pretty good match to run this query. Let's explore this idea further. We'll keep our same index from the last slide. So the column order is still last name, first name and state. But we're going to change up our query slightly. Now we know that we are looking for someone with the last name Harris from the state of Colorado. So in this case we have matching conditions for 2 columns in our index. The first and the third columns. But these columns are not consecutive. So we want to know if Oracle will be able to use this index in our query. The answer is a little bit more complicated than a simple yes or no. Since we do have a condition for the first column in our where clause. Oracle will consider using the index. What will determine if Oracle will choose to use the index is really going to be based on how selective the first column is. We're going to talk about selectivity extensively in another clip in this module. But what Oracle is going to evaluate, is how many rows are eliminated by this condition on the last name. If a column is highly selective, that means that there are only a few rows which will match any value within the column. And in that case oracle would probably use the index. In this case, last_name is probably selective enough that Oracle can use this index. So what Oracle will do, is it will use the provided last_name value of Harris to read all of the keys that start with Harris out of the index. And then Oracle will apply the state value of Colorado as a filter predicate to the index keys, to get just the index keys and corresponding row IDs. That match this condition. The application of this filter predicate occurs after the data has been read from the index, though. So this is more efficient than performing a full table scan, but not as efficient as if we had a condition on the first name. We could have even further reduced the number of values that we needed to read from the index. Remember, though, all of this is predicated on how selective the first column in the index is. If Oracle doesn't feel that that column is selective enough, then it may decide that a full table scan is more efficient. It really just depends on the distribution of data in your database.

Demo: Index Column Order

Let's take a look at a demonstration. We're in SQL developer, and we'll use the applications table to demonstrate what we've just seen. Right now, the only index I have on the applications table is the primary key. So let's go ahead and create our first index. This index will have the zip code column in the first position as the leading edge of the index. Then last name, and then first name. So let's go ahead and create this. And there we go. And now we'll have this query here. Which has conditions in the where clause for the last name and the first name. Which are the second and third columns in this index. So we'll run the query using the Auto-trace, so we can see the plan Oracle used to execute the query, and some stats about the query execution. Okay. We see that Oracle did not use our index here. It did a table access full, and this is because we don't have that leading edge of the index in our where clause. We'll take note of the cost of this operation, which is 478 and the number of logical reads which is over 1700. And we see from the last output rows column, the query did produce only one row, so we worked pretty hard to get that one row. Let's go ahead and add an additional where clause to this query for the zip code and see that we do use the index then. We'll execute this query again and we see indeed now we are using our index. The issue here is that we might not know the zip code value to use in this query. So, what we really have is a mis-match between the index that we have, and the data that we're likely to know. So, let's try out a different index that has the data that we're most likely to know at the beginning of the index. So, we're going to go ahead and drop that index that we just created. And now, we'll create this index, of which the column order is last name, first name and zip code, as you see. Okay, so there we go. Now let's go ahead, and get our query back. We'll clean things up here a little bit in our window. And we're going to run this query. And we see that this query is indeed using the index that is created. And what this is driven by is that we have a match between the first 2 columns in our index and those columns being in the where clause. Now if we go into our query and we get rid of this, the last name column here so we don't have that first column again. We see we're back to using a full table scale. So, let's do 1 more example and what we're going to do here is we're going to get rid of the second column that is in the index. And, we're going to add in the. Zip code column. So that now we have the first and third columns in our index, and now let's see what we get when we run this. So we see that we are still able to use the index. Our cost is a little bit higher. And what this is driven by is that Oracle's first having to read all the entries in the index that have the last name of Harris. And then after it's read these entries it's applying the filter predicate of zip_code to those entries. But that's after they're read from the index. So to wrap up column order, what we want to pay attention to is that when we're working with indexes in our SQL statements, we need to pay attention to the order of the columns in the index and what columns are included in our where clauses. We always want to have that first column of the index in our where clause, and ideally we want to have as many columns consecutively from the beginning of the index as possible in our where clause. To make sure that Oracle can use our indexes.

Index Skip Scan Operations

There is one situation where Oracle can still use an index if the first column in the index is not present in your SQL statement. If the first column in the index has low cardinality. We knew that there are relatively few distinct values. Oracle can perform what is known as an index skip scan operation. The key to this is the number of distinct values in the first column of the index. If there are a few dozen or less distinct values, then there's a stronger chance that Oracle will be able to perform a skip scan operation. For columns that have larger number of distinct values, then the skip scan operation won't make sense. So, what does Oracle do, when it performs a skip scan operation? What is actually happening is that Oracle gets all the distinct values of the first column of the index and then combines these. With the values provided as access predicates in your SQL statement. So in this table that we see here, it is the last name and first name values that were provided in our SQL statement. And those have the blue background. The values with the red background are those that are the distinct values of the enrollment term column that Oracle has determined, and as you can see, the unique values from the first column and the values we provided in our SQL statement. Have been combined together to form 5 distinct sets. Then for each distinct set, Oracle will traverse through the index and search for matching results. So in this case, we have 5 combinations so we're going to traverse the index five different times. When all of this is done, Oracle will aggregate together all of the results it found, and pass those results off to the next operation. So what's really happening here is that Oracle's taking advantage of the fact that the first column has just a few unique values. And, it's just probing the index for each unique value in the first column, looking for results that match the rest of our access predicates. The advantage to this is that in some situations where an index would have been disregarded by Oracle. Now there is chance to use an index operation, and it also means you may be able to reduce the total number of indexes you need in your database. If it is only an infrequent case where the first column is not included, then it can make sense to not create a separate index for these scenarios. But just let Oracle use a skip scan operation instead. The disadvantage to skip scan operations is that they are more expensive than a typical index operation. This is driven by the fact that we're traversing the index multiple times. So this is, by its very nature, slower. And secondly, there's only a narrow subset of conditions that a skip scan operation can be performed. Namely, when the first column has few unique values. I'm not going to go into skip scan operations in too much more detail in this course, as they are somewhat infrequent. You should know, though, that they do exist, and under what circumstances they may occur. One last point I'll make, though, is that if you have an index that's being accessed frequently by skip scan operations. Then you may want to consider a different index that doesn't contain the first column, as this will probably help improve performance of the sequel statement that are using the index skip scan operation.

Index Selectivity

There is a second factor that is important to consider when you are creating and evaluating indexes, and that is the selectivity of the index. When we ask how selective an index is, what we're really talking about is a measure of how unique or distinct each value in the index is. The more distinct each value is, the greater advantage we can take of the tree structure of the index because we're able to eliminate all the values that don't match and quickly zero in on just the values that interest us, and so what we want is to aim to create indexes that are highly selective. Because this helps us to quickly find the rows that we're interested in while reading a minimal number of blocks both from the index and any subsequent table operation. Sometimes, it's easier to understand a concept by having a couple of examples of that concept to consider. So let's do that with selectivity. The most selective indexes are indexes that are unique. So Any Primary Key is a very selective index. Other examples include things like Social Security numbers or Email addresses. We expect that these are going to be distinct for every row in the table, and in fact, we might even have unique indexes on these columns to prevent two instances of a user from having the same Email Address or Social Security number. We also have a column like phone number, which may contain a few duplicates due to the same phone number being re-issued over time, but generally is very unique. At the other end of the spectrum, a column such as gender is not very selective at all. There are only two possible values, so we would expect the data to be roughly divided evenly between these two values. Status codes are another example of columns that are typically not very selective. In each of these cases there just aren't many unique values. So for any one of the values specified we're going to get back a very large percentage of the rows in the table, and this doesn't really help us trim our data set down to a reasonable size. Of course most columns fall somewhere in the middle of this continuum, and what exactly their selectivity is depends upon the distribution of the data in your database. They may or may not form good index candidates on their own and may need to be combined with other columns, just depending upon that distribution of data. So why is index selectivity so important? We'll visualize the process of using an index in a SQL statement to understand this. When Oracle uses an index in a SQL statement, it reads all of the matching entries from the index and then uses the row IDs in those entries to know what rows from the table it needs to read. In a selective index there are only a few matching entries in the index and therefore Oracle only has to read a few blocks from the table to get the actual data that it needs. But if the index has poor selectivity, we'll have a lot of entries returned from the first step. And a lot of entries means a lot more rows to read from the table. As we know, these rows are distributed relatively randomly in the table. So if we have a lot of rows to read from the table, we may end up reading most or all of the table in this scenario. So, what has happened here is that we've had to pay the cost of reading the index and then the cost of reading most or all the table, anyway. So, this is actually less efficient than if we would have just read the entire table in the first place. So when the Oracle optimizer looks at statistics on your table and index, this is one of things that it's trying to figure out. And if the index is not selective enough, Oracle will use a full table scan because that's more efficient. And so even though we're talking about number of blocks read, we know intuitively that the more data that we have to read will probably have to perform more disk access and will need to use more CPU to process that data, and the end result will be a SQL statement that takes longer. If you read about Oracle, you'll see different numbers mentioned like that if you read more than 10% of the rows or 5% of the rows, that Oracle will not use the index. In reality, there is no magic number, but you can say that if your statement uses greater than x% of the rows, then your index won't get used. These are just guidelines. What Oracle is doing is calculating the number of reads it would need to do either way in choosing the path where it has to read and process the least amount of data. Still though, even though these are just guidelines, it's good to keep in mind that if you create an index, and the index is going to return 5% or more of the rows, it is highly likely that the index is not going to get used. What you really want to do is work on making your index and the sequel statements that use the index as selective as possible. Let's formally discuss a formula that we can use to calculate and compare the selectivity for a column, because this will help us in evaluating if a column is a good candidate for an index. To do so, what we'll do is take the number of distinct values for the column in the index, and divide this by the total number of rows in the table. So applying this formula, a unique index would have a value of one. Because we would have the same number of distinct values that there are rows in the table. What this tells us is that higher values as we approach one are better. These indexes are going to be more selective an do a better job of discerning out only the rows of interest to us and therefore provide better performance. Another way of looking at this problem is to flip the numerator and the denominator to where we're dividing the total number of rows in the table by the number of distinct values. And this tells us for any key value in the index how many rows we would expect to be returned. In this case, lower numbers are better because lower numbers mean that we're being more selective in getting fewer rows back. So how do we put some numbers to these values? Let's take the case of a single column index first, because that's the easiest to understand. The denominator is very simple. That is simply the number of rows in the table. And we can do a count over all the rows in the table to get this value. For the numerator, what we want to do is count all the distinct values in the column. And so, the way we do this is to use the syntax shown. We have our count function and inside, were doing a distinct select over the column name. So putting these two values together, we get a measure of the selectivity of this column. Now I do want to warn you, if you do have large tables, the statement shown on this slide can be fairly expensive in terms of system resources on your Oracle server. So these statements are best run in a test environment or at non-peak times, if you're running them against your production database. If you are interested in looking at the selectivity of multiple columns in a table, you can also do so by looking at the statistics that Oracle has on that table. The first thing that you want to do is make sure that statistics for the table are up to date. And you can do that by running this query here which will pull the last analyze date out of the all tables view. If you look at this date and decide it's prudent to re-gather stats on the table to make sure that their up to date, you'll want to take and use the DBMS stats package in Oracle. I'm showing two ways to call DBMS stats here. The first is the simplest form where Oracle will actually compute statistics over all rows in the table. All you need to provide is the owner or schema name of the table and the table name itself. The second example shows where you just want to sample some of the rows of the table. In this case 25%. The reason that you might want to just sample some of the table is the table might be very large, so looking at very row to compute stats will take a long time and use a lot of system resources. And you may be confident that your data distribution doesn't vary that much, so sampling a logical percentage of rows will give you the results that you need. Once you are satisfied that Oracle has up to date statistics for your table. You can simply run this query against the alt tabbed column's view. And this will show you the number of distinct values for each column. The result that you get back will look something like what I have here. And this is useful for being able to quickly scan through the columns in your table. To be able to understand what columns tend to be more unique.

Selectivity for Composite Indexes

So far, we've looked at selectivity in the context of just a single column. But as we all know, we frequently have indexes that contain multiple columns, so let's turn our attention to these composite indexes. The denominator, the number of rows, is easy. That's just the same query as before. For a multi-column index, though, we need to know the number of unique combinations between all the columns that are involved in the index. This will equate to the number of unique keys in the index. Now what you can't do is use a count distinct function over multiple columns. Oracle will return you an error because that's not supported. But what you can do is something like this, reading from the inside out, first we are going to determine all of our unique combinations that exist for these columns with our select distinct query. And then this whole query is wrapped in an inline view so that we can count the number of distinct values. So, this really isn't much different than what we did earlier, the syntax just looks a little bit different. How does this apply when we have an index that has multiple columns but our SQL statement is using only a subset of those columns? To answer this, let's go back to an example that we looked at earlier in the module, where we have this composite index over three columns last_name, first_name and state, and that is the order of the columns in the index. So we could run the queries from the last slide and we would calculate the selectivity of the index based on those three columns. However, this selectivity is only going to apply if we have all three columns specified in our SQL statement. So what happens when we have a statement like this? Or our SQL statement has conditions for the first two columns in the index, the answer is that the selectivity for the index for its use in this statement is calculated over the first two columns in the index,. Because these are the only two columns that this statement is using. So Oracle can still use the index it just won't be quite as selective as if all three columns where used. Now let's consider another SQL statement that we looked at earlier. And in this statement, we again have two columns of our index present as conditions in our where clause. But in this case, it's the first and third columns that are present. As we talked about earlier, column order matters. And what our selectivity is based on is the number of consecutive columns from the beginning of the index that are present in the sequel statement. So in this case, that is just one column, the last name column. So it is the selectivity of this column that matters when to terminate if it is efficient for Oracle to use this index. The point of all this though is that Oracle can use an index when only some of the columns in the index are specified in the SQL statement, but in these cases the selectivity of the index is determined not by all the columns in the index. But on the leading portion of the index that contains columns that match columns specified in your SQL statement. And so if this subset of columns is not selective enough, Oracle may choose to use a different index or resort to a full table scan.

Demo: Index Selectivity

Let's take a look at selectivity in a demonstration and see how this impacts indexing. Once again, we use the applications table and just to make sure that we're starting fresh, I removed all the indexes from this table except for the primary key. The first thing that I want to do is to get a row count on this table because that's denominator in all of our calculations. So to do so, we'll run this query from the slides and we see that we have 90,000 rows in the table. So this isn't nearly as large as some of the tables that many of you are dealing with in your applications, but it's large enough to perform some demonstrations against. So, first, we're going to look at selectivity of a single column, and we're going to do that on the last name column. So, to find the number of distinct last names, we're going to run this query here. And we see that we have about 15,000 a little bit under that of distinct last names in our table. This selectivity value comes out to a value of .15 so that's somewhere in the middle. Let's take a look at distinct keys for each column in the table. So that we can get a feel for the relative uniqueness in each column. And so to do this, we're first going to gather stats on the table. Okay, so now we've gathered stats. And now we're going to run this query that we had from the slides. So we see columns like phone number and email are at the top of the list and these are very unique. We've got a pretty good drop off down to last name and ZIP code. Those are similar in terms of uniqueness. But there is quite a bit of a drop off. And then we've got a pretty significant drop off down to first name and date of birth. These columns just really aren't all that selective. And so what this is telling us is that if we create an index solely on a field like date of birth, that probably wouldn't be very effective or only marginally effective because, at least in this database. That call doesn't tend to be very unique. Let's take a look at the selectivity of a couple of columns in a combination now, and to do that, we're going to take a look at last name and first name together. Again, to do this, we have this query from the slides, and we'll run that. And now we see we have many more unique combinations when we have both of these fields together, around 81,000. Before we were just a little bit under 15,000. So this gives us a selectivity value of about .9 and that's much better. So now that we have some calculations and some numbers to keep in mind. Let's go ahead and run a couple of examples and see how this impacts your indexes and the queries you run against your database. So, the first thing that I'm going to do is I'm going to create an index and I'm only going to have one column in that index, the last name column. There we go, the index is created. And now I'm going to run this query that we've been using against our database. And I'm going to run it with an autotrace here. So we can see that we are indeed using our index here. However, we can only use this index to get the entries that have the matching last name, and that turns out to be 347 rows. And then Oracle has to go read all those rows out of the table, and only after that can it perform the comparison on first name. So even though we're using an index here, our query really isn't all that efficient. We have a cost of 341 and we're having to read 321 blocks from disk. This is driven by the fact that we really aren't that selective in our in depth. So we're getting back a lot of rows from this index but we have to perform some subsequent processing on. So let's go ahead and drop this index. And now we'll create a better index, let's clean up our window hit a little bit first though. And now we're going to create an index on both last_name and first_name. Okay, so there we go. And now let's run our query again with an auto-trace and see what happens. And we see now that we're much more efficient. The cost of the query has dropped all the way down to two, and what does this driven by is the fact that our index is more selective. We're getting back many fewer entries from the index, and because of this, there are fewer rows that we have to go and look up in the table and read those blocks off of disk. So the learning to draw from this is we want to make sure not just that we're using an index, but we also want to make sure that we're being as selective as possible when we're using that index. This may mean making sure that other criteria that are in the where clause are also in the index because this allows Oracle to read fewer entries from the index and this is going to save us time and resources up on our database server.

Determining Index Column Order

Where all of this leads us to is that when we're creating indexes, we need to carefully consider what columns we are putting in the index, as well as the orders of the columns in the index. If our index is not selective enough, or we have columns in the wrong order, as we have seen, our index won't get used, or won't be very efficient. So what is some guidance for how we decide what columns should be in our index? First of all, we want the columns that are most frequently used in where clauses and join conditions at the front of the index. This is due to the fact that in most scenarios. If your SQL statement doesn't contain a condition for the first column in your index then Oracle will not consider using the index. So the most important criteria is to get the columns that are most frequently used to access the data in a table in the leading edges of the index and then try to have as many of the other frequently used columns in the front part of the index. The other factor you want to keep track of is selectivity. You want to make sure that your indexes are highly selective. And this extends past your indexes, really, to what combination of columns you're specifying in your WHERE clause. A selective criteria returns fewer rouges, which not only performs better, but is probably better for your user, as well, because no one wants to sort through 10,000 rows that are returned by a query. So when you are building your indexes pay attention to how selective they are. If you have a couple of columns that individually aren't very selective. Look to see if a combination of these columns is selective. If it is, this can be a good candidate for an index. Just make sure that you always include all of the columns in your WHERE clause. If you are able to query your table by highly selective columns like email address or phone number this is going to be very beneficial for performance. So be on the lookout for whatever these highly selective columns are in your application and see if it makes sense to access your data by using these columns. One last point, many people have trouble deciding the order to put columns in their indexes. Because they can't decide if they should put the most selective column in the first position, or the most frequently used column in this position. In these cases the guides from the database community is to put the most frequently used column first. The reason for this has to do with column order, and that even if a column is very selective, if it is not included as a predicate in the SQL statement, it won't get used. And then this will disqualify Oracle from considering your index. So favor the columns that are most frequently used to query your data, and put these at the front of the index. This covers the essentials of indexes in Oracle. You should have a good foundation now in how indexes work and why they work the way they do. From here, in the next two modules, we'll cover some more advanced topics and how to handle some special scenarios, so that you get the most out of indexing when these situations do occur.

Advanced Indexing Techniques

Module Outline

Hello. My name is David Berry. Welcome to this module on Advanced Indexing Techniques. In the last module, we covered the essentials of indexing. In this module, we're going to move into some of the more advanced techniques that you can apply in order to maximize the performance of your data access layer. First up, we're going to talk about covering indexes. This is a technique where Oracle can get all the data it needs from the index, avoiding a table operation which consequently reduces IO and the amount of time it takes a query to run. Second, we're going to look at function based indexes, and we're going to do so in two parts. Function based indexes are a technique that is unique to Oracle. So, if you come from another database, you may not be familiar with their application. We'll work through two scenarios or function based indexes to provide an elegant solution to common problems. The scenarios being conducting a case and sensitive search and an index over a subset of rows in a table. Through these examples, you'll be well equipped to design your own function based indexes to address any special situations you might have. Third, we'll talk about index compression which is a way that Oracle can save space by replacing a repeated part of an index key with a special prefix value. As it turns out, this will also provide a performance boost in many situations. Finally, we'll talk about invisible indexes which can be useful to employ when you want to create a new index to try out or drop an existing index, and you need to have some more control over how and when this is done.

Covering Indexes

Typically, when an index is used in a query, Oracle will first read the index to get the matching index keys, and then look up the corresponding rows in the table using the row IDs read from the index. However, if all the data needed by your query is contained within the index, then Oracle is able to skip the Table Lookup operation, and just return the data directly from the index. When this happens, the index is said to be a covering index, because the index is able to cover all of the data needs for this query. Whether an index is a covering index or not, it's query dependent. But if the index is a covering index, the query will perform faster because Oracle doesn't have to perform the row lookup operation on the table, thereby, saving IO from the table as well as processing time. The easiest way to understand the concept of covering indexes is to jump directly in to an example. So let's do that. I have a very simple asp.net MVC application here, and this page is used to search for students in the database. For this search form, the last name is required and all the other fields are optional. So you can imagine this form helps you search for someone if you know their last name, and maybe what department their majors in, if they're a first year student, second year student, and so on, or if you know their first name. So let's enter a last name and perform a search. So we see our results here. We can also go back and be a little bit more specific if we want to spell out a specific department. And again, now our results are narrowed down. So this is the output that we get. We have here the student's name, we have the information on what they're majoring and then their class standing. Again, that's first year, second year, what year of their studies that they're in. And if you click on the details button, you'll get a little bit more detailed information. Obviously, this is not a real application here or we'd have much more detailed information. What we really want to focus on, though, is the search process and the results that come back. And, the important thing to note is these columns that come back being the Student ID, First and Last Name, their Degree, and their Class Standing. So, let's take a look at the query that's producing these search results, and the index that supports this search. I'm in SQL Developer and first off, this is the index that supports the search that we just saw. The last name column is first because that is always included in the search, and then the first name column, the degree the student is pursuing which effectively is their department, and finally, their class standing code. So, something like first year, second year, third year and so on. Down here this is the query that the application was running. There are a couple of joins to get some data from other tables, and then down in the WHERE clauses if one or more of these criteria was included in the search, then the corresponding criteria in the WHERE clause would be included in the statement it wouldn't be commented out as shown here. So let's run this query with an auto trace and see what Oracle is doing for an execution plan. What I want to focus on is this pair of operations right here. The index lookup in the IX STUDENT SEARCH NAME index and the resulting table lookup in the STUDENTS table. As it turns out, the only reason we are performing the lookup to the STUDENTS table is because we need to get the value of one column, Student ID. The other four columns this query need can all be found in the IX Students Name Search Index. So it's just this one column that Oracle's performing this table access by row ID lookup for. This is causing 32 blocks of IO to occur in this case. Remember, the 34 blocks shown on this line includes all the sub trees, so two of these blocks are from the index operation that we see just below, which leaves us with 32 blocks for this table access by index row ID operation. What I want to show you is that if I drop the existing index, and I recreate the index with the student ID column included at the end. Now let's rerun our query and see what we get. We see that now Oracle is no longer performing the table lookup. It is able to get all of the information that it needs for this query from the index, and therefore for this query, the index is said to be a covering index. In this case we do have a performance benefit in that we're saving 32 logical IO operations. In absolute terms, this isn't very large because our students table isn't a very large table. But in this case, that was about 30% of the blocks that were needed in order to perform this query. If we could identify an index that is a covering index or that we could make a covering index, perhaps, by adding just one column. The benefit we receive is that Oracle does not have to perform the table lookup operation. If you have an index operation that's returning several hundred rows, a covering index can offer quite a benefit. Because these rows are probably randomly distributed in the table, so Oracle would have to read that many blocks in order to get the results that it needs. So you're saving these IO operations from needing to be performed because Oracle is getting all the data it needs from the index. So you may notice that there are some cases where Oracle is performing an index operation and then doing a subsequent table access by row ID to get the value of only one or maybe two columns. And if so, it may be strategic to add those columns to the index and convert the index into a covering index for that query, because this can provide a performance boost for that query. If the index that you have already has all of the columns that you need, then no problem. But if you do need to add a column to an index to make it a covering index. You have to weigh the benefits against the potential drawbacks. First of all, adding a column to the index is going to increase the size of the index. Second, there's the index maintenance cost to consider. If the column you add to the index is updated frequently, then the maintenance cost for this index is going to be higher. Finally, you have to consider that a covering index is probably only going to be a covering index for one or perhaps two query. So, in the end, if you're looking to add a column to an index, just to make that index a covering index, you have to analyze whether or not the benefits you receive are going to outweight the cost of adding this column to that index. In summary, you should be aware of what a covering index is. If you have an important query that is frequently run and the column's needed by the query are a close match to what is already in the index, then adding a column or two to create a covered index is a technique that can be used to boost the performance of the query because you're eliminating the table look up operation. A covering index should by no means be regarded as a silver bullet, though. Every column you add to an index adds overhead. And if you are adding multiple columns to your index, so that now your index is just a slightly skinnier version of your table, then you're going to have to pay for that in your DML operations. So whatever you do, make sure to thoroughly test what you plan to do in a solid test environment. And look not just at the effect on the query you're targeting, but on other SQL statements that use the table, to make sure that the benefits you receive justify the cost that you're going to incur. Apply it wisely, though. The covering index can provide a significant performance boost to a critical query.

Function Based Indexes Introduction

When you create an index in Oracle the index is created over the actual values in the table. There are times though when it is useful to create an index not over the actual values in the table but a derived value. This is where function based indexes come in. With a function based index, you specify a function in your create index statement, then Oracle will run this function against each row of the table to create a derived value, and the index will be built over this derived value. What this allows you to do is to be able to query the table by this derived value and do so very efficiently because now you're conducting an efficient index operation rather than a full table scan. And it turns out there's some situations which make this a very useful capability. The best way to demonstrate the capability of function based indexes is to work through an example. A common problem faced by anyone who works with a Oracle database is to be able to conduct a case insensitive search. In Oracle, all comparisons are done in a case sensitive manner. So this presents a problem when we want to do something like conduct a name search or any type of search on a string type field. Many times we want to be able to store data like name data in mixed case. And in fact, this may be a requirement from our business users. This is where the problem comes in. Since comparisons in Oracle are case sensitive, the only data that we will match is data that matches exactly including case. For this example, we're going to write a query to search for students in our students table by their last name. So regardless if we write the sequel ourselves or had the SQL generated out of an ORM. We're going to end up with a query that looks something like this if we want to search students by last name. Notice in this case that in the value supplied, that the letter N in McNeil is in upper case, reflecting how our user typed this value in to a search box in our application. The problem is that this query is only going to match nine of the rows, not all 11, because two of the rows in the table have the n character in lower case. So if the user was really searching for Dirk or William, they wouldn't find those names in the search results and that would be confusing. Another situation that may occur is that the user types in the name in all lowercase because when the user is typing the name into the search box in our user interface they probably aren't really thinking a whole lot about what case it should be in. So in this case, no records would be returned. The problem here is the case sensitivity of the search. Because of this, the user has to know the exact casing of how the data is stored in the database. So if they don't know the exact casing, they're not going to get the results that they expect to. I think we can all agree this is not a very usable search interface and would probably be pretty frustrating for our users to use. The way that most developers would try to solve this problem is by rewriting the query on the last slide like this using an upper function around both the last name column in the WHERE clause and the value supplied by our user. And now, we remove the case sensitivity from our query. This query is functionally correct. It will return all of the records for any given last name, regardless of the casing. So, from that standpoint, we have success. But there is a problem with this approach. By using a function in the where clause like this. Oracle will no longer be able to use any indexes that may exist on the last name column of our table and will instead resort to a full table scan to get the data. Furthermore, when Oracle runs this query, it's going to have to compute the uppercase version of the last name for all rows in the table. And this is going to be done while the query is executing each and every time we run this query. So in addition to performing a lot of IO to run this query, we're also going to be using a lot of CPU. This is where function based indexes come in. What we can do is to create an index not over the actual values in the last name column, but use the upper function in the create index statement itself and what this will do is first upper case all the last names in the table and then build the index over this upper cased value. Then, when we run our query from the previous slide, Oracle will recognize that the function used in the create index statement and the function used in the where clause match. In the Oracle optimizer then, we'll be able to use the function based index that was built over this derived value. In this way, you are going to get all of the benefits you normally would with a query that uses an index. It's important to recognize though, that the function used in the index and the function used in the where clause must match exactly. Otherwise, the Oracle optimizer won't be able to match up the functions between the SQL statement and the index, and Oracle wouldn't be able to use the function-based index to perform this query. When choosing the function for a function based index, there are some criteria that you want to be aware of. First of all, you can use both built-in Oracle functions as well as user-defined functions. In our example, we're using the upper function, but you could use the lower function. The SOUNDEX function if you wanted to conduct a phonetics search or even the SUBSTR function if that's what you need. Whatever function you use, whether a built-in function or one that you write, the function must be deterministic. That is to say, for a particular set of inputs, the function must always return the same output. Once again, it is the computed value from the function that is stored in the index. So if this value would change over time for any reason, then the index wouldn't work. Finally, you cannot use any aggregate functions to build a function-based index. These operate over multiple rows, not just one row. So they aren't allowed. You also want to make sure to keep performance in mind for any function that you use in a function-based index. And for this reason, you want to keep your functions short and concise. There is overhead in building and maintaining any index. But in a function-based index, this overhead is higher, because you also have to account for the time it takes to execute the function and compute the value. Remember, every time a row is inserted into the table or one of the columns the function-based index is built over is updated. Oracle will have to recompute the value for that row in the function-based index. If you have a lot of complex logic that takes a lot of time in your function, you're going to see an impact on the DML performance of that table, and this probably not what you want. Finally, know that with function-based indexes, you are just limited to a single column. But you can mix and match in composite indexes however you like. In this example, both the last name and first name make use of the upper function. Because that makes sense for those columns, so they can both me searched in a case sensitive manner. But the state column in the index is just a normal column. So as you might expect, if you wanted to use this index in a query that did a case-insensitive search on both first and last name, the respective functions for each column need to match between the index and the where clause as shown here. Constructing indexes in this way is perfectly legal and in fact, very common. There is no restriction on the number of columns that can use functions or in the order that they must appear. All of your normal index rules for index column order and the selectivity apply to computed columns just like a regular column. So let's move into a demo, so we can see function-based indexes in action.

Demo - Function Based Indexes: Case Insensitive Search

In this demonstration, I'm going to demonstrate how you can use a function-based index to conduct a case insensitive search. Right now, I have an index created on the last name column of the students table. But as we see here, this is just a normal index, not a function-based index at this time. So I'm going to use this query that we have from the slides. And I'll execute this query. And we'll see that we get nine rows back. Now, the issue here is that we really should have gotten eleven rows back. Because we have two additional rows in the database with the last name of McNeil. But those are stored in a different case. And I can demonstrate that if I change my query to use a lower case n, in the value of my WHERE clause. And now we see those other two rows. Of course, the problem is with query we're not seeing the original nine rows. And so the bigger problem is whatever our user types in, those are only the rows that we are going to get back. We're not going to get back all of the rows. And there's a problem, because if the user doesn't know the exact case, they are going to be confused by the results. Now, the good news with this query, is that if I execute it using the auto trace capability, we see that we are using our index and we have a very low cost, the cost of three. So our performance is good. It's just the problem with this query is that it returns the wrong answer most of the time. So let's fix this query. And the first fix that we're going to apply is one that's commonly tried. And that's to use the upper function in the where clause, both on the column name and on the value that we're searching for. And now, we execute this query. And we see that we're getting 11 rows back. And this is the right answer. So from a functional point of view, we've solved our problem. The issue here is that when we use the auto trace feature to execute our query, we see that we're no longer using the index that we have on the last name column. We're now doing a full table scan. The reason why is because of the function in the WHERE clause. By adding the upper function to the WHERE clause, Oracle is no longer able to use the index that we have on this table, which is just a normal index. So let's go ahead and drop this index. And we'll recreate this index as a function-based index. And now, I want to gather stats on the table to make sure the Oracle Optimizer has the most up to date information about the index. Technically, from Oracle 10g on, you're not supposed to need to do this anymore. There are times though, where I've seen some odd results by not re-gathering stats on the table. So we'll just be safe here and make sure that stats are up to date. Okay. So we're finished with that. And so, let's go ahead and get rid of this to clean up our window. And now we'll rerun our query up here that's using the upper function. And so we see that we still get the right answer back, which is good. And now, we'll execute this query with Autotrace. And see, we're using an index now to execute this query. And that's the function-based index that we just created. So this is very positive. We're now able to conduct a search that's case insensitive. And, thanks to our function-based index, we're able to use an index operation for this query. So we're getting good performance and we're getting the right answers when running this query. And this is really what function based indexes provide, is the ability to build this index over this computed value in a table. And then you can query that table on that computed value, but still use an efficient index operation and not have Oracle using a full table scan, which of course is much less efficient.

Selective Row Indexing With Function Based Indexes

There is another scenario where function-based indexes turn out to be very useful. And this is when you want to index only a subset of rows. There are many tables that track things like requests, orders, applications, and the like. And usually on these tables, we'll have a value that will indicate the status of that item. For example, the application_status column in the APPLICATIONS table used in the fictional university database that has been used throughout this course. In this simplified model, there are four potential values for this field. Representing new applications yet to be reviewed. Applications in the process of being reviewed. Applications that have been accepted. And applications that have been denied. The nature of the data is such that over time, we'll have lots of historical data. Meaning, the bulk of the rows in this table will a status of either A or D. Representing historical applications that have already been processed. We'll have relatively few applications in the New or In process status, because our admissions counselors will review these applications as they come in, making a disposition, and accepting, or denying, enrollment to those students. From the perspective of our admissions counselors though, it is these applications that are in new or in processing status that are really of interest. This is their work queue that tells them what they're currently working on and what applications need to be reviewed. So, we can imagine a user interface that probably has some sort of data grid. And lists out the new applications that need to be assigned out for review and the applications currently under review. But notice, these two status codes represent only a small fraction of our total data. If you have a system that tracks orders, you have a similar need, in terms that the orders that you are most interested in are the orders that are unprocessed and waiting to be shipped. In a help desk ticketing system, again, you're most interested in the new and open tickets, not the historical tickets that have already been closed. In all of these scenarios, what we're really interested in is accessing and displaying some small subset of rows. Whereas the vast majority of rows are in some sort of closed or completed status that isn't important to us. At least not for this use case. If we try to use a normal B-Tree index, what Oracle is going to do is index all of the rows in the table. Consequently, this index is going to be relatively large on disk, because we're indexing all of these rows. And we're going to have the overhead of maintaining the index for every row. So the problem is that we really want to index only a subset of rows. The subset of rows that we're interested in. And we can do this with a function-based index. In this case, the function we will use is the Oracle CASE function. And in the CASE function, we'll have the values that we want to index simply mapped to the value they already are. The values that we don't want to index, we'll map to a value of null. What this does is takes advantage of a property of B-Tree indexes in Oracle. That if all the values for a key in a B-Tree index is null, then Oracle will not index the row. So let's see what this looks like in a create index statement. Here is the syntax for the create index statement. And the first thing that you notice is the case function that we have. What this is doing is mapping the values that are an N or a P to those same values. And then rows that have any other values are mapped to null. So in short, we map the values that we are interested in indexing to a meaningful value. And the rows we don't want to index get mapped in null. And then what Oracle will do is build an index over these mapped values. But as we said in the last slide, Oracle won't index null keys. So the result is that the index is built only over the rows of interest. In this case, our new and in process application status values. The SQL statement to use this index would look like this. When we are using a function-based index, we have to have the same function in our WHERE clause as we have in the index. So we see that same CASE statement again in the WHERE clause. Now admittedly, this is not a very aesthetically pleasing SQL statement to look at with the CASE statement in the WHERE clause. But again, this is really no different than what we did in the last section with the upper function. The important piece is that the function used here in the WHERE clause of the SQL statement must match the function used to create the index. Otherwise, Oracle will not be able to match up the statement to the index and the index won't get used. If the aesthetics of the SQL statement really bother you. You could always wrap the case statement in a user defined function as shown here. If you do this, you need to make sure that first of all, you include the deterministic keyword in your function definition as shown here. Otherwise, Oracle will give you an error when you try to use this function to create a function based index. And secondly, you need to make sure that this function is indeed deterministic. Just marking the function with a deterministic function does not do that for you. It's just a marker keyword. It is up to you to make sure that the function will always return the same value for a particular set of inputs. Which in this case, it is pretty easy to see that it does. So if we decide to go the user-defined function route. Now, our Create Index statement and SQL statement will look like what you see on the slide. In this case, the SQL is a little cleaner to look at. And in that, it is less likely you'll make an error when typing the statement in. This also shows us that if we had some more complex logic. Say for example, the subset of rows we wanted to index was based on the values of two different columns. It's probably a good idea to encapsulate that logic into a user defined function, rather than coming up with some excessively complex SQL. And again, another benefit of this is that it will have a much cleaner looking SQL statement that we can use in our application. Let's move into a demonstration of an index that indexes only a subset of rows, so we can see this in action.

Demo - Function Based Indexes: Selective Indexing

In this demo, we'll use a function-based index to create an index over a subset of the rows in the applications table. Namely, those rows that have an application status of either N or P, representing new or in process applications. To get started, I want to create a traditional B-Tree Index on the application status column. And to do so, I'll just use the standard syntax. Now that this index is created, let's look at the size of this index, and we can do so with this query. Okay. We see that this index is taking 256 blocks. And this is taking up about two megabytes in this space. We also see that each key on average take up 41 leaf blocks. And of course, this average value reflects those two keys that are associated with almost all of the rows in the table. What is driving all of this is the fact that the index is over all of the rows in the table, not just the rows of interest, which of course are a small fraction of the rows. Now, we can run a query against this table. And we see that this index doesn't need work. The index is getting used and the index operation has a cost of three. The real problem here though, is that we're indexing a bunch of rows that we really don't care about. So let's try out our function based index. So first, we'll drop our existing index. And now, we'll create the function-based index. And again, let's get some information about this index. So we see right away, this index is significantly smaller. It's taking up just eight blocks and it's just 64K on disc. So this is a significant improvement from what we had before. One can imagine that if you had a larger table, these savings would be even more significant. Our table here only has 90,000 rows in it. Now, let's execute the query from the slides to see that this query does indeed use our function based index. And we see indeed that it does. Oracle's using the function based index and the cost is only one, so it's slightly less expensive. But the big benefit here is that now on one of these low cardinality columns we have the ability to index only the rows that we want, that small fraction of rows. So this keeps our index nice and small. In this case the full sized index was just two meg's, but this is because there is only about 90,000 records in the applications table. If you have a very large table, that tracks orders in your organization, or something like that, in that table contains several million rows. As you can imagine, the savings would be much larger from using a function-based index in this way, to track only that small subset of rows that you care about.

Index Compression

Index Compression is another technique that can help you not just to save storage space on your database server. But can also in many cases provide a performance boost to your SQL statements. In a compressed index, what Oracle does is take repeated values at the front of the index key, and compress them into a single prefix value, so if you were to look at the leaf box of a compressed index, you wouldn't see the repeated value over and over again. But a prefix value that mapped to the repeated value that Oracle could look up when it was needed. How this helps from a performance standpoint is that now the index can take up significantly less space on disk. This means the amount of IO needed to read the index is reduced, and usually when we reduce the amount of IO we have a corresponding performance improvement. Take for example the index shown on your screen. This would be a index on the course enrollments table that holds a row for each student registered in each course, and in fact what section the student is signed up for. If you remember back to college, you might have a section of a course that meets on Monday and Wednesday and another on Tuesday and Thursday. And maybe one section that meets at night every week. And this is what the course offering represents that section. The second key in this index is the student id of the student that signed up for the particular course. So if we visualize this data we know that for the course offering or section we're going to have many, many students in that same class. Sections for some classes may have 75, 100, or even more students in that. The result in our index, though, is that the first value in the key of our index has many repeated values as shown here. So what we can do in these cases is use index compression in Oracle, to not store this value over and over again in the index. But instead have all these entries point to a prefix value that represents this repeated value. To accomplish this what we want to do is include the COMPRESS keyword in the create index statement. With a value of the number of columns from the beginning of the index to be compressed. So in this case, we just want to compress the first column. Because that is the part of the index key that is repeated over and over. So we specified a value of one. As shown here a course offering id column that Oracle will replace with a prefixed value in the index key, in order to compress this index. As you might suspect we can compress more than one column in the index by supplying a different value with the compressed key word. Here we show a more generalized CREATE INDEX statement. And in this case we'll compress the first 2 columns of the index. Note a couple of items though about index compression. First of all, the compression works on repeated values of the combination of columns that you specify, not on repeated values individually in each column. So in this case the index compression is going to be on the repeated combinations of column 1 and 2. So for index compression to be effective you have to have enough redundant values in the combination of those 2 columns. Second, note that the columns that do compress are always consecutive from the front of the index. This is because you're replacing that combination. Whether it be 1 or multiple columns with a prefix value. So to do that, you always have to start from the front of the index. As mentioned at the beginning of this section, the primary benefit of index compression is a savings in disc space. The savings can actually be fairly significant. And since disk space on a storage array network or SAN tends to be pretty expensive, you find a lot of DBAs are interested in using index compression where they can. A side effect of this reduced storage space is that since the index takes up fewer blocks on disk. You can read the index faster, and the index will take up less space in Oracle's buffer cache. As we've talked about throughout this course, anytime we can reduce IO, that's a positive for performance. And in this case, we could be reducing the amount of physical IO, because the index is smaller on disc. So often times when you compress and index, you will also see an associated performance improvement. Finally, index compression can be used in both transactional processing databases and in data warehouse environments. In data warehouses there is a tendency to have more indexes that have these repeated key values in front of the index. So index compression is a good fit for these scenarios. But the technique of index compression does not apply exclusively to data warehouse databases. It can also apply in a transactional environment as well. On the downside, there are only some indexes you can supply index compression to. Mainly these indexes that have a lot of repeated values in the front part of the index key. If you have an index with a highly selected first column, then index compression isn't going to work in these cases. And in fact might even result in a larger index due to the overhead involved in creating the prefix value. Second, you will see a slight increase in CPU usage in using index compression. This is because when Oracle encounters the prefix value in the index, it needs to decode the prefix back into the actual value for the index key. So that takes some CPU. If your system is already CPU bound, index compression is probably not the answer to your problems. But if you do have some CPU available, what you are essentially doing is trading an increased in CPU for your reduction in IO. Will this be a performance benefit for your situation? Again, that is something that only testing can reveal. So to summarize, if you do have an index that has a large number of repeated values at the front of the index key, consider index compression. A lot of times, indexes on a foreign key column in a table can be a good candidate for index compression. Because by the very nature of the one to many relationship. You have a lot of repeated values in the child table. In any case, what you're going to want to do is try out different key compression links and measure the impact on performance. What you are likely to find though, while also getting a performance boost at the same time.

Invisible Indexes

One of the capabilities that Oracle provides is the ability to mark an index invisible. What this means is that the index will exist, but the Oracle Optimizer will not use the index in the execution of any SQL statements unless you set a session parameter to tell the Optimizer to consider invisible indexes. This is a primer that you would set manually, and it is on a per session basis. So in general, the index would not be available for use by any of the SQL statements in your applications. Just in a session where you specifically set the parameter. The index will be maintained by Oracle, so for any DML statements that are executed against the table. Oracle will keep this index in sync. And consequently you still have the overhead of maintaining the index. A logical question to ask is why would we want to do such a thing? Well there are a couple of scenarios where making the index invisible can make sense. The first situation is that we have a new index. But before we make it generally available, we want to do some additional testing ourselves, and we don't want to impact the execution plans that anyone else is getting, or how their statements are running at this point. So what you can do is create the new index as invisible. Then you can log into Oracle, set the session parameter to allow your session to use individual indexes. And test the index out, while not impacting anyone else. The second scenario that you might have is that if you have an index that is seldom used and you want to drop the index, but you have a little bit of doubt as to if this index is being used anywhere. If it turns out that the index is being used, and you have a process that starts performing poorly. Then you want to be able to quickly turn the index back on. So what can be done in this situation is to mark the index as invisible. Then, after you're satisfied that the index was not being used, you can drop it, or if need be, you can immediately turn the index back on, and you don't have to go through the process of rebuilding the index. So effectively, what you're doing is a soft drop of the index. So, let's look at some of the syntax around invisible indexes. If we want to create an index and have it be invisible, all we have to do is include the INVISIBLE keyword in the create index statement, like we see here. If we have an existing index that is currently visible and we want to make invisible, we perform an Alter Index and include the invisible keyword. To make an index that is currently invisible visible again, we use an Alter Index statement, but this time, the keyword is visible. As you're working with Invisible Indexes, you may need to check and see what indexes are visible and which are not. To do so, there's a column named visibility in the all indexes view in the Oracle data dictionary. So you can look at this column to determine the current status of the index. Finally, if we're logged into Oracle, and we want to make it to our current session can use invisible indexes. And we need to run an alter session command, and set the parameter optimizer use invisible indexes to true. If we no longer want our session to use invisible indexes, we set this parameter to false. So let's move into a demo, and take a look at invisible indexes in action.

Demo: Invisible Indexes

I'm back in SQL Developer, and what I'm going to do is create an invisible index on the student's table over the zip code column. Right now, the student's table has a few indexes on it, but not one on the zip codes column And, I'll demonstrate that to you by running this query. And so, we see Oracle's performing a full table scan. It's not using an index of any sort in this query. So let's go ahead and create our index. And notice in this create index statement we've included the INVISIBLE keyword. So what'll happen is that Oracle's going to create this index. It just won't be visible to the Oracle optimizer in any of the sessions. So let's get this index created. And now we'll run our query again. And we see we're still using a full table scan. Of course the reason why is because our index is marked as invisible. So we've told the oracle optimizer not to consider using this index. So, let's alter our session. So we can use the index in this session. If you are creating a new index, and just wanted to try it out on a limited basis, and run a couple of queries to see how they act with the index, this would be what you would do. Okay, our session has been altered. So now, when we run this query again, we see that Oracle is using our new index. Now this is the only session that is going to be able to see that index. To prove that to you I've also logged in to SQL Plus in another window. So this is a separate session, and let's run our query over here. As I see here in SQL*lplus we're not using our index. And the reason why is because the index is invisible, and we haven't altered the Oracle session over here in SQL Plus. To allow us to use invisible indexes. So think of SQL*Plus as your application that's running. It's not going to be affected by or use any invisible indexes that might exist. And this is the point. When you have an invisible index, you have to opt in to use that index and in this way, you can test things out and not impact any running applications. Okay. Let's head back to SQL developer. And the first thing I want to do is to disable our session from using invisible indexes. So I'll change this value here to false. And now our session won't be able to use invisible indexes anymore. And we can prove that. If we go back up here and run the query we see that we're back to the full table scan. At this point, we've done all the testing that we want to do. And now we're ready to move forward, and make our index visible so that everyone can use it. To do this we need to run an alter index command on the index. And now our index has been altered and it should be visible. So, if we come back up and run our query again, we see indeed it is using the index. Now, this is true not just for this session but for any session. So, if we go back to SQL Plus and we run our query again over here, we'll see that now we are using the index in this session as well. For the last part of this demonstration we're going to mark the query as invisible once again. To simulate us doing a type of soft drop of the index. So, to do that, I'm just going to change the alter index command to use the invisible key word. And so now our index should be invisible. So if I go back up here and I run the SQL statement one more time, we see that we're back to a full table scan meaning that index has been set back to being invisible. So, there you have it. Invisible indexes give you this type of on off switch functionality for an index. This is useful when you want to perform some testing in a dedicated session, before you make the index available to everyone. Or when you want to drop an index, but have a back out plan to quickly restore the index, in case you encounter any unexpected side effects.

Summary

In this module, we've extended our knowledge of indexes with some of the more advanced techniques that are available. First, we looked at covering indexes. When you have a query that is able to use a covering index. It means that the query can get all the data it needs from the index, and doesn't have to perform a subsequent table lookup. From a performance standpoint, this is a benefit because we saved the IO required to look up the data in the table. Second, we looked at function based indexes. Which allow you to build an index over a computed value, rather than only on the values that are in the table. This is one of the most useful techniques in Oracle, as you can create a sort of virtual column to index your data over. Third, we looked at index compression. If you have an index that has a lot of repeated keys in the front part of the index. This can save you not just a lot of storage space, but also some IO for when Oracle has to read the index. While there is some additional CPU involved when Oracle has to reconstruct the repeated key, the net savings in IO oftentimes provide an overall performance improvement. Finally, we discussed how to make indexes invisible. This is useful for when you want to create a new index and just try it out in one session before making it globally visible. Or when you want to soft drop an index, just in case you need to switch it back on.

Application Indexing Practices

Introduction

Hello. My name is David Berry. Welcome to Pluralsight and this module on Application Indexing Practices. Over the last two modules on indexing, we've covered a lot of ground about the type of indexes in Oracle and how they work. In this module, we're going to focus a little more closely on how to apply indexes to your application database so you can effectively index that database. In the first part of this module, we'll talk about indexing strategies. The primary focus here will be to help you understand what columns you should be indexing in your database to make sure your application runs efficiently. In the second part of this module, we'll talk about the costs associated with indexes and overindexing. Having too many indexes can also be a performance killer. So we'll talk about how you can find out what indexes your application is using and which ones it isn't, so you aren't paying overhead on those indexes that you aren't using. Finally, we'll wrap up by talking about the reasons why Oracle might not be using your index. There is nothing more frustrating than when you've created an index and it doesn't get used and you can't figure out why. So we'll talk about the most common reasons this occurs.

What Should I Index?

The first columns to consider are your primary key columns. The good news is that you do not need to do anything with these columns as Oracle will automatically create a unique index for each primary key. The only action that you might want to take is to supply a meaningful name with your primary key, either in the create table or the alter table statement, whichever way you create the primary key on a table. If you don't supply your name, Oracle will create a system generated name that starts with the characters, sys. I myself like to have my primary keys have a more meaningful name. And one of the reasons for this is because it makes execution plans easier to read. As a side note, make sure that each of your tables has a primary key to find. This is strictly more of a data modeling issue rather than a performance issue but it will save you trouble down the road. One good reason for this is when it comes time to perform an update or delete statement against a row of data, you want to be using the primary key as the criteria in the where clause. First of all this assures that you'll only impact your intended row. But also because the primary key is a unique index these operations will be completed very quickly. If there is not a natural key on the table to serve as a primary key, then create a surrogate key for the table using an Oracle sequence. In the long run you will be glad that you did as overall it just makes the table much easier to work with. The next set of columns you want to consider are any columns that contain unique values in your database. These could be items like username and email address or a combination of columns, like the combination of a student and a course offering which makes sure that a student can only enroll in a particular course once per semester. You want to create these unique indexes on these columns for a couple of reasons. First of all, you want to enforce uniqueness on these columns at the database level. And this is a good thing. You may enforce uniqueness at an application level as well, and you probably should. By enforcing this constraint at the database level, you can be absolutely certain that no duplicates will exist. Secondly, you will often find yourself querying data out of the database using these unique values. For example, when you need to authenticate a user you are usually looking up that user by their user name or their email address, not some sort of numerical user ID. So there's often a good overlap between these unique columns in your database and some of the query requirements your application has. So go ahead and create unique indexes on these columns. The next step you should look at is to index the foreign key columns in your database. There are two reasons for doing this. First of all, these columns are going to be used in any joint operations between two tables. And so you want to index so that Oracle can efficiently perform these joins. Secondly, I often find that many of my queries will follow along the lines of foreign keys between tables. For example, in the table shown here, it is likely we'll have some sort of function in our application to find what courses a particular student is enrolled in. So an index on the STUDENT_ID column in the COURSE_ENROLLMENTS table makes sense. Further, a second index in the COURSE_ENROLLMENTS table over the COURSE_OFFERING_ID column also makes sense. Because it's likely we'll want to have some sort of function to list what students are enrolled in a particular section of a course. So what you will find is that if you get all of foreign key columns indexed, this will provide a good base for indexing your application and the types of queries that you'll end up running. The final aspect to look at is the aspect that is also the most important, and that is what columns are commonly used in your WHERE clauses when querying data. This is where knowledge of how the application works and how data is used by the application is really critical. Because you can use this knowledge to identify how the application will need to extract data out of the database. So you'll be able to take these criteria that your application queries data by and identify the corresponding columns in the database. And these are the columns that likely should be indexed. I always pay special attention to date columns as well when indexing a database. A lot of data ends up being queried by a date range. So it is often important to have some good indexes on a date column in a table. If we think of the canonical customers orders database, we probably aren't just getting all the orders for a customer, but we're probably getting all of the orders for the customer over some date range. Whether this is in the last week, the last month or the last year. So take a look at your date columns and understand how these fit into your application and how your users ultimately want to query your data using these columns. Once you start identifying the columns that need to be indexed for a table, you'll want to see how these columns fit in with the indexes on the foreign keys you've already created. Oftentimes you'll be querying a table by the foreign key values and some additional criteria. So in this case you want to add those additional columns to the index you created on the foreign key columns, not create a separate index. As you go through this process, you want to identify clusters of columns that are always used together to query a table and then what additional criteria are provided some of the time. Columns that are always used to query a table should be grouped together into single composite index. And then, you can add the additional less frequently used columns at the end of these indexes. In doing this, you should start to see patterns emerge in your indexes about how the user will want to access data and these patterns should match what you have in your application. At this point, you should have a good foundation for the indexing strategy of your database. What you'll want to do from here is to take some of the critical SQL statements from your application, and generate execution plans for these to make sure that they're using the indexes that you've just created. Or if there's any fine tuning that you need to do. Ideally you would generate an execution plan for every SQL statement your application runs. Unfortunately as we all know there always seems to be too much to do and not enough time to do it in, so you'll probably need to prioritize the statements you look at. How do you prioritize the most critical statements in your application? These would be SQL statements that run against your largest tables or in your most critical business functions or other most complex SQL statements in your application. For example, if you have a statement that is doing multiple joins or involves a sub query, that is a statement that you probably want to get an execution plan on. Looking at these execution plans, you'll be able to determine how long your index is matched up with these statements and make any adjustments as needed. Performance tuning is often an iterative process, so don't be afraid to try something out at this stage of development. After you figure out these iterations, you'll be able to settle on what is the best indexing strategy for your database.

Indexing Costs and Overhead

Something that you need to be mindful of is overindexing your database. Sometimes in technology, when we learn about a new feature or capability, we have a tendency to apply this capability to every problem we see whether it's a fit or not. There's an old interview question that I'll sometimes ask along these lines of, if indexing columns improves performance, why don't we index every column in a table? The answer is, because indexes are a separate data structure. They take up additional space on disk and they create overhead in that the index has to be maintained for any DML operations against the table. What this really goes back to is the idea that indexes are an investment. It is a worthy tradeoff to pay some of these costs if an index is used thousands of times a day to speed up our SQL statements. But if an index is seldom used or never used, then we're just paying these costs without the benefit. So let's take a moment and discuss these costs in more detail. The first cost is the easiest to understand, and that is disk space. Again, each index is a separate data structure, and this data structure has to be stored on disk. But Oracle is using the index, it will load the blocks it needs from the index into the buffer cache and cache those blocks just like it would blocks in a table. Now Oracle doesn't have to read all of the blocks in the index and cache all of the blocks when it uses the index. It's smart enough to read and cache only the blocks that it needs. The point is though, whether we're talking about storing the index on disk or having the index in the buffer cache, the index is a separate data structure from your table data. And this separate data structure takes up space. What is the size of the structure? You can find out by running this query. For this query you'll need permissions to the DBA segments table in the Oracle data dictionary. And in this case we're joining that to the all indexes view. This version of the query will actually get all the indexes for a particular table and tell you their size on disk both in terms of bytes and the number of blocks they take. Why does the amount of space we are devoting to an index matter? A primary reason is the cost of storage. Now you might be tempted to think that hard drive prices have come way down over the years. And you go online and purchase a hard drive of several terabytes for less than a hundred dollars, so space isn't really that expensive. That's true. And that hard drive would work well in a laptop, but it wouldn't work very well in a storage array network. At least at the time of this recording, space in a SAN remains relatively expensive. So, yes, storage is cheap but fast storage is not, so this is a cost that we have to be mindful of. The other major cost we pay with an index is the overhead in maintaining the index when DML operations are executed against the table. For example, if we have a table with three indexes, we do an insert against the table. Oracle also has to update all three indexes. This means that we aren't just doing one write but a total of four writes. If our table had six indexes rather than three then we'd be doing a total of seven writes, one for the table and six for the indexes. And so you can see the problem, as we get more indexes for any table, doing DML operations against that table increases because we have to keep all those indexes in sync. If we're updating a row in our table then every index that contains a column whose value is being updated will have to be modified. So in this case, we are showing that two of the indexes contain a column that is being updated by our update statement. So we'll have total of three write operations and two for the two indexes that contain the column being updated. And so you can see the overhead the indexing brings. Because indexes have to be kept in sync with the data in the table, Oracle has to incur this overhead whenever a DML statement is run against the table. Some of this you just have to consider the cost of doing business. It's normal that you're going to have a primary key index on a table and anywhere from two to four indexes on the table. And so in these cases, there's nothing you can really do to avoid this overhead. But if you had 10, 12, or more indexes on a table this would really start to impact your DML operations. So you want to look at all of these indexes and understand if they were being used, or if they were just adding overhead. Let's move into a quick demonstration so we can see what the impact of having too many indexes is when we're trying to conduct DML operation.

Demo: Indexing Overhead

Let's do a brief demonstration to see what the impact of index overhead is as we add more indexes to a table. I have two tables in Oracle, APPLICATIONS_ONE_INDEX and APPLICATIONS_OVER_INDEXED. Both of these tables have a primary key so that is one index for each. Then, on the APPLICATIONS_ONE_INDEX table, I have one additional index so that's two indexes total. On the APPLICATIONS_OVER_INDEXED table, I have ten additional indexes. Basically, one on almost every column in the table and that makes a total of 11 indexes for this table. What I'm going to do is run the C# program and it is going to insert 50,000 rows into the table that I specify on the command line. So we'll run this program twice to get results for both tables. If we scroll down You can see that this program is just doing a simple insert. There is an implicit commit in the ODP.NET driver. Having this implicit commit is intentional on my part, I do want to run the test this way. I don't want to do a bulk load, like just selecting all of the rows from another table and inserting those into our two tables. Having that implicit commit is intentional. I'm trying to simulate a little bit better of what a program would do that would be running against a transactional database, where you would be committing after every row and not bulk loading the data into the database. Also we're going to record some metrics for each test run. If we scroll back up, we'll see that we're using a stopwatch here in .net to get the total elapsed time. And then if we scroll down here to the bottom, we have a couple of helper methods. And we can see that the first one, we're going to get the amount of CPU time used by Oracle and this is a value that will come back to us in centiseconds, that's ten millisecond increments. That's the unit that Oracle gives us this value in the v dollar sign set stats table. Finally, we're going to get the number of blocks that Oracle has recorded as being changed. So, we'll get an idea of the IO impact of having more indexes on our table. So, we'll go ahead and get these test runs underway. I will let you know upfront these do take a little bit of time to run, so what I'm going to do is I'm going to get the test run started. I'm going to pause the video and then come back, so what you're seeing is not real time. But there's really not much of a point of you sitting here and staring at a blank power shell prompt for a minute or so while nothing's going on. So, let's go ahead and get the first test kicked off. We'll run the test that's going to insert it into the table that has just the primary key and the one index to start with. Okay there we are it's finished and now we'll go ahead and kick off the second test. Okay and now the second test is finished. We can see we do have a discrepancy in the numbers. What I'm going to do is I'm going to transcribe these numbers onto a power point slide and then come back and discuss them a little bit. So, here are results. I've cleaned the data up a little bit. I've taken all the values and converted those to seconds so that they're a little bit easier to read. And not very surprisingly we see that the table that had ten indexes, plus the primary key, that not just took longer, that also consumed more system resources. And pretty significantly, there were quite a few more block changes for this table, again which is something that we would expect with all those additional indexes to update. Now this data shouldn't dissuade you from having indexes on your table, having indexes is fine. Just realize that there is a cost to your DML statements when you do have those indexes. And what you want to do and, we'll talk about this in the next segment, is you want to make sure that if you have indexes on your table that you're getting value out of those indexes and you don't just have indexes that are there that you're not using. Because those indexes are costing you over head and you're not getting any value out of those indexes. So we'll talk about how to locate those indexes in the next section, so you can evaluate whether or not you really need to keep those.

Similar Indexes

If you try to create the exact same index on a table, meaning you have exactly the same columns in the same order, Oracle won't let you do that and you'll get an error. So having an index that's an exact duplicate in Oracle is not possible. What you are allowed to do is something like what is shown on the slide. In this case I have two very similar indexes. The only difference being the second index shown has the state column as a third column. But notice the first two columns in each index are exactly the same and in the same order. While you are allowed to do this, you probably don't want to. If you have a SQL statement that only uses the first two columns of this index in the where clause, but not the third column, that SQL statement could still use the second index shown on the slide, the one with all three columns. While the indexes aren't duplicates, the second index will work for every SQL statement that the first index would be used for. So you're probably better off just dropping the first index and avoiding the overhead of maintaining that index. Now it is possible that two indexes have the same first column, but different second columns. And that could be okay. Those indexes may be targeted in different queries in fulfilling different roles. But any time you have two indexes that have multiple columns at the front of the index that are the same, meaning the same columns in the same order. You really want to question whether or not you need both indexes. You may find out that one of those indexes isn't being used at all. Or that the sequel statements running using one index will run just as effectively using the other index. So if this is the case, you can drop one of the indexes and save yourself some overhead.

Monitoring Index Usage

Sometimes, you want to monitor what indexes are actually being used. So that you can find and drop any indexes that aren't being used. Oracle provides a capability to do this via a setting on the index. What you do is run the SQL shown: ALTER INDEX, your index name, MONITORING USAGE. This is very low overhead and what Oracle will do is once the index has been used. Get set, you can look at the value of this flag via the v$object usage view with the query shown on the slide. When you're ready to turn index monitoring off, you can run the alter index command again. This time with the non-monitoring keyword, Go. Now this monitoring is pretty basic. It's just a true false flag. It doesn't tell you when the index was used or how many times but it can be used as a first pass to find out if an index is being used or not. If you want to get some more details on how often index is being used, another option you have is to look at the v$sql_plan view. Through this view any execution plan that Oracle currently has cashed in the shared SQL area. You're able to view the individual steps of the execution plan four. So we can query this view and find all the execution plan steps that are using an index and what index they're using. And then join this information back to the alt indexes view and the V$ SQL stats view. To get an idea of how often the index is being used, and by how many different sequel statements. The query you see on your screen is an example of such a query. Now don't worry about pausing the video to write this query down, I'm going to post this query on my blog, which is listed on the title slide of each module. But this does provide us some more information rather than just a simple yes no flag. That having been said, there are some limitations to this approach. Since this query uses the V$ SQL plan view, this is only capturing what statements are in the shared SQL area right now. So a statement that ran a few hours ago may have been cleared out of the cache but wouldn't show up in this query. So you don't just to run this query once and determine that an index isn't being used because maybe a particular index is only used at night during your batch process or something like that. But you could set up a script to run this query every hour for a few days. Maybe dumping the results to a table each time. And then it would be pretty easy to see what indexes are being used and how frequently. Regardless of what you do, a query like this or a different variation does provide you a lot more details, because now you can see how many different statements are using a particular index and how often. There is one other way to get index usage information. And this is through the Oracle AWR, or Automated Workload Repository tool. This is a tool that's separately licensed by Oracle, and it's really designed more for DBA. But, if you suspect that you have some indexes that aren't being used, or only lightly being used talk to your DBA. Your DBA will be able to tell you if you're licensed to use AWR, and if you are, they'll probably be happy to run an AWR report for you that tells you how often a particular index is being used, and which ones aren't being used. This wraps up our discussion of indexing cost and how to find indexes that aren't being used or not being used very much. There are a couple of take aways from this discussion. The first is to realize that indexes do have a cost, both in terms of disk space, and overhead to maintain the index. The second take away though, is that indexing is really all about finding balance. Don't let the fact that an index has some cost associated with it dissuade you from using indexes. If the index is helping a number of other statements run faster, then that is a worthy tradeoff. Maybe you have an index and it's there for one statement, but this is a very critical statement in your nightly batch process that helps that process run in just a few seconds, rather than taking tens of minutes. Again, that's a worthy tradeoff. What you want to be concerned about is that you don't want to be creating indexes that aren't going to get used. These don't serve any purpose and just create overhead. So if you do have some of these indexes out there, you want to find them and drop them and get them out of your database.

Why isn't Oracle Using My Index?

There is probably nothing that frustrates developers more then when you have an index on a table, and you think Oracle should be using the index in your SQL statement. But for some reason, you just can't seem to get Oracle to use the index. Now I assure you, while committing an act of physical violence against your laptop or the Oracle server might make you feel better in the short-term. It won't do anything to solve the problem. In almost all cases, when the Oracle optimizer is choosing not to use an index, it is making the right decision. The optimizer isn't broke or malfunctioning. It is probably something that you have overlooked, either in your SQL statement or in the definition of the index. So to help you understand what the likely causes are. The next few sections will go through the most common reasons why Oracle may not be using your index. And what action you can take to solve the situation. And I promise you no laptops were harmed in the creation of these segments.

Missing Leading Edge of Index

One of the first things you want to check when Oracle is not using an index is if you've included the leading edge. That is the first column of the index in your where of clause. As we discussed in the indexing essentials module, if you don't include the leading edge of the index as part your where of clause, Oracle will not use your index except in the special circumstance of an index skip scan operation. So this is usually the first thing to check. If you're not using the first column in the index, you have two choices. First, you can add that column into the where clause of your sequel statement if you know the value. If you know the value, this is probably the best approach, because we always want to give Oracle as much information as possible so it can efficiently process our statement. Our other option is to consider reordering the columns in our index, or adding a new index that has the columns we do have in our SQL statement at the front of the index. If the SQL statement we are evaluating is representative of what columns will be included in the where clause most of the time. This is probably the best option to investigate. Our goal is to have the columns that are included in our where clause most frequently at the front part of the index. And any column that is always included with the where clause should be the first column in the index. Because if this first column is not present in our statement, Oracle won't use the index.

Index not Selective Enough

Another of the major reasons that an index does not get used is because of a lack of selectivity. Either in the index itself or in the column supply in the where clause. Take for example the query shown on the screen. Which is going to get all the applicants who reside in the state of California. The problem here is that the query just isn't selective enough. We'll return over 10% of the rows from the table with this query. And when Oracle does the math, it will actually be more expensive to use the index than to just read the table directly in a full-table scan. So to solve this problem, we need to improve the selectivity. If our index has only one column in it and this column is not very selective on its own, then we need to add additional columns to the index so we have more distinct keys in our index and improve the selectivity of the index. This isn't enough, though. You also need to make sure to include those columns in the where clause of our SQL statement. Remember, the selectivity of an index for a particular statement is based, not on the total number of columns in the index, but on the number of columns in a consecutive sequence from the front of the index that the SQL statement is using. So you want to make sure not just that your index is selective but, also, your where clause is selective. And this will help ensure that an index operation is the most efficient way to perform a SQL statement and Oracle will use the index.

Using a Like Clause and a Leading Wildcard

There are times when someone decides to wrap the value that they are searching for in both leading and trailing wildcard characters as shown on the slide. Usually the reason for doing this is because they are searching for a string which is not at the start of the value in the column. Or perhaps they're trying to do some sort of keyword search against the text contained in the column. In this case, the trailing wildcard is not a problem. The issue is the leading wildcard character. Whenever we have a leading wildcard carrier, like we see here, Oracle will not use an index on that column. Recall the data in an index is in sorted order. Just like in a telephone book, it doesn't help you very much if you know the second letter in someone's last name, but you don't know the first character. The telephone book, like an Oracle index, is predicated on the fact that you know information not just from the leading edge of the index but from the beginning of the item you are searching for. Otherwise the tree structure of the index doesn't do us much good. How do you solve this problem? The easiest answer is do not ever include the percent sign, which is the wildcard character in SQL at the beginning of your search string. You need to make sure that you know what the first part of the value you are searching for is. Otherwise, Oracle won't be able to use an index. Now there are situations where a developer did indeed intend to search for a specific word in a field. Perhaps you have a description field or a field that represents a business name and you need to search for keywords in that field. If this is the case a traditional index and a like statement probably aren't very good tools to solve this problem. What you need to look at are some of the full text indexing products that are available. There is an Oracle solution to this problem called Oracle Text. This is built in to all the editions of Oracle 12c and if you are already an Oracle shop, it is probably worthwhile to investigate if this solution works for you. The link that you see on the slides will take you to the Oracle Text homepage, where you can get more information about this feature. Your other option is to investigate a third-party full text indexing solution. There are many of these products that are out there on the market, both open source and commercial. If you do need this type of full text query, you're best off using a dedicated package, either Oracle Text or one of these third-party packages rather than trying to design your own solution. Or attempt to shoe horn a standard index into a solution. Because this type of work just isn't what a standard index is designed to support.

Like Clauses and Index Selectivity

Another situation that occurs with a Like clause. Is when for the value of a where clause. There is just a single character or a couple of characters that are given and then the wildcard character. A lot of times what is going on here is that someone is trying to do a name search. Or maybe they don't know how that name is spelled. So they just want to include a character or two at the beginning and then find all the matching entries and pick the correct record out of the result set. The problem here is one of selectivity. In the query shown on the slide, all of the last names that start with the character S is going to be a pretty good percentage of the rows in this table. Oracle is going to infer this from database statistics. Realize that it has to read a significant part of the index, and a significant part of the table. And then often it's going to make the decision to resort to a full table scan instead. So in this case, even though the index might have sufficient selectivity, since our query is only providing a character or two at the beginning of the column. It is this fact that is governing our selectivity for this query. And not the overall selectivity of the index. In these cases what you need to do is see if you can set some lower limits. On the amount of information that the user must provide. For example if the user can give you the first three characters of a last name rather than just one. This should help to make the query much more selective. The idea here, is that the more information you can give Oracle the better job oracle can do in finding an efficient way to process your sequel statement. The difference between giving Oracle one character and giving Oracle three or four characters can be significant. Now sometimes you will run into situations like my last name of Berry. The way that I spell my last name is with a b-e, but there are many people who spell the last name of Berry with a b-a and when searching for someone, you may not know what the correct spelling is, so you want to specify only the first character of the last name. Situations like this are tough, but they are real situations that are encountered by your users of your application. One solution would be to make sure the user gives you more information about the first name of the person you're searching for if the first name is also part of the index. In this case, Oracle will have to read all the keys in the index that have a last name that starts with B. But then it can reply a more restrictive filter predicate to these index results. And narrow these results down before doing a table lookup operation. There are downsides in this approach, in that you'll be reading quite a few blocks from the index. And in this case, you'd also get results back for David Baker, David Bond, and even David Beckham. But on the plus side, while you might end up reading quite a few index blocks,. The more restrictive criteria provided on the first name column. Will filter down the results from the index operation. So that using the index is still effective. And you can avoid needing to look up too many rows in the table itself.

Function in the Where Clause

One of the least understood aspects of indexes. Is that if you include a function on a column in your where clause. And Oracle won't be able to use any standard indexes on that column. Note this applies to using a function on the column in the WHERE clause, not the value itself. When you include a function on a column, what Oracle has to do is to read each row out of the table, compute the value from the function, and then perform a comparison to the value provided. Because what you're asking Oracle to do is to find all the roads that match the computed value from the function, not the actually value stored in the table and in the index. Too often, someone is trying to fix a functional problem, like needing to perform a case insensitive search. Or searching for some string that's embedded into another field with a substring function. Consequently, they become focused on the functional aspects of the problem and forget about the performance aspects. But when you use a function in this way, Oracle won't be able to use any of the regular indexes on the table. As was discussed in the advanced indexing techniques module, one solution to this problem is to create a function-based index over the computed value. If you're doing something like a case-insensitive search or phonetic search, this should be your first option. Function-based indexes work very well, and they're a proven solution to these problems. Just make sure that the function you use in your WHERE clause matches the function used in your create index statement exactly. Otherwise Oracle won't be able to match up your statement with the function based index. The other types of functions I frequently see used in WHERE clauses are functions to do some sort of data cleanup or data conversion. These are functions like trim, todate to convert a date stored as text into a date data type, or substring, to extract out part of our string for comparison. In each of these cases, you could use a function-based index and this would work. But you may want to ask yourself a deeper question in each of these cases. And that is, is the data modeled correctly? If you have leading and trailing white spaces, why is that not being cleaned up at the front end when the data is inserted? If a data store has a large, hard data type, why is this? Why isn't that date being stored as a date data type. And if you're constantly using substring to extract a value out of a larger field maybe that field should be split into two fields. In each of these cases there very well could be a performance problem. But this performance problem is most likely just a symptom of a larger data modeling problem. And that is probably causing pain in other areas of your application as well.

Data Type Conversion in the Where Clause

Let's imagine for a moment that we have a definition of a table like is shown on the slide. And now we run a query like the query shown at the bottom of the slide against this table. What is important to know is that in the table the course number column is defined as a VARCHAR2 data type. So a data type that's intended for string values. But in our query we have specified the course number just as a number. In this case the number 101. So what we have here is a mismatch between the data type and the table and the SQL statement. Now Oracle will not error, it will run the statement for you just fine. But it will not use any indexes that have been built over the course number of column due to this implicit data conversion that's going on. The solution to this problem is very easy. You want to make sure that the data types you use in your SQL statements always match up with the data types defined in the table. This is more of an issue of attention to detail rather than anything else. But you do need to be mindful of these data types. Otherwise your index may not get used, and the performance of your statement will be adversely affected. Also when you use bind variables in your application it is best to explicitly specify the data type that bind variables should use. If you don't Oracle will do the best job it can to infer the data type from the value you've provided and the column type in the table. And almost always Oracle infers correctly. But Oracle can make an incorrect guess. And so I always make sure to specify the data type directly and then I avoid this problem altogether. Finally, it is important to pay attention to the data types of columns in your tables as you define these tables in your schema, while this is more of a data modeling issue than a performance issue. Choosing the incorrect data type can cause a lot of problems down the road, including both performance problems and just making your database scheme more difficult to use. Most people expect dates to be stored as a date or time stamp data type, number value to be stored as numbers and so on. When you start deviating from these conventions is where you get into trouble. Because someone will make an assumption about a column type that isn't true. So put some thought into what data types your columns should be, and you'll save yourself a lot of trouble going forward.

Outdated Database Statistics

When the Oracle Optimizer evaluates your SQL statement and generates an execution plan. It uses the database statistics for all the tables and indexes involved with the statement to determine the most efficient execution plan. If these statistics are out of date or do not reflect the current state of the database objects, then it is likely that the optimizer will generate a suboptimal execution plan. For example, if the statistics indicate that the database table is very small in size, say just 1,000 rows. Then Oracle will assume it is efficient to perform a full table scan of the table due to the small size. In reality, if the table is very large, you'd want to use an index operation. Because Oracle has incorrect information about the table, it's going to make a bad choice and not use an index in this situation. Typically, in production environments, your DBA will set up a recurring job that will keep database staff up to date, so the situation occurs less frequently. If you are in a test environment though, things may be less automated. So you want to be conscious of how current your database statistics are. Stats can also be out of date if you've just done a bulk load or bulk deletion of data. Since these operations can significantly change the amount of data and the distribution of data in a table. And, sometimes, after you create a new index, Oracles doesn't seem to have very good information about that index. So it may not use that index in all the situations it should. Technically, from Oracles 10g onward, you're not supposed to have to regather stats after creating a new index. But I've personally seen some situations where I've got some strange results. So I've gotten into the practice of gathering stats after creating a new index. Just to be on the safe side. If you are concerned about your stats being out of date, the solution to get up to date stats is very simple. You just want to use the DBMS stats package. You gather your stats on a table as shown in the top item ans this will cascade down to all indexes on the table as well. If the table is particularly large and you just want to sample a percentage of the rows, this can be done as well to speed up the gathering stats process. If you want to gather stats on every object in a schema, you can do that with the syntax at the bottom of the slide. You prepare, though, that if we have a large schema with a lot of tables, this could take considerable time. Whatever route you choose though, having up-to-date statistics will allow the Oracle Optimizer to make the most informed decisions about how it should execute your sequel statement. And many times, out-of-date stats are the reason why Oracle will seemingly refuse to use an index.

Summary

In this module, we looked more closely about how you should apply indexes and an indexing strategy to your application. First, we looked at what columns you should look to index in your application's database. This included a discussion of identifying and indexing columns with unique values, why you should index foreign key columns, and what other columns you should look to build indexes over. Next we look at the cost of indexes and how you determine if an index is really being used in your application. Having unused indexes will slow your application down. So part of an effective indexing strategy is identifying unused or seldom used indexes, figuring out why those indexes are not being used and then modifying or potentially dropping those indexes. Finally we spent some time going through the most common reason why Oracle would not be using an index you created. At some point, you'll have a statement you are certain should be using an index, but it won't be. And you'll need to diagnose why Oracle isn't using the index. So hopefully, these pointers will help you determine how to resolve the issue.

Monitoring Oracle Applications

Motivation for Performance Monitoring

Hello, my name is David Berry. Welcome to this module on Monitoring Application Performance in Oracle. One of the capabilities we want to have is the ability to monitor and troubleshoot applications as they run in real time. There are a variety of reasons for this. One reason is that once we deploy our application to production, we want to get some data on how it is performing and if it is performing up to our expectations. No matter how much we test and tune our application before we go live, sometimes things can be a little bit different in production. So we want to be proactive in monitoring the health of our application and use any data we find to draw a further performance improvements. Similarly, we might be taking over the support of an existing application. And we need to know if there are any performance bottlenecks. And indeed, what the application is doing and how it is interacting with the database. Having the ability to look inside Oracle and get some of this information, so you can correlate it with what you find in the application code is extremely useful. Another reason why it is important to be able to monitor an application's performance is that at some point, you're probably going to have some sort of performance event that you have to deal with. Now this may be an event where the application is running very, very slowly, users are receiving time outs and the application is unusable. In these cases, you need to quickly diagnose what is happening in the application. Whether that's a surge of users, a slow running query or some issue completely unrelated to the database. But you want to be able to look inside of Oracle and tell what's going on. So you know that if this is a database issue, you should investigate further or look somewhere else. Finally, some of the queries in this section can help you as you capture data from load and performance test that you might run. This is useful because you can capture some data on how your data is running as a whole, rather than just individual statements. And find the SQL statements that are taking up the most resources while you're still in a test environment, so that you can get these statements tuned appropriately. Fortunately, Oracle makes available a wealth of data to us about what is happening inside of the database. This data can help guide our efforts to know what is a problem and what isn't. The primary source for data that we will talk about during this module are the Dynamic Performance Views. Sometimes, also referred to as the V$ views. Whenever a user logs into Oracle or a SQL statement is run, Oracle is taking data about this event. And exposes that data to us via the Dynamic Performance Views. There's nothing that you have to do to turn this on. There's no script to run. All this data is automatically collected in the background for you by Oracle. Now one thing that you do need to be aware of is that these views generally represent a point in time snapshot of what is happening within Oracle. So they're going to list the sessions that are logged into Oracle right now or the contents of the Shared SQL Area right at this moment. They generally, don't provide historical information or trending type data. Still though, they are extremely useful as they give us insight into what's happening inside of our Oracle database. The other set of views to be aware of are the Data Dictionary Views. Now these are not performance views per se, but they contain information about all the database objects within Oracle. So things like tables, columns, indexes and views. Many times, you'll want to join these tables with the Dynamic Performance Views to get a more useful view of what's happening. Other times, you may just want to query these views on their own. Now these views come in three flavors. But they are all essentially the same. The three flavors reflect what data the view will return. All of the views that start with a dba_ will show data for all objects in the database. By default, you may not have access to this set of data dictionary views. The other two sets though, you will. The views that start with all underscore will show data for all of the objects that the currently logged in user has access to. So for example, all_tables would show all of the tables that the current user has permissions to. The user_ views only contain data for the objects that the currently logged in user owns. In this case, user_tables would just list the tables owned by the current logged in user. There is a set of views that I will not be covering in this module. And this is the oracle AWR or Automated Workload Repository Views. The reason that I will not be covering these views is because the AWR functionality is a separately licensed product from Oracle. That is you need to be licensed for the Oracle Diagnostics Pack in order to use not just the AWR tools, but also to query any of the AWR views. Now, these views are interleaved with the DBA_views and the V$ views in your Oracle database. So any views that start with DBA_HIST or the V$ACTIVE_SESSION_HISTORY view, these are views that are part of the Oracle AWR which I won't be covering here. Now if you do want to query these views, the best advice that I can give you is talk to your DBA team and see if you are licensed for these views and go from there. In Oracle 12c, these views are turned off by default. But in some prior versions of Oracle, they were turned on by default even if you weren't licensed. So in any case, make sure to check your licensing situation before exploring these views. Oracle does contain a capability to see if these views have been used. So if you're not licensed, this could show up on a software licensing audit. And then your company would be liable for purchasing the appropriate licenses from Oracle. So it's really best to check and make sure beforehand and stay on the right side of the licensing situation.

Required Permissions

There are some permissions that you're going to need to access the Dynamic Performance Views. And these are permissions that your DBA will need to grant to the user, you log into oracle with in order to run the queries that we're about to look at. Now unfortunately, granting these permissions is a contentious issue in many organizations about whether or not developers should have access to these views. These views allow you to see what is going on on a database-wide basis. So if the Oracle instance has multiple database schemas to support multiple applications, then you would be able to see session and statement information for those applications through the V$ views. And some organizations consider that a security risk. Now these views don't give you access to the table data, just to the SQL statements being run. But if someone was using literal values in their SQL statement, which of course we know that they shouldn't be doing anyway, then you could see those literal values. My thoughts are if organizations really consider this that big of a security risk, say, like an HR database where you have compensation information. Then this data probably should be in a separate Oracle instance, anyway. The other reason that is often given for denying access to the V$ views to application teams is the developers don't know what they're doing and don't understand what the data in these views mean. So it would just cause a lot of trouble for the DBA team by having access to these views. It is unfortunate that this stance is taken in many organizations, that keeps their application teams in the dark about this performance information. These views provide extremely valuable information about how your SQL statements are being executed and can help the application team really understand what their application is doing when it interacts with Oracle. My feeling is that we should always want to encourage individuals to learn more about how their applications and indeed, their entire system works. Because ultimately, this data and knowledge will lead to a better performing system. So if you're DBA team is not sure about granting you these permissions, what are some strategies that you can use to help convince them? First of all, if you just send off an email listing all the views you want access to, that probably isn't going to be successful. It may even be regarded as threatening. Instead, meet with your DBA and explain what you are trying to accomplish. You may discuss current performance issues that your application has that you're trying to troubleshoot and what the impact of these issues are having on the application. What you want to do is shift the discussion from purely a permissions discussion to a discussion about what you're trying to accomplish and why. Second, demonstrate that you have knowledge about how Oracle works. And the performance information that you are trying to obtain. No one wants to hand the controls over to an inexperienced pilot. So while you don't have to be an expert. Demonstrating that you've watched this course and maybe you've done some reading on your own will help to put everyone at ease about granting the permissions that you seek. Finally, focus on the benefits that you and your organization will receive by having access to the dynamic performance views that you need, especially the mutual benefits for you and the DBA team. Talk about how being able to query this information will help you write better applications, which will reduce load on the Oracle server and the number of incidents the DBA team has to handle. And you might also talk about that as you become more proficient at database tuning, you'll be mentoring other developers on how to write better applications. Again, which will benefit the DBA team. Finally, don't forget to talk about how better performing applications benefit your companies users and customers. Which should be everyone's goal. By reaching out and having a dialogue with your DBA team, you're more likely to be able to gain access for the performance data that you need. Sometimes building these relationships does take time, but opening up these communication channels can only benefit everyone involved. So from a technical stand point, how do you get access to these views? There are two ways. The first is that you can put a user in the SELECT_CATALOG_ROLE role. This is the easiest and fastest way to get access to all of the Dynamic Performance Views. And this role is already built-in, so that's a plus. The potential downside is that this role gives the user access to a larger number of objects in the database, including all of the deviate under score data dictionary views. So from that point of view, the role really gives the user access to a lot of things that they don't need or don't care about. And because this access is so wide, some organizations consider this a security concern. The second way is to grant access to the individual view. What you probably want to do is to create a role with permissions to the views you need. And then add appropriate users to the role as shown here. What you see on your screen is an abbreviated list of the views that you would want for effective performance tuning. But you can see the full list that I believe is important to have at the provided link. Obviously, you can fine tune this list to meet your particular needs. One last thing that I want to point out here is that when we query these views, we usually do so using names like V_$SESSION or V_$SQL. But you will notice on the slide that I'm granting permissions to V_$SESSION and V_$SQL. That is because these names with the underscore is the name of the actual view. The other names, the names without the underscore is actually just a synonym in Oracle. This is a little bit confusing. But if you go this route and make sure that you have the name of the actual view in your grant statement. If you aren't sure if the name you have is a view or a synonym, you can look in the dba underscore synonym data dictionary view and that will help to answer that question.

Introduction to Queries

For the rest of this module, I'm going to cover some of the queries that I find most useful when working with Oracle. These are queries that I usually try to keep handy in a text document, so that I can quickly copy and paste them when needed. Especially, for those situations when a performance event is going on and I need to be able to quickly see what's happening inside of Oracle. So I'd recommend you do something similar. Something else that you'll want to do is to experiment with these queries and variations of them. There may be other columns that you feel that it's important to include or maybe there's some columns that I've included that you feel should be left out. That's fine. Experimentation is encouraged and desirable. Over time, you'll come up with your own library of performance queries that work for you.

Session Information

There are often times, when it is useful to see what sessions are logged into Oracle and you can do that by querying the v dollar session view. This view contains the Oracle username, the osuser, the client process is running as, the name of the program. The machine or host name, where the client process is running. And the process ID of the client process, as well as information like the login time and how long the connection has been idle. One thing you will notice is in this query, I am limiting the type to user and what this does is it filters out all the background Oracle processes. Because generally, we don't care about those. This is all very useful information if you're trying to troubleshoot a problem with connections. Because you can get up on the database to see who is connected and where those connections are coming from. You can also see if you have a lot of idle connections or if the connections are busy. And as we'll see in a moment, what all these sessions are doing. One variation you can do on the query on the previous slide is to do a group by function on the user machine program and process information and this will effectively give you a count of connections that are coming from each client process. If you have a web application running in a production environment, you may expect to see the same number of connections coming from each application process on your web servers. Assuming you're doing some sort of load balancing on your web servers. There's one time that I was involved in troubleshooting an issue and something didn't seem quite right. So I ran this query against the database and I found out that all of our database connections were coming from only one of our web servers not both of the web servers. I shared this information with one of our server admins. And after some investigation, he found that the load balancer was incorrectly configured so that no requests were getting forwarded to that one web server. So, sometimes, being able to get data like this and group it and filter it in different ways can be very useful because this is giving you some real time actual data of what's happening in your Oracle Database. And in this case it helped alert us to an issue with our load balancer.

Session Resource Usage

You also have times when you want to know what sessions are consuming the most resources. Once again, you might have a performance event going on and you need to quickly figure out who's using up all the resources on the Oracle server. In that case, what you can do is join the v$session view to the v$sessmetric view. The v$sessmetric view contains a number of useful columns about the amount of resources that each session has used in the last 15 second interval. These are items like CPU, logical reads, physical reads and so on. And so by querying this view, you can find out if there's a session that is taking up a lot of resources and causing other work on the database to slow down. And by joining back to v$session, you'll know who this is. Sometimes a session using a lot of resources is due to a bad SQL statement in an application. But it also could be due to a user running an expensive query in production from a tool like SQL Developer, Toad, or even Excel. So in those cases, you want to find out who's doing that and get them to cancel their query, so they can stop impacting other applications that are using Oracle.

What Statements are Running Right Now?

There are times when you need to know what statements are executing right now in the database. I like to call this query the one to use when you're in the middle of troubleshooting a performance issue and your boss is standing at your desk wanting to know what's going on. In this case, The v$session view contains column named sql_id, which if the session is currently executing a SQL statement, will contain the ID of that statement. So you can join the v$session view to the v$sql view and this will give you a list of all the statements currently executing in your Oracle database. One of the other useful aspects of the query that you see on your screen is that it will also give you back information if your statement is being blocked by another SQL statement. The v$session table contains a column named blocking_session. So what you can do is then join back to the v$session and v$sql views a second time as to get information about what session is blocking another session and the statement that is actually causing the blocking. This query or variations of it turn out to be pretty useful, because there's lots of times when you just need to know what the database is doing, and this gives you a look inside.

Finding the Worst Performing Statements

One of the most useful things to do is to find the SQL statements that take the longest, or are the most resource intensive to run in your application. These are the statements that you need to target for performance tuning. By using the v$sqlstats view, you can use a query like the one shown on the slide to look at all of the SQL statements in Oracle shared SQL area and see their execution statistics to tell you which statements are the most expensive to run. The v$sqlstats view contains information like the number of times a statement is executed, the total amount of elapsed time spent executing the statement, the total amount of CPU time the statement has used, the total number of buffer gets, which are the logical IOs, and the total number of disk reads. You can get a lot of this same information from the v$sql view, but Oracle is now recommending that you use the v$sqlstats view, as it's supposed to be a little bit more efficient. By taking all of these values and dividing by the number of executions, you can get the average values for each one of these metrics, so you can see on average what statements take the most elapsed time, what statements take the most CPU. As you can see, I am limiting this version of the query to the top 25 results, but you can adjust this to whatever makes sense for you. Sometimes I'll also add an additional WHERE clause to only look at statements that have executed, say, ten times or more, so I can filter out any one-off statements that may be hanging around the shared SQL area. Finally, as you see, this version of the query is being sorted by the average elapsed time. So these are the statements that are taking the longest to execute on average. The sort order you use, though, is one of the primary places where you want to vary this query. For example, you can sort the data by the most number of logical reads per execution or the most CPU time per execution. And this would give you the statements that are the most resource intensive in the database. These statements that are using a lot of resources on each execution probably also have some of the highest average elapsed times to execute. So these are definitely statements you want to target in your performance tuning efforts. You can also sort by the total amount of logical reads or the total amount of CPU being used. Since this is a total amount, these are, after all, the statements that are using the most resources overall on your database server. You may have a statement that uses a moderately high amount of resources on each execution, but doesn't make the top 25 list above. But due to the frequency of execution, this statement is overall using a lot of resources. And again, you want to get these statements onto your performance tuning list. Finally, I like to sort by the number of times that a statement has been executed. If you notice that you have a query that is executing very frequently, but the data that this query retrieves is static, then there might be an opportunity to cache some of the this data in the application, rather than running the query over and over again. What's useful about the query on the prior slide, in all these different variations, is that you could probably take this query and run it in your production Oracle database today, and you probably would find some performance tuning opportunities for your application. What you do want to remember is that this query is looking at statements that are currently in the shared SQL area. So this is just looking at statements in a snapshot of time. How often are statements cached out of your shared SQL area, to where they would not show up in this query? That depends on a number of factors. Like the amount of memory devoted to the shared SQL area, and how many distinct SQL statements are running against your database. The time any individual SQL statement spends in the shared SQL area could range from several minutes to several hours. Also, if a statement is executed only once, it will be cached out much quicker than a statement that's being executed frequently, because Oracle's managing this cache using a least recently used algorithm. Just know that statements can and do age out of the cache. So if you run this query at 10:00 a.m. in the morning, you probably won't see statements from your batch process that executed at 2:00 a.m. Also, if you run this query at 10:00 a.m. and at 4:00 p.m., you may get somewhat different results, because the workload at those times might be different. The answer here is just to run this query several times throughout the day, and even at night, if you have a batch process. As you do, you'll see some patterns start to emerge about what statements take the longest to run and consume the most resources. And this is where you want to start your performance tuning efforts.

Statements Conducting Full Scan Operations

Another way to attack the problem of expensive and time consuming SQL statements is to find out what statements are doing full scan operations, either of a table or an index, because we know that whenever you have to do a full scan operation, that tends to be a lot of I/O and therefore is relatively slow. For every statement in the shared SQL area, Oracle has the cached execution plan that's used to run that statement. So, we can view these execution plans and indeed their individual steps through the v$sql_plan view. This view contains columns for the type of operation that was performed and against what object. So what we can do is run a query like you see on the screen to tell Oracle that we want to see all the plans that contain full scan operations. Then we can join our results back to the v$sql view and get the text of the SQL command. So if we get results from this query, where we see full scan operations that are occurring on our larger tables, or against the indexes of those tables, we probably want to investigate those statements further and understand what they're doing. Now, remember, a full scan operation by itself isn't bad. The table might be small, and in these cases you're better off having Oracle perform a full table scan operation. But again, this query gives us another tool to find potentially expensive operations in our SQL statements that we can evaluate to see if there's some performance tuning that we should be doing.

Retrieving Execution Plans

Sometimes you'll find a SQL statement through the v$sql or the v$sqlstats view and you want to view the execution plan that Oracle used for that SQL statement. This is especially useful when you're trying to troubleshoot a problem with a statement in your production environment, because you want to see the actual execution plan that was used for that statement. To do this you can use the dbms_xplan package. And you just provide as an argument to the function the SQL ID of the SQL statement that you want the execution plan for. What Oracle will give you back is a text formatted table of your execution plan. But it will include all of the steps of the plan, the predicate information, and everything that you're used to seeing from an execution plan.

Monitoring Index Usage

In one of the prior modules, we discussed how indexes do have a cost associated with them. So you want to make sure that all the indexes in your database are serving a purpose and being used. Indexes that are not being used should either be modified or dropped, as you are paying the cost of maintaining this index, but not reaping any of the rewards. The query on this slide uses the v$sql_plan view to find all of the index operations of execution plans currently in the shared SQL area. It then performs an outer join with the all_indexes view so you can see what indexes are and aren't being used. And finally, it does a join with the v$sqlstats view, so you can see what statements are using those indexes. From this data, there are a number of items we can look at. First, we can look for indexes that are not being used by any execution plans. Those are indexes that we want to understand why they aren't being used and investigate further. Second, we can investigate what I call lightly used indexes. These are indexes that are only used by one or two statements. And maybe those statements only run a handful of times during the day. Now, if an index is lightly used, that doesn't mean that you should automatically drop it. After all, that one statement that uses the index may be a critical statement in your nightly batch process. But it is worthwhile to look at other indexes on the table and see if the statements using the lightly used indexes could use a different index on the table, because if they could, this would eliminate the need for one of the indexes, and that would save some overhead. Finally, be aware that the statement on the prior slide draws its data from the shared SQL area. So again, the data you see here is a snapshot of time and is only reflective of the statements currently in the shared SQL area. What you can do, though, is run this query multiple times a day to get a full picture of what indexes are being used and which ones aren't. What I like to do is set up this query to run on an hourly basis and dump its data into a table. And then after a few days, I have a pretty good picture of the index usage on a database.

Monitoring Hard Parsing and SQL Using Literal Values

In an earlier module in the course, we discussed the negative effects of hard parsing SQL on the performance of your Oracle database and ultimately your application. What is useful to see is how much hard parsing is going on system-wide throughout your Oracle database, so you can judge if this is a problem in your situation and what you may need to do in order to address it. The v$sysstat view contains a large number of statistics about the overall system performance of Oracle. And we'll come back to that in a moment. For hard parsing, two statistics of particular interest are the total number of statements that have been hard parsed and the amount of CPU time devoted to hard parsing. Both of these values are cumulative values since the Oracle instance was last started. Also, know that the CPU value returned is in microseconds of CPU time. So what you want to do is run this query at a couple of regular intervals and take the delta of the two values. And that will provide an indication of how much hard parsing your Oracle database is doing. There was one time I ran this query against a target system, and found that over 10% of our CPU time was going just to hard parsing SQL statements. So fixing this problem became a priority for us. Now, as I said earlier, there's a long list of system statistics that Oracle tracks and make available, and you can see that entire list by querying the v$statname view. For obvious reasons, I'm not going to go through all the statistics that are available, but you can look through this view and experiment with the statistics that are of interest to you. One last note. When you do query statistics from the v$sysstat view, Oracle recommends that you write your WHERE clause using the name of the statistic and not the statistic number or ID, because Oracle reserves the right to change the ID number of a statistic between different versions of Oracle. So how do we find SQL statements that are using literal values and not bind variables and are therefore causing a lot of hard parsing in our Oracle database? What we can do is look at the v$sql view, and included in that view are two columns, force_matching_signature and exact_matching_signature. If these two values are not equal, then the statement contains literal values. What I'm doing in this query is finding all of these statements and then using an analytic function to sort them and then selecting out just the first instance so I can see a representative example of each SQL statement. Knowing what this SQL statement looks like, you should have a much easier time tracking down what application or source file that SQL is in so that you can get that fixed. Now, this query isn't perfect because if you have a statement that is using both bind variables and literal values. But the literal values vary as well in the statement. This query won't find those, but it still help to find a number of your statements that could benefit from bind variables.

Table Information

It is oftentimes useful to get some information about the tables in your database. And you can do this using the data dictionary views like user tables, all tables, or DBA tables if you have access to the DBA tables view. The data in these views is going to be accurate as of the last time the statics was gathered on the table. So you do want to pay attention to the last analyzed date column and make sure that the information you're getting isn't out of date. As you can see, some of the information I'm returning in this query is the number of rows in the table, the number of block, the average row size, and the total amount of space that is allocated to the table on disk. To get the space information about the table, you need to join either to the user_segments or the dba_segments data dictionary view. For this particular view, there is no all_segments option. The user version will give you just the segments for the tables that your login user owns while the dba_segments version will contain information for all segments in the oracle database. You will also notice several different block columns in this query. The block column from the tables data dictionary view is the number of blocks in the table below the high water mark. So if a table scan is performed against this table, these are the number of blocks that Oracle will read. The blocks column in the segments view is all of the blocks that are allocated to the table, which is usually a little bit larger because Oracle may not have had a need to use all the allocated blocks as of yet. Along with table information, you can also get detailed information about each column in the table. Again, this data is accurate as the last time that stats were gathered on the table. So make sure you're up to date. What I have done here is just pull a few columns out of the all_tab_columns view. This is a quick way to see how many distinct values each column has. How many null values each column contains. And especially for columns, the average length of the data in these columns. You will see a lot of tools for Oracle, like SQL developer or toad. Make similar information like this available in a nice graphical user interface. All of the data in these tools is coming from data dictionary views like the data dictionary views that you see here. Sometimes though, it's useful to be able to go in and directly access the data and not look at it through a user interface, because maybe you want to take that data and join to some other data or maybe you want to write your own custom where clause to look for something in particular. So be aware that these data dictionary views do exists, and there are a lot of different variations you can try out in order to get to the data that you need.

Index Information

Just like it is useful to get information about your tables, it can also be useful to get information about your indexes. Again, at the risk of sounding like a broken record, this information is accurate as of the last time the index was analyzed whether stats were gathered directly on the index or as part of a stats gathering operation against the table. One of the reasons this query is useful is because it tells us how much space an index is taking and that's always something we want to pay attention to. Second, this query tells us the number of distinct keys in the index. So this helps us understand how selective our index is. We could go through and issue separate sequel statements to find the number of distinct keys in each of our indexes. But this view makes it convenient because we can simply query this information from the view and see all of our indexes for a table side by side.

Summary

In this module, we introduced how you can monitor what your application is doing in Oracle, primarily through the use of the dynamic performance views or v$ views. These are views that are built into Oracle that are collecting data all of the time. So there is nothing special that you have to do in order to turn these views on. Along the way, we explored a number of useful queries for obtaining information from Oracle that can help us understand how an application is behaving and performing. This module served as an introduction to the dynamic performance views. There are over 700 dynamic performance views in Oracle 12c. And you can find the full documentation of all of these views at the link provided. We only touch the surface of some of the most important query's that developers often need to use, but there is much more out there. Finally, don't be afraid to experiment with the queries provided in this module. And don't be afraid to look at queries that others have written and incorporate those ideas into your own personal library of performance monitoring queries. There are endless variations to what you can do with these views, so exploration is encouraged, as this will help you find the right set of queries to know how your application is behaving.

Pitfalls and Practices

Module Introduction

Hello. My name is David Berry. Welcome to this module on Pitfalls and Practices in Developing Oracle Applications. Over the prior modules in this course, we've looked extensively at topics such as reading execution plans and how to index your application. In this module, we'll switch gears a little bit and take a look at some of the practices you should employ when developing your application. First, we'll discuss why you should design your application such, that any reporting you do is separate from your transactional database. Next, we'll discuss why you need to pay attention to how much data your application is loading and especially when this data is being loaded. So you can avoid creating lengthy procedures that load a bunch of data upfront, only to never see this data being used in the application. Third, we'll discuss transaction SCO. And the default commit behavior of the ODP.NET and JVDC drivers. By understanding when you should commit, you will end up with the right functionality in your application and potentially a performance boost. Finally, we'll talk about a couple of common issues that people encounter when using object relational mappers in the development of their application. And what you can do to recognize these issues and solve them so your ORM doesn't cause a degradation in performance.

Separating Transactional and Reporting Databases

A common pitfall that occurs is when a database used to support business applications is also used to simultaneously support reporting applications. When we say business applications, we're talking about transactional type applications that help to run your business. Whether these are order entry applications, applications for managing customer data or something of that nature. And thereby, the database used by these applications is designed to maximize transaction processing. Too often times, what happens is that you have one of these business applications. And then sometime later, someone decides that they need to report on the data in this database. This could be the number of orders shipped, the number of customers signed up, you get the idea. So what happens is that some queries get written to support this need. And they're written against that same transactional database. You can usually identify these queries because they have group by clauses and they're heavy on SQL functions like count, average and sum. And these queries fulfill that reporting need. The problem we run into is performance. And the root cause of this problem is that we really have two different use cases that we're trying to fulfill in the same database. For our business applications, we want statements that execute very quickly and against a small amount of data. These are users on our website placing orders or our customer service reps helping our customers. So quick, focused SQL is the rule. In addition, typically, when we design these databases, we're trying to normalize our data. Because we're focused on maintaining the data integrity of the system. For reporting applications, we're reporting over and summing up a large number of rows. Which means we have to read a large number of rows out of the database. Further, we are hampered because the data in the transactional database isn't really in a good format for reporting. So we also have to join many tables together to get the data we need for our reports. So the tendency of these queries is that they run less frequently, but each query uses a lot of resources and they take a long time to run. So inevitably what happens is that someone is running one of these reports during the day. And they start consuming a lot of resources on the Oracle server. And because they're consuming so many resources on the Oracle server, we start to get poor response time in our business applications. The problem is that these two use cases are really at odds with each other. They just don't coexist very well. So it's very difficult to have these two use cases running against the same Oracle database. The answer here is that your transactional processing database that supports business applications needs to be separate and distinct from your reporting applications and your reporting database. And what you do is have some sort of ETL or extract, transform, and load process, that loads data from your transactional database to your reporting database at regular intervals, oftentimes on a nightly basis. This ETL process could just load data into the reporting database in the same format as your transactional database. Or more likely, you will transform this data into a structure that is much more optimized for reporting applications. This allows those reporting applications to be much more efficient. Because they're operating against a table structure which is optimized for reporting. And it also allows you to deploy some of the very sophisticated business intelligence tools that are available on the market today. For our purposes, it separates these workloads from each other and that is the benefit. Now, when an intensive reporting query is run, we don't have to worry about it impacting our business applications and our transactional database. As a developer, you are most likely familiar with the single responsibility principle. And I think that applies here too, to these databases. A database needs to have one purpose and remain focused on that purpose. Just like in the code you write, a database won't do very well if it has too many jobs to do.

Loading too Much Data

One pitfall that people encounter when building data access layers is loading too much data into their objects upfront. You may have a complex object that is composed of many other objects. And I've seen times where people will load the data for each and every object on the initial load of the primary object. There are performance implications to this. Depending on the depth of your object hierarchy, your data access layer may have to issue a large number of queries to load data for each of the constituent objects. Or it may be running some relatively complex join operations. So, in these cases, the application, and hence the user, will be waiting on the data access layer while it loads all the data into this object. While subsequent operations will be very fast, this initial load could be time consuming and cause the user to wait quite a while. Another factor to consider is that depending on what functions the user ends up performing in your application, they may not even use all the data that is being loaded up front. So a lot of this data may wind up going to waste. For example, if you're like me, you probably order a lot of products from Amazon.com. In my case, I've been ordering products since 1996. Would it make sense if every time I logged into Amazon's website, they loaded my entire order history into memory? Probably not. Now there is a page on Amazon's website where I can go and see my entire order history. Most likely, this data is only being loaded when I go to that page. Since I don't go to that page very frequently, there isn't a need to load this data up front. In fact, loading all that data up front would probably degrade my experience every time I initially browse to Amazon's website. Because they would be taking time to load a bunch of data that I'm unlikely to need and unlikely to care about. Now, there is a balance. And sometimes, it is smart to load some data up front. So there are two pieces of advice that I can offer you. First, use your knowledge of the application and how the user uses the application to make intelligent choices about what data is loaded and when. In this process, focus on what data the user needs to complete their task. And then focus on any other commonly used related data that the user almost always is going to make use of when completing this task. For data that is less frequently used, leverage designed patterns like lazy loading. Many ORM's support lazy loading out of the box. Or if you're writing your own data access layer, there are other courses here on Pluralsight that can teach you the lazy loading pattern. Think of this as on-demand access for your data. This way, when you need a piece of data in your application, it will be available to you. At the same time though, you delay loading this data until you're sure that you really need it. So you eliminate any waste of resources by loading data that never gets used.

Committing Data too Frequently

If you are using ODP.NET or JDBC, you may not be aware, but by default, autocommit is turned on in these packages. This means that every time you issue an insert, update, or delete command. If you have not taken character wrap this block of data access code in a transaction, this command will automatically commit to Oracle. This is different than in tools like SQL Plus or SQL Developer. Whereas you probably know, you have to explicitly issue a commit command in order to commit your changes to the database. Having Autocommit turned on by default has some big implications. First of all, you may really want your data to be inside of a transaction. Meaning, all of your changes commit or none of them do so. So if this is the behavior you want, you need to explicitly create a transaction in your data access code and manage it, so you get this behavior. Secondly though, from a performance standpoint, if you are committing too often, this could actually slow down your data access code that uses DML. Let's see what we mean by this. We're taking a look at a piece of data access code in C Sharp. And what this code is going to do is take all the course offering ID's a student has registered for in a term and insert them into the course enrollment table. Since most students take anywhere from four to six courses in a semester, we would expect this work to commit as a logical unit of work. That is, either we insert all of the courses that this student is enrolling in or none of them. In this piece of code here, we're just using the default behavior of the ODP.NET driver. And this is the same default behavior that you have on the JDBC driver. So as we look through these courses, and run our command right here, since autocommit is on by default, what is happening is an implicit commit is being issued with each statement. Now you might say, what is the big deal about that? If we picture what's really happening, though, we're executing a statement and then committing. Executing and then committing, and so on. Remember, Oracle is an asset compliant database that insures the consistency of its data at all times. That is one of the reason that you use Oracle, is because Oracle will guarantee that a transaction will not get lost. So up on the Oracle server, whenever a commit occurs, Oracle must make sure that the data is written to it's transactional redo logs before it can return from the call and tell your application that the insert succeeded. This way, if for any reason the Oracle server was to crash the incident after this happened, your data would be safe and not get lost. This is one of the distinguishing features of a relational database, the data consistency that they offer. So returning to our code, what is happening here is however many courses are getting passed in, in this array, that is how many times we're going to commit the data in Oracle. And this is probably the wrong answer for two reasons. One, it isn't very good for performance. And two, if our application were to crash, say after record number two was inserted, we wouldn't be in a very good state data wise. Because our student will be enrolled in two courses, but not in the other courses that they were trying to sign up for. So we can't just rerun our process to insert the data. We'd have to rerun just part of our process. And this is one of the reasons why you use transactions is to avoid these sticky situations. Now, we could solve both of these problems by using a transaction. So let's scroll down and see how we do that. By wrapping our data access code in a transaction, we're going to get the business behavior that we want. And now, since we're only going to issue one commit, Oracle is going to have just one disk operation to write to its redo logs rather than five or six, like it would have had before. And so this saves time on our Oracle server. Let's imagine for a moment that rather than inserting five or six rows, we were inserting 1000 rows. And then it's very easy for us to imagine that doing just this one disc operation will be faster than doing 1000 separate disc operations. Even if that one disc operation that we were doing was a little bit larger than any of those individual operations. Whenever you get a live small chatty operation, that tends to be very slow. And in this case, when we have autocommit turned on, what it's like is that we're painting a small tax on each DML operation. Now, you should not change how often you commit based solely on performance. The size of your transaction should be determined by your business need. If it makes sense from a business standpoint to commit after every row, then that's what you should do. An example of this might be inserting data into an error logging table. In this case, you do want to insert after every row. Because you don't want to wait for another error to occur. Because you might lose data. And that data might be important to resolve the problem. But if you do have some DML statements that should be part of a larger transaction. It's important to know that you can and probably should explicitly spell out your transactions in your application code. And not rely upon the autocommit behavior because this probably isn't what you want. Explicitly spelling out your transactions will give you the business functionality that you're after, and also help you in terms of performance. I've seen a number of programs over the years whose job it was just to import data into a database. And many of these programs didn't define their own transactions. So they were autocommitting after each row. Just by changing the program over to properly use transactions provided a significant performance increase. So be aware of how the ODP.NET and JDBC drivers are configured by default. And make sure, for your transactional data, you have the right transaction scope, as you can get both a functional and performance benefit.

ORMs and Abstraction From the Database

Over the last few years, Object Relational Mappers, or ORMs, have become very popular in the development community. And it's easy to see why. ORMs allow us to work at higher levels of abstraction. They also eliminate the need to write a lot of very repetitive data access code just to get data into and out of the database. Consequently, they help us increase our productivity as a developers. One the down side though, removing us from having to work directly with the database gives us less visibility and less control into how our data access code is executed. And so one of the side effects of this is, that if you just leave your data access up to the ORM and don't think about how the data access is performed, you can get some very poor performance. Let's see what we mean by this. I have a very simple application here that's using Entity framework to connect Oracle. And what I want to focus your attention on are these lines here, where I have a link query to find all the applicants that have a GPA over 3.5, and an SAT math score over 700. This is one of the things that we like about ORM's, is the fact they make it very easy to write this fluent type code, and the ORM will worry about the details of generating the SQL for us. The issue is that it becomes easy just to think about interacting with the ORM and forget the behind the scenes. This LINQ code actually has to be translated into SQL and then run against Oracle. And in this case, there's not an index on either of these columns. So this query will end up performing a full table scan. This situation can occur anytime that we have these methods that return an IEnumerable object and let us build these fluent type queries against the IEnumerable object. Whether we're using Entity framework database first, like I am here. Entity framework DB context. A repository pattern that returns an IEnumerable. And this could also occur in other ORMs. By having this fluent interface, where we can easily add criteria against our IEnumerable object, we have a very flexible interface. We can easily construct any combination or criteria that we want. The downside of that flexibility though, is that as we get further abstracted away from our SQL, and the tables that the SQL will run against, it becomes easier to lose sight of the fact that we aren't querying a small number of objects in memory. But this query will have to run against potentially a very large database table. And if there are not indexes on our columns, or our criteria is not selective enough, we could experience some very poor performance. So the practice you should follow is, understand what your ORM is doing in terms of the SQL it is generating, and running. And make sure that you evaluate this SQL, just like any other SQL that you would write yourself by examining the execution plan and appropriately indexing columns in your tables. If need be, don't be afraid to take back control from the ORM in certain situations that require high performance. While these tools advance every year, there are still some scenarios where handwritten SQL will provide much better performance.

ORMs and the n+1 Selects Issue

There is another issue that occurs on a regular basis when using an ORM. And that is the n+1 selects issue. So what do we mean by this? In looking at the code that you see on the screen, we're using editing framework to first load a student. And then, we're loading all of the courses that that student has completed and calculating their GPA. That's what this code does down here. In order to do this though, we have to get some data from some other tables. One table is the course enrollments table, that stores the courses that the students took, and their grade. We also have to quarry the courses table, because we need to determine how many credits each course is worth. And on the way to get the course data, we also end up having to query the course offerings table, because this is the table that links the course enrollments and courses table. By default, Entity Framework will lazy load all of this data. That is it will only load the data when it's needed. And in this case, that works against us. What happens here is that Entity framework gets all the student data and that's one query. Then, to get the course enrollment data, that is a second query. But when we start looping over all these course enrollment records, and we get to this line here, Entity framework is going to have to lazy load the course data. So it's going to issue a query for each course offering record, and then for each course record. And it's going to do that each time it goes through the loop. In this case, the student has completed 40 courses at our university. So what happens here is that editing framework is going to actually run 80 queries inside of this loop. And then on top of this, there are actually are three more queries that get run to get the data out of the grades table. So, this relatively simple looking loop is issuing a total of 85 queries. And even though these queries are all very small and simple, just the chattiness of all the back and forth between the application and the database is not going to result in very good performance. I do want to point out that this issue is not unique to entity framework. It can occur any time we have lazy loading like this going on. So if you're using Hibernate or another ORM, you're still susceptible to this problem. Let's run this program real quick. And we'll be able to actually see all of these queries in Oracle. Okay. Now, let's head over to SQL Developer. And I should tell you that before I started this demo, I did clear out the shared SQL area, so that only the queries from this application would be in the cache. And also, the reason I have all these where clauses in this query is that in the background, the ODP.NET driver and Oracle are exchanging some information and I want to filter all of that out. So we can get down and just see the queries that our application is running. So let's do that. So here we are. And we can see indeed that we're hitting the database 80 plus times. And we see a couple of these queries being run 40 times each. Again, all of these queries individually are very fast. But the problem is the number of these queries. If we could run just one query, while even though that query would be a little bit more complex, that would result in better performance. Because we'd get rid of all this back and forth that's going on between our application and our database. So let's see how we do that. In Entity Framework, we can specify that we want data to be loaded upfront, or eagerly loaded. And to do that, we use an include in our link queries. And now, what Entity Framework is going to do is it's going to issue one query to get all of the course information. And that's going to join all these tables together. The course enrollment's. The course offerings. The courses table. And the grades table. So, we're going to see that we get rid of a lot of queries. So, let's go ahead and run this again. And we'll demonstrate that. Before we run it though, we'll go over and we'll clear out our shared SQL area again, so we're starting fresh. There we go. And now we will run this again. We get the same answer for GPA, which is always a good start. And if we go back to SQL Developer. And so now we see, we're issuing just one query. So this should be an improvement over what we had before. We can copy this query out here and look at it. And we will see that this is a more complex query. But still, we're going to be doing a much better job because we don't have all that back and forth of the 85 queries that we had before. So what you want to remember, is that you want to be aware of the implications of lazy loading. A lot of times, lazy loading is a big benefit, because it keeps us from loading a bunch of data that we never intend to use. But there are a few scenarios like this one where it can work against us. So you want to recognize these situations like this, where you're looping over a bunch of objects that are going to be lazy loaded. And then see if you can instruct your ORM to use eager loading to load all that data upfront. Because this is going to yield better performance.

Summary

In this module, we have covered some of the performance pitfalls that you can encounter when developing applications for Oracle. And the practices you should employ to solve these issues. The practices we've talked about have included separating reporting functions from running against a transactional database. Making sure you only load the data that is needed upfront in an application. Only committing when it is appropriate and not relying on the default auto commit behavior. And practices for understanding what your ORM is doing when it's generating SQL. This list is by no means comprehensive of every situation that you will encounter when developing an application. But they do serve to illustrate the basic problem-solving steps. We want to use our knowledge of the application to break down each functional component of the application into smaller steps. Ultimately, somewhere within these smaller steps, we're going to have some data access code and some SQL. Whether that is SQL that we're writing or is being generated for us out of a tool like an ORM. And then we want to use our knowledge of how our application works, how Oracle works, and how those two interact together. In order to understand what is happening in each of those steps and why. In this way, troubleshooting an Oracle performance problem is no different than solving any other problem. We just break the problem down into smaller steps. Understand the function of each one of those steps. And how that function can be performed better. There is one last practice that you should always employ, and that is to keep learning. Better understanding of how Oracle works and how your application interacts with Oracle will not just help you to solve a wider range of performance issues. But many times, will help you to understand how to avoid them in the first place. An excellent place to start is the Oracle documentation. The documentation is extensive, well written and covers every topic within Oracle. The links provided will take you to the HTML table of contents for each subject. From there, you can navigate the documentation. Or in many cases, you're able to download the entire topic in PDF, ePub or Mobi format, so you can read it on your mobile device. Also, Oracle publishes Oracle Magazine on a monthly basis. You can sign up for free to have a print copy sent to you, or you can just read it online. You will also find all of the back issues online. Oracle Magazine doesn't just contain information of interest to DBAs. There are recurring columns on Java and .NET development, features on how to better use SQL, and articles on performance tuning. So as we wrap up this module and this course, the final thought I will leave you with, is summed up in the words of Ben Franklin when he said, an investment in knowledge always pays the best interest.

Course author

    
David Berry
David Berry is a software engineer with over 15 years of experience developing applications in languages such as Java and C#. Throughout his career, he has worked extensively with enterprise...
Course info

