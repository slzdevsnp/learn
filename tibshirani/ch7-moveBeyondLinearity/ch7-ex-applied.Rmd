---
title: "ch7-ex-applied"
author: "S Zimine"
date: "12/5/2017"
output: html_document
---


### Ex 6
#### 6.a
From Asad solution

```{r}
library(ISLR)
library(boot)

dfmax <- 10
kcv <- 10
all.deltas <- rep(NA, dfmax)
for (i in 1:dfmax) {
  glm.fit <- glm(wage~poly(age, i,raw=TRUE), data=Wage)
  all.deltas[i] = cv.glm(Wage, glm.fit, K=kcv)$delta[2] #10-folkd CV
}

plot(1:dfmax, all.deltas, xlab="Degree", ylab="CV error", type="l", pch=20, lwd=2, main="dataset MSE as a function of polynomial degree in wage~poly(wage,i)")
min.point = min(all.deltas)
print(min.point)
sd.points = sd(all.deltas)
abline(h=min.point + 0.2 * sd.points, col="red", lty="dashed")
abline(h=min.point - 0.2 * sd.points, col="red", lty="dashed")
legend("topright", "0.2-standard deviation lines", lty="dashed", col="red")
```

The plot indicates that $df=3$ is a minimal degrees of freedom with a small enough test MSE.

Lets compare it with `anova()` results.

```{r}
fit1 <- lm(wage~poly(age,1,raw=TRUE),data=Wage)
fit2 <- lm(wage~poly(age,2,raw=TRUE),data=Wage)
fit3 <- lm(wage~poly(age,3,raw=TRUE),data=Wage)
fit4 <- lm(wage~poly(age,4,raw=TRUE),data=Wage)
fit5 <- lm(wage~poly(age,5,raw=TRUE),data=Wage)
fit6 <- lm(wage~poly(age,6,raw=TRUE),data=Wage)
fit7 <- lm(wage~poly(age,7,raw=TRUE),data=Wage)
fit8 <- lm(wage~poly(age,8,raw=TRUE),data=Wage)
fit9 <- lm(wage~poly(age,9,raw=TRUE),data=Wage)

anova(fit1,fit2,fit3,fit4,fit5,fit6,fit7,fit8,fit9)
```
`anova()` also shows that the minimul number of degree of freedom is 3.

Plot the fit with the determined number of $df$.
```{r}
agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])

fit<- glm(wage~poly(age,3,raw=T),data=Wage)
pred <- predict(fit, newdata=list(age=age.grid),se=TRUE)
se.bands <- cbind(pred$fit+2*pred$se  , pred$fit-2*pred$se )

plot(Wage$age,Wage$wage, main="Wage vs age with df=3", col="darkgray", pch=20, cex=0.7 )
lines(age.grid, pred$fit, lwd=2, col="blue")
matlines(age.grid, se.bands,col="blue", lty=2)    
```

### 6.b

```{r}
cutmax <- 10
all.cvs = rep(NA, cutmax)
for (i in 2:10) {
  Wage$age.cut = cut(Wage$age, i)
  lm.fit = glm(wage~age.cut, data=Wage)
  all.cvs[i] = cv.glm(Wage, lm.fit, K=10)$delta[2]
}
plot(2:10, all.cvs[-1], xlab="Number of cuts", ylab="CV error", type="l", pch=20, lwd=2)
```

Number of cuts is 8

```{r}
fit <- glm(wage~cut(age,8), data=Wage)
preds <- predict(fit, newdata=list(age=age.grid))
plot(wage~age, data=Wage, col="darkgrey", cex=0.8)
lines(age.grid, preds,  lwd=2,col="blue")
```

### Ex 7

```{r}
plot(wage~maritl, data=Wage)
plot(wage~jobclass, data=Wage)
```

From plots it appears a married couple makes more money on average than other groups. It
also appears that Informational jobs are higher-wage than Industrial jobs on
average.



#### gams
```{r}
require(gam)
gam1<- gam(wage~maritl+jobclass+s(age,df=4), data=Wage)
par( mfrow=c(1,3))
plot(gam1,se=TRUE)
```

#### Polynomial and Step functions
```{r 7.2}
fit = lm(wage~maritl, data=Wage)
deviance(fit)
fit = lm(wage~jobclass, data=Wage)
deviance(fit)
fit = lm(wage~maritl+jobclass, data=Wage)
deviance(fit)
```

### Ex 8

The flollow variables were identified as having non-linear relationship
```{r}
data(Auto)
par(mfrow=c(2,3))
plot(mpg~displacement, data=Auto)
plot(mpg~horsepower, data=Auto, cex=0.6)
plot(mpg~weight, data=Auto, cex=0.6)
plot(mpg~acceleration, data=Auto, cex=0.6)
plot(mpg~cylinders, data=Auto, cex=0.6)
```

Lets examine `cylinders`
```{r}
# polynomial regression
maxpd <-4
all.deltas = rep(NA, maxpd)
for (i in 1:maxpd) {
  glm.fit = glm(mpg~poly(cylinders, i), data=Auto)
  all.deltas[i] = cv.glm(Auto, glm.fit, K=10)$delta[2]
}
plot(1:maxpd, all.deltas, xlab="Degree", ylab="CV error", type="l", pch=20, lwd=2)

cylr <- range(Auto$cylinders)
cyl.grid <-seq(from=cylr[1], to=cylr[2])

cy.fit = glm(mpg~poly(cylinders, 3), data=Auto) # 
cy.preds <- predict(cy.fit, newdata=list(cylinders=cyl.grid),se=TRUE)
cy.se.bands=cbind(cy.preds$fit+2*cy.preds$se.fit ,cy.preds$fit-2*cy.preds$se.fit )

plot(mpg~cylinders,data=Auto)
lines(cyl.grid,cy.preds$fit,lwd=2,col="blue")
matlines(cyl.grid,cy.se.bands,lwd=1,col="blue",lty=3)
```

#### Les study the displacement variable with various approaches

##### Polynomial
```{r}
rss <- rep(NA,10)
fits <- list()
for (d in 1:10){
  fits[[d]] <- lm(mpg~poly(displacement,d), data=Auto)
  rss[d] <- deviance(fits[[d]])
}
rss
which.min(rss)
```
train RSS expectedly decreases with an increase of a polynomial degree

```{r}
anova(fits[[1]],fits[[2]],fits[[3]],fits[[4]])
```
Anova shows that the polynimial degree of 2 for displacment is enough.
Les see how cross-validation selects a degree of polynomical
```{r}
require(glmnet)
set.seed(1)
cv.errs <- rep(NA,15)
for (d in 1:15){
  fit <- glm(mpg~poly(displacement,d), data=Auto)
  cv.errs[d] <- cv.glm(Auto, fit, K=10)$delta[2]
}
which.min(cv.errs)
```
Suprisingly cross validation selects a 10th degree polynomial.

#### Step functions

```{r}
cv.errs <- rep(NA,10)
for (c in 2:10){
  Auto$dis.cut <- cut(Auto$displacement,c)
  fit <- glm(mpg~dis.cut, data=Auto)
  cv.errs[c] <- cv.glm(Auto, fit, K=10)$delta[2]
}
which.min(cv.errs)
```

###### Splines 

```{r}
require(splines)
cv.errs <- rep(NA,10)
for (df in 3:10){
  fit <- glm(mpg~ns(displacement,df=df), data=Auto)
  cv.errs[df] <- cv.glm(Auto, fit, K=10)$delta[2]
}
which.min(cv.errs)
```
Train RSS decreases with increase of degrees of freedoms in splines for feature displacement.

####GAMS 
```{r}
library(gam)
fit <- gam(mpg ~s(displacement,4)  + s(horsepower,4) + weight, data=Auto)
summary(fit)
par(mfrow=c(1,3))
plot(fit,se=T)
```
from Anova, weight appears to be a linear predictor


### Ex 9

#### a)
```{r}
require(MASS)
attach(Boston)
fit.1 <- lm(nox~poly(dis,3), data=Boston)
dis.grid <- seq(range(dis)[1], range(dis)[2], by=0.05)
preds <- predict(fit.1, newdata=data.frame(dis=dis.grid), se=T )
plot(nox~dis,data=Boston,cex=0.6,col="gray")
lines(dis.grid, preds$fit, lwd=2, col="red")
```

##### b)
```{r}
rss<-rep(NA,10)
fits <- list()
preds <- list()
plot(nox~dis,data=Boston,cex=0.6,col="gray")
for (d in 1:10){
  fits[[d]] <- lm(nox~poly(dis,d), data=Boston)
  rss[d] <- deviance(fits[[d]])
  #rss[d] <- sum(fits[[d]]$residuals^2) #same result
  preds[[d]] <- predict(fits[[d]], newdata=list(dis=dis.grid))
  lines(dis.grid, preds[[d]], lwd=1, col=d)
}
print(rss)
```
As expected, train RSS decreases monotonically,  the df=3 is sufficient for the dis predictor. 

####  c)

```{r}
set.seed(1)
cv.errs <- rep(NA,10)
for (d in 1:10){
  fit <- glm(nox~poly(dis,d), data=Boston)
  cv.errs[d] <- cv.glm(Boston, fit, K=10)$delta[2]
}
print( which.min(cv.errs) )
plot(cv.errs, type="l", xlab="polynomial degrees")
```

10-fold X-validation chooses the `df=4`


### d)

```{r}
require(splines)
fit <- lm(nox~bs(dis,df=4, knots=c(3,6,8)), data=Boston)
summary(fit)

pred <- predict(fit, newdata=list(dis=dis.grid), se=T )
plot(nox~dis,data=Boston,cex=0.6,col="gray")
lines(dis.grid,pred$fit,lwd=2, col="blue")
lines(dis.grid,pred$fit+2*pred$se ,lty="dashed")
lines(dis.grid,pred$fit-2*pred$se ,lty="dashed")

```

Knots were chosen from the visual aspect of the data.

##### e) 
```{r}
rss <- rep(NA,14)
for (df in 3:14){
  fit <- glm(nox~bs(dis,df=df), data=Boston)
  rss[df] <- sum(fit$residuals^2)
}
print( rss[-c(1,2)] )
```

RSS decreases monotonically

#### f)

```{r}
options(warn=-1)
cv.errs <- rep(NA,14)
for (df in 3:14){
  fit <- glm(nox~bs(dis,df=df), data=Boston)
  cv.errs[df] <- cv.glm(Boston, fit, K=10)$delta[2]
}
print( which.min(cv.errs) )

plot(3:14, cv.errs[-c(1, 2)], lwd = 2, type = "l", xlab = "df", ylab = "CV error")
```



