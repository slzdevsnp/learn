---
title: "ISLR Chapter 6 Labs on Model Selection and Regularization"
author: "S Zimine"
date: "11/13/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Lab 1: Subset Selection Methods
================================

```{r cars}
library(ISLR)
data(Hitters)
names(Hitters)
```
We use the dataset Hitters containing features about baseball players and wish  to predict the player's `Salary`.

## Data Cleaning

We first need to check if we should clean the data.  Checking for NA occurencies in data.
```{r}
dim(Hitters)
sum(is.na(Hitters$Salary))
```
We do have  missing `Salary` points. So lets clean the dataset.
```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

## Best Subset Selection
Use `regsubsets()` func from `leaps` package to perform  best Subsets selection. We can do since our number of features is `r -1+dim(Hitters)[2]`.

Doing best subset selection on the full dataset.
```{r}
library(leaps)
p_features<- dim(Hitters)[2] -1
regfit.full<-regsubsets(Salary~., data=Hitters, nvmax=p_features)
reg.summary<-summary(regfit.full)
reg.summary
names(reg.summary)
```
Asteriks show the best selected features for a given `k`.

Lets explore $R^2$. Then plot RSS, adusted $R^2$, $C_p$ and $BIC$.
```{r}
reg.summary$rsq

par(mfrow=c(1,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l", main="Best subset select")
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l", main="Best subset select")

idx_max<- which.max(reg.summary$adjr2)
paste("R-squared is at max with  number of features:",idx_max)
points(idx_max,reg.summary$adjr2[idx_max],col="red", cex=2, pch=20)
```

In similar fashion lets plot $C_p$ and $BIC$ statitstics.
```{r}
par(mfrow=c(1,2))
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp",type="l", main="subset select")
cp_idx <- which.min(reg.summary$cp)
points(cp_idx,reg.summary$cp[cp_idx],col="red", cex=2, pch=20)

plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC",type="l", main="subset select")
b_idx <- which.min(reg.summary$bic)
points(b_idx,reg.summary$bic[b_idx],col="red", cex=2, pch=20)

```

The `regsubsets()` has a built-in `plot()`.
```{r}
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
```

Smallest BIC is for a model with 6 predictors. Print their coeffs
```{r}
coef(regfit.full,6)
```

### Forward and Backward Stepwise selection

`resubsets()` acn be also used to do forward and backwards stepwise selection

```{r}
p_features<- dim(Hitters)[2] -1
regfit.fwd<-regsubsets(Salary~., data=Hitters, nvmax=p_features,method="forward")
reg_f.summary<-summary(regfit.fwd)
reg_f.summary
regfit.bwd<-regsubsets(Salary~., data=Hitters, nvmax=p_features,method="backward")
reg_b.summary<-summary(regfit.bwd)
reg_b.summary
```
For the model with 7 predictors the coeffs are different by the best subset, forward-step and backward step selections
```{r}
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
```

## Choosing Among Models using Validation Set approach and Cross-Validation

We will be fitting all models using only observations from **training dataset**.
### Validation Set Approach
We first divide the dataset into a training set and a test set. A training set is determined by an index vector or boolean values. **TRUE** means an observation is in a training set, **FALSE** is a test set

```{r}
set.seed(1) #for repoducibility of results
#split the dataset roguhly in half
train <- sample( c(TRUE,FALSE), nrow(Hitters), rep=TRUE)
length(train)
sum(train)  # how main elmements in train

#test is the opposite of train
test <- !train
sum(test) #how many elements in test
```
Aplying `regsubsets()` for best subset selection only `train` dataset.
```{r}
regfit.best=regsubsets(Salary~.,data=Hitters[train,], nvmax =p_features)
```

Now lets compute explicitly the validation test error
```{r}
test.mat<-model.matrix(Salary~. ,data=Hitters[test,])
dim(test.mat)

val.errors <- rep(NA,p_features)
#loop over models of all sizes between 1 and 19
for (i in 1:p_features){
	coefi <- coef(regfit.best,id=i) #best model of size k=i
    pred  <- test.mat[,names(coefi)]%*%coefi
    val.errors[i] <- mean( (Hitters$Salary[test]-pred)^2 )
}
val.errors
```
Lets plot the validation test error curve and the coefficients of the best model:
```{r}
plot(val.errors, ylab="test error", xlab="number of parametrs", type="l")
ve_min <- which.min(val.errors)
points(ve_min,val.errors[ve_min],col="red", cex=2, pch=20)
ve_min
coef(regfit.best,id=ve_min)

This was a little tedious
```